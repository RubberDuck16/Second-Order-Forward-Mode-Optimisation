{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09734926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.nn import relu\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, jacobian, value_and_grad, vmap, jvp\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from helper_functions import *\n",
    "import optax\n",
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a79201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_params(di, Nh, do, key):\n",
    "    w_key, c_key = random.split(key)\n",
    "    C = 1/jnp.sqrt((di+1)) * random.normal(c_key, (Nh, di + 1)) \n",
    "    W = 1/jnp.sqrt(Nh) * random.normal(w_key, (do, Nh)) \n",
    "    return {\"C\": C, \"W\": W}\n",
    "\n",
    "def shallow_nn(x, params):\n",
    "    x = jnp.vstack((x, jnp.ones((1, x.shape[1]))))           # Add 1 as the bias input\n",
    "    #h = jnp.tanh(jnp.matmul(params[\"C\"], x)) \n",
    "    h = relu(jnp.matmul(params[\"C\"], x))\n",
    "    y = jnp.matmul(params[\"W\"], h)  \n",
    "    return y                                          \n",
    "\n",
    "def output_from_flat_params(flat_params, unravel_fn, x):\n",
    "    params = unravel_fn(flat_params)\n",
    "    return shallow_nn(x, params)        # shape: (do, no_of_samples)\n",
    "\n",
    "def compute_GGN(current_flat_params, unravel_fn, x):\n",
    "    no_of_samples = x.shape[1]\n",
    "    J = jacobian(output_from_flat_params)(current_flat_params, unravel_fn, x)       # shape: (do, no_of_samples, P)\n",
    "    J = J.reshape(-1, current_flat_params.shape[0])                                 # shape: (do * no_of_samples, P)\n",
    "    GGN = 2/no_of_samples * (J.T @ J)\n",
    "    return GGN\n",
    "\n",
    "def mse_loss(flat_params, unravel_fn, x, y_true):\n",
    "    y_pred = output_from_flat_params(flat_params, unravel_fn, x) \n",
    "    return jnp.mean(jnp.sum((y_pred - y_true) ** 2, axis=0))  # sum over outputs, mean over samples\n",
    "\n",
    "\n",
    "def Jv_fn(flat_params, unravel_fn, x, V):\n",
    "    def model_fn(p):\n",
    "        return output_from_flat_params(p, unravel_fn, x).ravel()\n",
    "\n",
    "    # jvp for a single tangent vector\n",
    "    def single_jvp(v_col):\n",
    "        _, jv = jvp(model_fn, (flat_params,), (v_col,))   # jv is (d_o * N,)\n",
    "        return jv\n",
    "\n",
    "    # vmap across columns of V  (axis 1)\n",
    "    Jv = vmap(single_jvp, in_axes=1, out_axes=1)(V)       # shape (d_o * N, K)\n",
    "    return Jv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0adb98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_SOFO(K, N, initial_params, x, y, learning_rate=1, damping=False, alpha=1e-4):\n",
    "    \"\"\"\n",
    "    K = sketching dimension\n",
    "    N = number of iterations\n",
    "    initial_params = initial parameters in the form of a pytree\n",
    "    x = input data\n",
    "    y = labels for input data\n",
    "    learning_rate = step size\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    current_theta, unravel_fn = ravel_pytree(initial_params)    # flatten the parameters into a 1D array\n",
    "    P = current_theta.shape[0]\n",
    "    \n",
    "    no_of_samples = x.shape[1]\n",
    "\n",
    "    grad_loss_fn = value_and_grad(mse_loss, argnums=0)\n",
    "\n",
    "    for n in range(N):  \n",
    "        # sketching matrix (random tangents)\n",
    "        V = np.random.randn(P, K)  \n",
    "\n",
    "        loss, gradient = grad_loss_fn(current_theta, unravel_fn, x, y)      # gradient of cost function\n",
    "        losses.append(loss)\n",
    "\n",
    "        # use jvp to sketch: compute J @ V \n",
    "        Jv = Jv_fn(current_theta, unravel_fn, x, V)  # shape: (N * do, K)\n",
    "\n",
    "        # compute GGN\n",
    "        G_sketched = (2 / no_of_samples) * (Jv.T @ Jv)  # K x K\n",
    "        \n",
    "        g = V.T @ gradient          # sketched gradient (K x 1)\n",
    "\n",
    "        # parameter update\n",
    "        if damping:\n",
    "            U_damp, s, Vt = np.linalg.svd(G_sketched)\n",
    "            damping_factor = alpha * np.max(s)\n",
    "            inverted_G = U_damp * (1 / (s + damping_factor)) @ Vt\n",
    "            dtheta = V @ inverted_G @ g\n",
    "        else:\n",
    "            dtheta = V @ (np.linalg.solve(G_sketched, g))\n",
    "        current_theta = current_theta - learning_rate * dtheta\n",
    "\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Step {n} | Loss: {loss:.6f}\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "def nn_SOFO_eigs(K, N, initial_params, x, y, approx_G, learning_rate=1, damping=False, alpha=1e-4):\n",
    "    \"\"\"\n",
    "    K = sketching dimension\n",
    "    N = number of iterations\n",
    "    initial_params = initial parameters in the form of a pytree\n",
    "    x = input data\n",
    "    y = labels for input data\n",
    "    approx_G = the approximated GGN\n",
    "    learning_rate = step size\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "\n",
    "    if isinstance(approx_G, list):\n",
    "        U, _ = get_eigenvectors(approx_G)   \n",
    "    else:\n",
    "        U, _, _ = np.linalg.svd(approx_G)\n",
    "    \n",
    "    current_theta, unravel_fn = ravel_pytree(initial_params)    # flatten the parameters into a 1D array\n",
    "    P = current_theta.shape[0]\n",
    "    \n",
    "    no_of_samples = x.shape[1]\n",
    "\n",
    "    grad_loss_fn = value_and_grad(mse_loss, argnums=0)\n",
    "\n",
    "    for n in range(N):  \n",
    "        # sketching matrix (eigenvector tangents)\n",
    "        indices = (np.arange(K) + (K*n % P)) % P\n",
    "        V = U[:, indices]    \n",
    "\n",
    "        loss, gradient = grad_loss_fn(current_theta, unravel_fn, x, y)      # gradient of cost function\n",
    "        losses.append(loss)\n",
    "\n",
    "        # use jvp to sketch: compute J @ V \n",
    "        Jv = Jv_fn(current_theta, unravel_fn, x, V)  # shape: (N * do, K)\n",
    "\n",
    "        # compute GGN\n",
    "        G_sketched = (2 / no_of_samples) * (Jv.T @ Jv)  # K x K\n",
    "        \n",
    "        g = V.T @ gradient          # sketched gradient of cost function (K x 1)\n",
    "\n",
    "        # parameter update\n",
    "        if damping:\n",
    "            U_damp, s, Vt = np.linalg.svd(G_sketched)\n",
    "            damping_factor = alpha * np.max(s)\n",
    "            inverted_G = U_damp * (1 / (s + damping_factor)) @ Vt\n",
    "            dtheta = V @ inverted_G @ g\n",
    "        else:\n",
    "            dtheta = V @ (np.linalg.solve(G_sketched, g))\n",
    "        current_theta = current_theta - learning_rate * dtheta  \n",
    "\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Step {n} | Loss: {loss:.6f}\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "def nn_SOFO_eigs2(K, N, initial_params, x, y, layers, approx_freq=500, sketching_iters=25000, approx_K=10, learning_rate=1, damping=False, alpha=1e-4):\n",
    "    \"\"\"\n",
    "    K = sketching dimension\n",
    "    N = number of iterations\n",
    "    initial_params = initial parameters in the form of a pytree\n",
    "    x = input data\n",
    "    y = labels for input data\n",
    "    learning_rate = step size\n",
    "    approx_freq = frequency of approximating the GGN (re-estimate GGN every approx_freq steps)\n",
    "    sketching_iters = number of iterations for learning the GGN\n",
    "    approx_K = sketching dimension for learning the GGN\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    current_theta, unravel_fn = ravel_pytree(initial_params)    # flatten the parameters into a 1D array\n",
    "    P = current_theta.shape[0]\n",
    "    \n",
    "    no_of_samples = x.shape[1]\n",
    "\n",
    "    grad_loss_fn = value_and_grad(mse_loss, argnums=0)\n",
    "\n",
    "    for n in range(N):  \n",
    "        # approximate GGN \n",
    "        if n % approx_freq == 0:\n",
    "            print(\"Recomputing GGN\")\n",
    "            current_GGN = compute_GGN(current_theta, unravel_fn, x)\n",
    "            learned_G, _, _, _ = learn_G_multiple_layers(layers, current_GGN, iters=sketching_iters, K=approx_K, learning_rate=1e-4)\n",
    "            U, _ = get_eigenvectors(learned_G)  \n",
    "\n",
    "        # sketching matrix (eigenvector tangents)\n",
    "        indices = (np.arange(K) + (K*n % P)) % P\n",
    "        V = U[:, indices]    \n",
    "\n",
    "        loss, gradient = grad_loss_fn(current_theta, unravel_fn, x, y)      # gradient of cost function\n",
    "        losses.append(loss)\n",
    "\n",
    "        # use jvp to sketch: compute J @ V \n",
    "        Jv = Jv_fn(current_theta, unravel_fn, x, V)  # shape: (N * do, K)\n",
    "\n",
    "        # compute GGN\n",
    "        G_sketched = (2 / no_of_samples) * (Jv.T @ Jv)  # K x K\n",
    "        \n",
    "        g = V.T @ gradient          # sketched gradient of cost function (K x 1)\n",
    "\n",
    "        # parameter update\n",
    "        if damping:\n",
    "            U_damp, s, Vt = np.linalg.svd(G_sketched)\n",
    "            damping_factor = alpha * np.max(s)\n",
    "            inverted_G = U_damp * (1 / (s + damping_factor)) @ Vt\n",
    "            dtheta = V @ inverted_G @ g\n",
    "        else:\n",
    "            dtheta = V @ (np.linalg.solve(G_sketched, g))\n",
    "        current_theta = current_theta - learning_rate * dtheta  \n",
    "\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Step {n} | Loss: {loss:.6f}\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "def gradient_descent(N, initial_params, x, y, learning_rate=1):\n",
    "    \"\"\"\n",
    "    N = number of iterations\n",
    "    initial_params = initial parameters in the form of a pytree\n",
    "    x = input data\n",
    "    y = labels for input data\n",
    "    learning_rate = step size\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    current_theta, unravel_fn = ravel_pytree(initial_params)    # flatten the parameters into a 1D array\n",
    "\n",
    "    grad_loss_fn = value_and_grad(mse_loss, argnums=0)\n",
    "\n",
    "    for n in range(N):  \n",
    "        loss, gradient = grad_loss_fn(current_theta, unravel_fn, x, y)      # gradient of cost function\n",
    "        losses.append(loss)\n",
    "    \n",
    "        current_theta = current_theta - learning_rate * gradient\n",
    "\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Step {n} | Loss: {loss:.6f}\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "def adam_training(N, initial_params, x, y, learning_rate=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Train for N iterations with Adam.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N              : int                – number of iterations\n",
    "    initial_params : pytree            – initial parameters\n",
    "    x              : ndarray           – input data\n",
    "    y              : ndarray           – target labels\n",
    "    learning_rate  : float             – Adam lr (default 1e-3)\n",
    "    beta1, beta2   : float             – Adam momentum coefficients\n",
    "    eps            : float             – Adam epsilon\n",
    "    \"\"\"\n",
    "    # Flatten parameters to 1-D vector\n",
    "    theta0, unravel_fn = ravel_pytree(initial_params)\n",
    "\n",
    "    # Optax optimiser\n",
    "    opt = optax.adam(learning_rate, b1=beta1, b2=beta2, eps=eps)\n",
    "    opt_state = opt.init(theta0)\n",
    "\n",
    "    # Value-and-grad function on the flat parameter vector\n",
    "    loss_and_grad = value_and_grad(mse_loss, argnums=0)\n",
    "\n",
    "    losses = []\n",
    "    @jit\n",
    "    def update_step(theta, opt_state):\n",
    "        loss, g = loss_and_grad(theta, unravel_fn, x, y)\n",
    "        updates, opt_state2 = opt.update(g, opt_state)\n",
    "        theta2 = optax.apply_updates(theta, updates)\n",
    "        return theta2, opt_state2, loss\n",
    "\n",
    "    theta = theta0\n",
    "    for n in range(N):\n",
    "        theta, opt_state, loss_val = update_step(theta, opt_state)\n",
    "        losses.append(loss_val)\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Step {n:5d} | Loss: {loss_val:.6f}\")\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92d3926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 225\n",
      "y.shape: (3, 600)\n",
      "x.shape: (5, 600)\n",
      "GGN_teacher shape: (225, 225)\n"
     ]
    }
   ],
   "source": [
    "no_of_samples = 600      # no of training samples\n",
    "di = 5                   # input dimension\n",
    "do = 3\n",
    "Nh = 25\n",
    "\n",
    "P = Nh * (di + 1) + do * Nh\n",
    "print('P:', P)\n",
    "\n",
    "layers = [(Nh, di+1), (do, Nh)]  \n",
    "\n",
    "key = random.PRNGKey(8)\n",
    "key1, key2, key3, key4 = random.split(key, 4)\n",
    "\n",
    "## Generate covariance matrix from which we will sample inputs ##\n",
    "alpha = 1.5\n",
    "eigenvalues = np.array([1 / (i + 1)**alpha for i in range(di)])  # power law\n",
    "Q, _ = np.linalg.qr(random.normal(key1, (di, di)))\n",
    "sigma = Q @ np.diag(eigenvalues) @ Q.T\n",
    "\n",
    "# Sample input data\n",
    "z = random.normal(key2, (di, no_of_samples))         # input vector (di x no_of_samples)\n",
    "x = np.linalg.cholesky(sigma) @ z\n",
    "\n",
    "# make ground-truth / teacher params\n",
    "teacher_params = random_params(di, Nh, do, key3)\n",
    "flat_teacher_params, teacher_unravel_fn = ravel_pytree(teacher_params)\n",
    "\n",
    "y = shallow_nn(x, teacher_params)  # shape: (do, no_of_samples)\n",
    "print('y.shape:', y.shape)\n",
    "print('x.shape:', x.shape)\n",
    "\n",
    "GGN_teacher = compute_GGN(flat_teacher_params, teacher_unravel_fn, x)\n",
    "print(\"GGN_teacher shape:\", GGN_teacher.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39278531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     0 | Loss: 0.574181\n",
      "Step   100 | Loss: 0.086457\n",
      "Step   200 | Loss: 0.033250\n",
      "Step   300 | Loss: 0.019621\n",
      "Step   400 | Loss: 0.014993\n",
      "Step   500 | Loss: 0.012594\n",
      "Step   600 | Loss: 0.010890\n",
      "Step   700 | Loss: 0.009624\n",
      "Step   800 | Loss: 0.008679\n",
      "Step   900 | Loss: 0.007985\n",
      "Step  1000 | Loss: 0.007391\n",
      "Step  1100 | Loss: 0.006877\n",
      "Step  1200 | Loss: 0.006417\n",
      "Step  1300 | Loss: 0.006010\n",
      "Step  1400 | Loss: 0.005647\n",
      "Step  1500 | Loss: 0.005344\n",
      "Step  1600 | Loss: 0.005074\n",
      "Step  1700 | Loss: 0.004826\n",
      "Step  1800 | Loss: 0.004575\n",
      "Step  1900 | Loss: 0.004364\n",
      "Step  2000 | Loss: 0.004181\n",
      "Step  2100 | Loss: 0.004011\n",
      "Step  2200 | Loss: 0.003855\n",
      "Step  2300 | Loss: 0.003690\n",
      "Step  2400 | Loss: 0.003543\n",
      "Step  2500 | Loss: 0.003411\n",
      "Step  2600 | Loss: 0.003283\n",
      "Step  2700 | Loss: 0.003166\n",
      "Step  2800 | Loss: 0.003061\n",
      "Step  2900 | Loss: 0.002961\n",
      "Step  3000 | Loss: 0.002858\n",
      "Step  3100 | Loss: 0.002759\n",
      "Step  3200 | Loss: 0.002651\n",
      "Step  3300 | Loss: 0.002532\n",
      "Step  3400 | Loss: 0.002414\n",
      "Step  3500 | Loss: 0.002313\n",
      "Step  3600 | Loss: 0.002221\n",
      "Step  3700 | Loss: 0.002108\n",
      "Step  3800 | Loss: 0.001952\n",
      "Step  3900 | Loss: 0.001824\n",
      "Step  4000 | Loss: 0.001714\n",
      "Step  4100 | Loss: 0.001616\n",
      "Step  4200 | Loss: 0.001525\n",
      "Step  4300 | Loss: 0.001450\n",
      "Step  4400 | Loss: 0.001386\n",
      "Step  4500 | Loss: 0.001329\n",
      "Step  4600 | Loss: 0.001269\n",
      "Step  4700 | Loss: 0.001218\n",
      "Step  4800 | Loss: 0.001169\n",
      "Step  4900 | Loss: 0.001129\n",
      "Step  5000 | Loss: 0.001093\n",
      "Step  5100 | Loss: 0.001061\n",
      "Step  5200 | Loss: 0.001033\n",
      "Step  5300 | Loss: 0.001001\n",
      "Step  5400 | Loss: 0.000970\n",
      "Step  5500 | Loss: 0.000941\n",
      "Step  5600 | Loss: 0.000916\n",
      "Step  5700 | Loss: 0.000877\n",
      "Step  5800 | Loss: 0.000844\n",
      "Step  5900 | Loss: 0.000810\n",
      "Step  6000 | Loss: 0.000776\n",
      "Step  6100 | Loss: 0.000745\n",
      "Step  6200 | Loss: 0.000705\n",
      "Step  6300 | Loss: 0.000656\n",
      "Step  6400 | Loss: 0.000621\n",
      "Step  6500 | Loss: 0.000597\n",
      "Step  6600 | Loss: 0.000574\n",
      "Step  6700 | Loss: 0.000556\n",
      "Step  6800 | Loss: 0.000540\n",
      "Step  6900 | Loss: 0.000524\n",
      "Step  7000 | Loss: 0.000513\n",
      "Step  7100 | Loss: 0.000505\n",
      "Step  7200 | Loss: 0.000496\n",
      "Step  7300 | Loss: 0.000490\n",
      "Step  7400 | Loss: 0.000482\n",
      "Step  7500 | Loss: 0.000477\n",
      "Step  7600 | Loss: 0.000473\n",
      "Step  7700 | Loss: 0.000470\n",
      "Step  7800 | Loss: 0.000467\n",
      "Step  7900 | Loss: 0.000465\n",
      "Step  8000 | Loss: 0.000462\n",
      "Step  8100 | Loss: 0.000461\n",
      "Step  8200 | Loss: 0.000460\n",
      "Step  8300 | Loss: 0.000462\n",
      "Step  8400 | Loss: 0.000458\n",
      "Step  8500 | Loss: 0.000457\n",
      "Step  8600 | Loss: 0.000455\n",
      "Step  8700 | Loss: 0.000453\n",
      "Step  8800 | Loss: 0.000450\n",
      "Step  8900 | Loss: 0.000446\n",
      "Step  9000 | Loss: 0.000443\n",
      "Step  9100 | Loss: 0.000442\n",
      "Step  9200 | Loss: 0.000439\n",
      "Step  9300 | Loss: 0.000436\n",
      "Step  9400 | Loss: 0.000431\n",
      "Step  9500 | Loss: 0.000430\n",
      "Step  9600 | Loss: 0.000424\n",
      "Step  9700 | Loss: 0.000413\n",
      "Step  9800 | Loss: 0.000400\n",
      "Step  9900 | Loss: 0.000386\n",
      "Step 10000 | Loss: 0.000380\n",
      "Step 10100 | Loss: 0.000371\n",
      "Step 10200 | Loss: 0.000361\n",
      "Step 10300 | Loss: 0.000348\n",
      "Step 10400 | Loss: 0.000343\n",
      "Step 10500 | Loss: 0.000339\n",
      "Step 10600 | Loss: 0.000333\n",
      "Step 10700 | Loss: 0.000330\n",
      "Step 10800 | Loss: 0.000328\n",
      "Step 10900 | Loss: 0.000327\n",
      "Step 11000 | Loss: 0.000326\n",
      "Step 11100 | Loss: 0.000325\n",
      "Step 11200 | Loss: 0.000324\n",
      "Step 11300 | Loss: 0.000325\n",
      "Step 11400 | Loss: 0.000323\n",
      "Step 11500 | Loss: 0.000322\n",
      "Step 11600 | Loss: 0.000322\n",
      "Step 11700 | Loss: 0.000321\n",
      "Step 11800 | Loss: 0.000321\n",
      "Step 11900 | Loss: 0.000322\n",
      "Step 12000 | Loss: 0.000321\n",
      "Step 12100 | Loss: 0.000320\n",
      "Step 12200 | Loss: 0.000318\n",
      "Step 12300 | Loss: 0.000317\n",
      "Step 12400 | Loss: 0.000315\n",
      "Step 12500 | Loss: 0.000315\n",
      "Step 12600 | Loss: 0.000314\n",
      "Step 12700 | Loss: 0.000314\n",
      "Step 12800 | Loss: 0.000314\n",
      "Step 12900 | Loss: 0.000313\n",
      "Step 13000 | Loss: 0.000313\n",
      "Step 13100 | Loss: 0.000313\n",
      "Step 13200 | Loss: 0.000313\n",
      "Step 13300 | Loss: 0.000313\n",
      "Step 13400 | Loss: 0.000313\n",
      "Step 13500 | Loss: 0.000313\n",
      "Step 13600 | Loss: 0.000312\n",
      "Step 13700 | Loss: 0.000312\n",
      "Step 13800 | Loss: 0.000312\n",
      "Step 13900 | Loss: 0.000312\n",
      "Step 14000 | Loss: 0.000312\n",
      "Step 14100 | Loss: 0.000312\n",
      "Step 14200 | Loss: 0.000312\n",
      "Step 14300 | Loss: 0.000311\n",
      "Step 14400 | Loss: 0.000311\n",
      "Step 14500 | Loss: 0.000311\n",
      "Step 14600 | Loss: 0.000311\n",
      "Step 14700 | Loss: 0.000311\n",
      "Step 14800 | Loss: 0.000311\n",
      "Step 14900 | Loss: 0.000311\n",
      "Step 15000 | Loss: 0.000311\n",
      "Step 15100 | Loss: 0.000311\n",
      "Step 15200 | Loss: 0.000311\n",
      "Step 15300 | Loss: 0.000311\n",
      "Step 15400 | Loss: 0.000311\n",
      "Step 15500 | Loss: 0.000311\n",
      "Step 15600 | Loss: 0.000311\n",
      "Step 15700 | Loss: 0.000311\n",
      "Step 15800 | Loss: 0.000313\n",
      "Step 15900 | Loss: 0.000311\n",
      "Step 16000 | Loss: 0.000310\n",
      "Step 16100 | Loss: 0.000310\n",
      "Step 16200 | Loss: 0.000311\n",
      "Step 16300 | Loss: 0.000310\n",
      "Step 16400 | Loss: 0.000310\n",
      "Step 16500 | Loss: 0.000311\n",
      "Step 16600 | Loss: 0.000311\n",
      "Step 16700 | Loss: 0.000310\n",
      "Step 16800 | Loss: 0.000310\n",
      "Step 16900 | Loss: 0.000310\n",
      "Step 17000 | Loss: 0.000310\n",
      "Step 17100 | Loss: 0.000310\n",
      "Step 17200 | Loss: 0.000310\n",
      "Step 17300 | Loss: 0.000310\n",
      "Step 17400 | Loss: 0.000309\n",
      "Step 17500 | Loss: 0.000310\n",
      "Step 17600 | Loss: 0.000310\n",
      "Step 17700 | Loss: 0.000309\n",
      "Step 17800 | Loss: 0.000309\n",
      "Step 17900 | Loss: 0.000309\n",
      "Step 18000 | Loss: 0.000309\n",
      "Step 18100 | Loss: 0.000309\n",
      "Step 18200 | Loss: 0.000309\n",
      "Step 18300 | Loss: 0.000309\n",
      "Step 18400 | Loss: 0.000309\n",
      "Step 18500 | Loss: 0.000310\n",
      "Step 18600 | Loss: 0.000309\n",
      "Step 18700 | Loss: 0.000309\n",
      "Step 18800 | Loss: 0.000309\n",
      "Step 18900 | Loss: 0.000309\n",
      "Step 19000 | Loss: 0.000309\n",
      "Step 19100 | Loss: 0.000309\n",
      "Step 19200 | Loss: 0.000309\n",
      "Step 19300 | Loss: 0.000310\n",
      "Step 19400 | Loss: 0.000309\n",
      "Step 19500 | Loss: 0.000309\n",
      "Step 19600 | Loss: 0.000309\n",
      "Step 19700 | Loss: 0.000309\n",
      "Step 19800 | Loss: 0.000309\n",
      "Step 19900 | Loss: 0.000309\n",
      "Step 0 | Loss: 0.574181\n",
      "Step 100 | Loss: 0.055032\n",
      "Step 200 | Loss: 0.034225\n",
      "Step 300 | Loss: 0.025756\n",
      "Step 400 | Loss: 0.021066\n",
      "Step 500 | Loss: 0.017958\n",
      "Step 600 | Loss: 0.015855\n",
      "Step 700 | Loss: 0.014253\n",
      "Step 800 | Loss: 0.013116\n",
      "Step 900 | Loss: 0.012219\n",
      "Step 1000 | Loss: 0.011463\n",
      "Step 1100 | Loss: 0.010954\n",
      "Step 1200 | Loss: 0.010422\n",
      "Step 1300 | Loss: 0.009979\n",
      "Step 1400 | Loss: 0.009561\n",
      "Step 1500 | Loss: 0.009266\n",
      "Step 1600 | Loss: 0.009001\n",
      "Step 1700 | Loss: 0.008780\n",
      "Step 1800 | Loss: 0.008561\n",
      "Step 1900 | Loss: 0.008373\n",
      "Step 2000 | Loss: 0.008171\n",
      "Step 2100 | Loss: 0.007979\n",
      "Step 2200 | Loss: 0.007815\n",
      "Step 2300 | Loss: 0.007661\n",
      "Step 2400 | Loss: 0.007520\n",
      "Step 2500 | Loss: 0.007364\n",
      "Step 2600 | Loss: 0.007252\n",
      "Step 2700 | Loss: 0.007111\n",
      "Step 2800 | Loss: 0.007007\n",
      "Step 2900 | Loss: 0.006910\n",
      "Step 3000 | Loss: 0.006801\n",
      "Step 3100 | Loss: 0.006684\n",
      "Step 3200 | Loss: 0.006565\n",
      "Step 3300 | Loss: 0.006506\n",
      "Step 3400 | Loss: 0.006435\n",
      "Step 3500 | Loss: 0.006323\n",
      "Step 3600 | Loss: 0.006244\n",
      "Step 3700 | Loss: 0.006162\n",
      "Step 3800 | Loss: 0.006080\n",
      "Step 3900 | Loss: 0.006004\n",
      "Step 4000 | Loss: 0.005941\n",
      "Step 4100 | Loss: 0.005880\n",
      "Step 4200 | Loss: 0.005818\n",
      "Step 4300 | Loss: 0.005764\n",
      "Step 4400 | Loss: 0.005714\n",
      "Step 4500 | Loss: 0.005665\n",
      "Step 4600 | Loss: 0.005607\n",
      "Step 4700 | Loss: 0.005556\n",
      "Step 4800 | Loss: 0.005502\n",
      "Step 4900 | Loss: 0.005448\n",
      "Step 5000 | Loss: 0.005394\n",
      "Step 5100 | Loss: 0.005331\n",
      "Step 5200 | Loss: 0.005282\n",
      "Step 5300 | Loss: 0.005238\n",
      "Step 5400 | Loss: 0.005199\n",
      "Step 5500 | Loss: 0.005162\n",
      "Step 5600 | Loss: 0.005127\n",
      "Step 5700 | Loss: 0.005085\n",
      "Step 5800 | Loss: 0.005050\n",
      "Step 5900 | Loss: 0.005012\n",
      "Step 6000 | Loss: 0.004969\n",
      "Step 6100 | Loss: 0.004934\n",
      "Step 6200 | Loss: 0.004896\n",
      "Step 6300 | Loss: 0.004846\n",
      "Step 6400 | Loss: 0.004805\n",
      "Step 6500 | Loss: 0.004763\n",
      "Step 6600 | Loss: 0.004717\n",
      "Step 6700 | Loss: 0.004686\n",
      "Step 6800 | Loss: 0.004657\n",
      "Step 6900 | Loss: 0.004620\n",
      "Step 7000 | Loss: 0.004588\n",
      "Step 7100 | Loss: 0.004553\n",
      "Step 7200 | Loss: 0.004521\n",
      "Step 7300 | Loss: 0.004485\n",
      "Step 7400 | Loss: 0.004450\n",
      "Step 7500 | Loss: 0.004414\n",
      "Step 7600 | Loss: 0.004375\n",
      "Step 7700 | Loss: 0.004348\n",
      "Step 7800 | Loss: 0.004312\n",
      "Step 7900 | Loss: 0.004274\n",
      "Step 8000 | Loss: 0.004240\n",
      "Step 8100 | Loss: 0.004214\n",
      "Step 8200 | Loss: 0.004185\n",
      "Step 8300 | Loss: 0.004151\n",
      "Step 8400 | Loss: 0.004123\n",
      "Step 8500 | Loss: 0.004089\n",
      "Step 8600 | Loss: 0.004061\n",
      "Step 8700 | Loss: 0.004032\n",
      "Step 8800 | Loss: 0.004005\n",
      "Step 8900 | Loss: 0.003985\n",
      "Step 9000 | Loss: 0.003962\n",
      "Step 9100 | Loss: 0.003939\n",
      "Step 9200 | Loss: 0.003912\n",
      "Step 9300 | Loss: 0.003879\n",
      "Step 9400 | Loss: 0.003855\n",
      "Step 9500 | Loss: 0.003832\n",
      "Step 9600 | Loss: 0.003808\n",
      "Step 9700 | Loss: 0.003784\n",
      "Step 9800 | Loss: 0.003757\n",
      "Step 9900 | Loss: 0.003732\n",
      "Step 10000 | Loss: 0.003707\n",
      "Step 10100 | Loss: 0.003689\n",
      "Step 10200 | Loss: 0.003670\n",
      "Step 10300 | Loss: 0.003647\n",
      "Step 10400 | Loss: 0.003619\n",
      "Step 10500 | Loss: 0.003592\n",
      "Step 10600 | Loss: 0.003571\n",
      "Step 10700 | Loss: 0.003548\n",
      "Step 10800 | Loss: 0.003525\n",
      "Step 10900 | Loss: 0.003503\n",
      "Step 11000 | Loss: 0.003474\n",
      "Step 11100 | Loss: 0.003448\n",
      "Step 11200 | Loss: 0.003423\n",
      "Step 11300 | Loss: 0.003395\n",
      "Step 11400 | Loss: 0.003373\n",
      "Step 11500 | Loss: 0.003352\n",
      "Step 11600 | Loss: 0.003327\n",
      "Step 11700 | Loss: 0.003304\n",
      "Step 11800 | Loss: 0.003281\n",
      "Step 11900 | Loss: 0.003255\n",
      "Step 12000 | Loss: 0.003234\n",
      "Step 12100 | Loss: 0.003218\n",
      "Step 12200 | Loss: 0.003193\n",
      "Step 12300 | Loss: 0.003169\n",
      "Step 12400 | Loss: 0.003150\n",
      "Step 12500 | Loss: 0.003130\n",
      "Step 12600 | Loss: 0.003111\n",
      "Step 12700 | Loss: 0.003091\n",
      "Step 12800 | Loss: 0.003070\n",
      "Step 12900 | Loss: 0.003050\n",
      "Step 13000 | Loss: 0.003037\n",
      "Step 13100 | Loss: 0.003020\n",
      "Step 13200 | Loss: 0.003003\n",
      "Step 13300 | Loss: 0.002990\n",
      "Step 13400 | Loss: 0.002978\n",
      "Step 13500 | Loss: 0.002963\n",
      "Step 13600 | Loss: 0.002951\n",
      "Step 13700 | Loss: 0.002935\n",
      "Step 13800 | Loss: 0.002924\n",
      "Step 13900 | Loss: 0.002907\n",
      "Step 14000 | Loss: 0.002894\n",
      "Step 14100 | Loss: 0.002883\n",
      "Step 14200 | Loss: 0.002870\n",
      "Step 14300 | Loss: 0.002859\n",
      "Step 14400 | Loss: 0.002844\n",
      "Step 14500 | Loss: 0.002832\n",
      "Step 14600 | Loss: 0.002821\n",
      "Step 14700 | Loss: 0.002810\n",
      "Step 14800 | Loss: 0.002797\n",
      "Step 14900 | Loss: 0.002784\n",
      "Step 15000 | Loss: 0.002769\n",
      "Step 15100 | Loss: 0.002755\n",
      "Step 15200 | Loss: 0.002740\n",
      "Step 15300 | Loss: 0.002731\n",
      "Step 15400 | Loss: 0.002721\n",
      "Step 15500 | Loss: 0.002708\n",
      "Step 15600 | Loss: 0.002693\n",
      "Step 15700 | Loss: 0.002681\n",
      "Step 15800 | Loss: 0.002672\n",
      "Step 15900 | Loss: 0.002664\n",
      "Step 16000 | Loss: 0.002653\n",
      "Step 16100 | Loss: 0.002642\n",
      "Step 16200 | Loss: 0.002630\n",
      "Step 16300 | Loss: 0.002619\n",
      "Step 16400 | Loss: 0.002605\n",
      "Step 16500 | Loss: 0.002595\n",
      "Step 16600 | Loss: 0.002586\n",
      "Step 16700 | Loss: 0.002576\n",
      "Step 16800 | Loss: 0.002567\n",
      "Step 16900 | Loss: 0.002557\n",
      "Step 17000 | Loss: 0.002546\n",
      "Step 17100 | Loss: 0.002537\n",
      "Step 17200 | Loss: 0.002528\n",
      "Step 17300 | Loss: 0.002521\n",
      "Step 17400 | Loss: 0.002512\n",
      "Step 17500 | Loss: 0.002504\n",
      "Step 17600 | Loss: 0.002495\n",
      "Step 17700 | Loss: 0.002486\n",
      "Step 17800 | Loss: 0.002476\n",
      "Step 17900 | Loss: 0.002468\n",
      "Step 18000 | Loss: 0.002459\n",
      "Step 18100 | Loss: 0.002451\n",
      "Step 18200 | Loss: 0.002440\n",
      "Step 18300 | Loss: 0.002429\n",
      "Step 18400 | Loss: 0.002421\n",
      "Step 18500 | Loss: 0.002409\n",
      "Step 18600 | Loss: 0.002400\n",
      "Step 18700 | Loss: 0.002392\n",
      "Step 18800 | Loss: 0.002383\n",
      "Step 18900 | Loss: 0.002372\n",
      "Step 19000 | Loss: 0.002363\n",
      "Step 19100 | Loss: 0.002354\n",
      "Step 19200 | Loss: 0.002344\n",
      "Step 19300 | Loss: 0.002337\n",
      "Step 19400 | Loss: 0.002328\n",
      "Step 19500 | Loss: 0.002321\n",
      "Step 19600 | Loss: 0.002314\n",
      "Step 19700 | Loss: 0.002308\n",
      "Step 19800 | Loss: 0.002301\n",
      "Step 19900 | Loss: 0.002293\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2154.142578125 for layer [False]\n",
      "Iteration: 100, Loss: 320.3658752441406 for layer [ True]\n",
      "Iteration: 200, Loss: 2116.405029296875 for layer [False]\n",
      "Iteration: 300, Loss: 1807.4527587890625 for layer [False]\n",
      "Iteration: 400, Loss: 219.60540771484375 for layer [ True]\n",
      "Iteration: 500, Loss: 280.37646484375 for layer [ True]\n",
      "Iteration: 600, Loss: 1610.39306640625 for layer [False]\n",
      "Iteration: 700, Loss: 188.9257354736328 for layer [ True]\n",
      "Iteration: 800, Loss: 1365.0474853515625 for layer [False]\n",
      "Iteration: 900, Loss: 1153.5601806640625 for layer [False]\n",
      "Iteration: 1000, Loss: 108.21387481689453 for layer [ True]\n",
      "Iteration: 1100, Loss: 1027.761474609375 for layer [False]\n",
      "Iteration: 1200, Loss: 1284.729248046875 for layer [False]\n",
      "Iteration: 1300, Loss: 1093.1422119140625 for layer [False]\n",
      "Iteration: 1400, Loss: 128.2809600830078 for layer [ True]\n",
      "Iteration: 1500, Loss: 134.0095672607422 for layer [ True]\n",
      "Iteration: 1600, Loss: 84.76541900634766 for layer [ True]\n",
      "Iteration: 1700, Loss: 111.27171325683594 for layer [ True]\n",
      "Iteration: 1800, Loss: 737.9534301757812 for layer [False]\n",
      "Iteration: 1900, Loss: 81.28598022460938 for layer [ True]\n",
      "Iteration: 2000, Loss: 67.10264587402344 for layer [ True]\n",
      "Iteration: 2100, Loss: 90.61642456054688 for layer [ True]\n",
      "Iteration: 2200, Loss: 73.36328125 for layer [ True]\n",
      "Iteration: 2300, Loss: 506.45355224609375 for layer [False]\n",
      "Iteration: 2400, Loss: 64.9726791381836 for layer [ True]\n",
      "Iteration: 2500, Loss: 67.24933624267578 for layer [ True]\n",
      "Iteration: 2600, Loss: 58.77504348754883 for layer [ True]\n",
      "Iteration: 2700, Loss: 48.25952911376953 for layer [ True]\n",
      "Iteration: 2800, Loss: 366.41558837890625 for layer [False]\n",
      "Iteration: 2900, Loss: 303.40667724609375 for layer [False]\n",
      "Iteration: 3000, Loss: 39.8793830871582 for layer [ True]\n",
      "Iteration: 3100, Loss: 283.8625183105469 for layer [False]\n",
      "Iteration: 3200, Loss: 282.12896728515625 for layer [False]\n",
      "Iteration: 3300, Loss: 225.00013732910156 for layer [False]\n",
      "Iteration: 3400, Loss: 26.186601638793945 for layer [ True]\n",
      "Iteration: 3500, Loss: 251.69235229492188 for layer [False]\n",
      "Iteration: 3600, Loss: 28.497718811035156 for layer [ True]\n",
      "Iteration: 3700, Loss: 209.24681091308594 for layer [False]\n",
      "Iteration: 3800, Loss: 152.6968231201172 for layer [False]\n",
      "Iteration: 3900, Loss: 19.671966552734375 for layer [ True]\n",
      "Iteration: 4000, Loss: 22.01123809814453 for layer [ True]\n",
      "Iteration: 4100, Loss: 14.184374809265137 for layer [ True]\n",
      "Iteration: 4200, Loss: 25.390180587768555 for layer [ True]\n",
      "Iteration: 4300, Loss: 18.619386672973633 for layer [ True]\n",
      "Iteration: 4400, Loss: 14.810619354248047 for layer [ True]\n",
      "Iteration: 4500, Loss: 20.50114631652832 for layer [ True]\n",
      "Iteration: 4600, Loss: 12.37463092803955 for layer [ True]\n",
      "Iteration: 4700, Loss: 79.6865463256836 for layer [False]\n",
      "Iteration: 4800, Loss: 53.43505859375 for layer [False]\n",
      "Iteration: 4900, Loss: 62.565032958984375 for layer [False]\n",
      "Iteration: 5000, Loss: 12.214137077331543 for layer [ True]\n",
      "Iteration: 5100, Loss: 8.74173355102539 for layer [ True]\n",
      "Iteration: 5200, Loss: 47.468387603759766 for layer [False]\n",
      "Iteration: 5300, Loss: 11.407903671264648 for layer [ True]\n",
      "Iteration: 5400, Loss: 42.93750762939453 for layer [False]\n",
      "Iteration: 5500, Loss: 9.96094799041748 for layer [ True]\n",
      "Iteration: 5600, Loss: 10.548088073730469 for layer [ True]\n",
      "Iteration: 5700, Loss: 30.341684341430664 for layer [False]\n",
      "Iteration: 5800, Loss: 28.526100158691406 for layer [False]\n",
      "Iteration: 5900, Loss: 21.791927337646484 for layer [False]\n",
      "Iteration: 6000, Loss: 5.696066856384277 for layer [ True]\n",
      "Iteration: 6100, Loss: 19.772249221801758 for layer [False]\n",
      "Iteration: 6200, Loss: 18.69793701171875 for layer [False]\n",
      "Iteration: 6300, Loss: 14.400754928588867 for layer [False]\n",
      "Iteration: 6400, Loss: 6.557624816894531 for layer [ True]\n",
      "Iteration: 6500, Loss: 2.842574119567871 for layer [ True]\n",
      "Iteration: 6600, Loss: 3.5096664428710938 for layer [ True]\n",
      "Iteration: 6700, Loss: 10.188230514526367 for layer [False]\n",
      "Iteration: 6800, Loss: 2.988476514816284 for layer [ True]\n",
      "Iteration: 6900, Loss: 8.292176246643066 for layer [False]\n",
      "Iteration: 7000, Loss: 7.194338798522949 for layer [False]\n",
      "Iteration: 7100, Loss: 8.202593803405762 for layer [False]\n",
      "Iteration: 7200, Loss: 5.201351642608643 for layer [False]\n",
      "Iteration: 7300, Loss: 2.468937397003174 for layer [ True]\n",
      "Iteration: 7400, Loss: 3.690342664718628 for layer [False]\n",
      "Iteration: 7500, Loss: 2.597797393798828 for layer [ True]\n",
      "Iteration: 7600, Loss: 1.8546028137207031 for layer [ True]\n",
      "Iteration: 7700, Loss: 3.0553557872772217 for layer [False]\n",
      "Iteration: 7800, Loss: 3.400352716445923 for layer [False]\n",
      "Iteration: 7900, Loss: 1.5281834602355957 for layer [ True]\n",
      "Iteration: 8000, Loss: 1.9503498077392578 for layer [ True]\n",
      "Iteration: 8100, Loss: 1.2520660161972046 for layer [ True]\n",
      "Iteration: 8200, Loss: 0.8390539884567261 for layer [ True]\n",
      "Iteration: 8300, Loss: 2.9994161128997803 for layer [False]\n",
      "Iteration: 8400, Loss: 0.8199017643928528 for layer [ True]\n",
      "Iteration: 8500, Loss: 2.41536283493042 for layer [False]\n",
      "Iteration: 8600, Loss: 6.4105377197265625 for layer [False]\n",
      "Iteration: 8700, Loss: 0.9582894444465637 for layer [ True]\n",
      "Iteration: 8800, Loss: 2.617842674255371 for layer [False]\n",
      "Iteration: 8900, Loss: 5.39314603805542 for layer [False]\n",
      "Iteration: 9000, Loss: 0.6057276725769043 for layer [ True]\n",
      "Iteration: 9100, Loss: 0.5860365033149719 for layer [ True]\n",
      "Iteration: 9200, Loss: 0.8173987865447998 for layer [ True]\n",
      "Iteration: 9300, Loss: 3.7518203258514404 for layer [False]\n",
      "Iteration: 9400, Loss: 2.209899663925171 for layer [False]\n",
      "Iteration: 9500, Loss: 0.4702199697494507 for layer [ True]\n",
      "Iteration: 9600, Loss: 1.8642594814300537 for layer [False]\n",
      "Iteration: 9700, Loss: 0.33050066232681274 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.2891194522380829 for layer [ True]\n",
      "Iteration: 9900, Loss: 1.258974552154541 for layer [False]\n",
      "Iteration: 10000, Loss: 1.9156758785247803 for layer [False]\n",
      "Iteration: 10100, Loss: 0.22627359628677368 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.15722544491291046 for layer [ True]\n",
      "Iteration: 10300, Loss: 2.362316608428955 for layer [False]\n",
      "Iteration: 10400, Loss: 0.23295892775058746 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.1461116373538971 for layer [ True]\n",
      "Iteration: 10600, Loss: 0.9516761302947998 for layer [False]\n",
      "Iteration: 10700, Loss: 0.18890272080898285 for layer [ True]\n",
      "Iteration: 10800, Loss: 0.9391874670982361 for layer [False]\n",
      "Iteration: 10900, Loss: 0.11191034317016602 for layer [ True]\n",
      "Iteration: 11000, Loss: 0.9551820158958435 for layer [False]\n",
      "Iteration: 11100, Loss: 1.4636400938034058 for layer [False]\n",
      "Iteration: 11200, Loss: 0.09689855575561523 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.0742497593164444 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.09207139909267426 for layer [ True]\n",
      "Iteration: 11500, Loss: 0.9374807476997375 for layer [False]\n",
      "Iteration: 11600, Loss: 0.09497162699699402 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.04741775244474411 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.038542281836271286 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.057178299874067307 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.037242475897073746 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.04987320676445961 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.028094371780753136 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.027766644954681396 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.027684420347213745 for layer [ True]\n",
      "Iteration: 12500, Loss: 1.525556206703186 for layer [False]\n",
      "Iteration: 12600, Loss: 0.5856668949127197 for layer [False]\n",
      "Iteration: 12700, Loss: 1.1143746376037598 for layer [False]\n",
      "Iteration: 12800, Loss: 0.022220894694328308 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.0199004877358675 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.010511478409171104 for layer [ True]\n",
      "Iteration: 13100, Loss: 1.4012906551361084 for layer [False]\n",
      "Iteration: 13200, Loss: 0.012161067686975002 for layer [ True]\n",
      "Iteration: 13300, Loss: 0.7471807599067688 for layer [False]\n",
      "Iteration: 13400, Loss: 0.007710193283855915 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.007644232362508774 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.007672654930502176 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.004385360516607761 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.005861536134034395 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.00557800754904747 for layer [ True]\n",
      "Iteration: 14000, Loss: 0.5147806406021118 for layer [False]\n",
      "Iteration: 14100, Loss: 0.0038334503769874573 for layer [ True]\n",
      "Iteration: 14200, Loss: 0.7057147026062012 for layer [False]\n",
      "Iteration: 14300, Loss: 0.0018963116453960538 for layer [ True]\n",
      "Iteration: 14400, Loss: 1.5726064443588257 for layer [False]\n",
      "Iteration: 14500, Loss: 0.5947601795196533 for layer [False]\n",
      "Iteration: 14600, Loss: 0.4636284410953522 for layer [False]\n",
      "Iteration: 14700, Loss: 0.8715588450431824 for layer [False]\n",
      "Iteration: 14800, Loss: 0.8396347761154175 for layer [False]\n",
      "Iteration: 14900, Loss: 0.6266114711761475 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0008248815429396927 for layer [ True]\n",
      "Iteration: 15100, Loss: 0.4791894853115082 for layer [False]\n",
      "Iteration: 15200, Loss: 0.9978339076042175 for layer [False]\n",
      "Iteration: 15300, Loss: 0.8470824956893921 for layer [False]\n",
      "Iteration: 15400, Loss: 0.8372082114219666 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0006880882428959012 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0004037729522679001 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.0006763717392459512 for layer [ True]\n",
      "Iteration: 15800, Loss: 0.8453736901283264 for layer [False]\n",
      "Iteration: 15900, Loss: 0.00026489453739486635 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0003340980038046837 for layer [ True]\n",
      "Iteration: 16100, Loss: 0.84527188539505 for layer [False]\n",
      "Iteration: 16200, Loss: 0.00014917150838300586 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0001324680633842945 for layer [ True]\n",
      "Iteration: 16400, Loss: 1.2322152853012085 for layer [False]\n",
      "Iteration: 16500, Loss: 0.676017165184021 for layer [False]\n",
      "Iteration: 16600, Loss: 0.35184040665626526 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0004508834972511977 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.00028444486088119447 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.00013029322144575417 for layer [ True]\n",
      "Iteration: 17000, Loss: 9.988583042286336e-05 for layer [ True]\n",
      "Iteration: 17100, Loss: 0.9925986528396606 for layer [False]\n",
      "Iteration: 17200, Loss: 1.0717570781707764 for layer [False]\n",
      "Iteration: 17300, Loss: 4.811460530618206e-05 for layer [ True]\n",
      "Iteration: 17400, Loss: 1.0585379600524902 for layer [False]\n",
      "Iteration: 17500, Loss: 1.4598990678787231 for layer [False]\n",
      "Iteration: 17600, Loss: 0.6067089438438416 for layer [False]\n",
      "Iteration: 17700, Loss: 9.718145156512037e-05 for layer [ True]\n",
      "Iteration: 17800, Loss: 0.8297550678253174 for layer [False]\n",
      "Iteration: 17900, Loss: 1.2125191688537598 for layer [False]\n",
      "Iteration: 18000, Loss: 0.8563672304153442 for layer [False]\n",
      "Iteration: 18100, Loss: 5.992120350128971e-05 for layer [ True]\n",
      "Iteration: 18200, Loss: 7.659063703613356e-05 for layer [ True]\n",
      "Iteration: 18300, Loss: 1.119579792022705 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00010010354890255257 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.00010502195073058829 for layer [ True]\n",
      "Iteration: 18600, Loss: 0.8573132753372192 for layer [False]\n",
      "Iteration: 18700, Loss: 0.6344069242477417 for layer [False]\n",
      "Iteration: 18800, Loss: 1.097229242324829 for layer [False]\n",
      "Iteration: 18900, Loss: 0.6600701808929443 for layer [False]\n",
      "Iteration: 19000, Loss: 7.737700070720166e-05 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.3262161016464233 for layer [False]\n",
      "Iteration: 19200, Loss: 0.00022999798238743097 for layer [ True]\n",
      "Iteration: 19300, Loss: 0.8191765546798706 for layer [False]\n",
      "Iteration: 19400, Loss: 0.9584453105926514 for layer [False]\n",
      "Iteration: 19500, Loss: 0.00034067020169459283 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0002031076146522537 for layer [ True]\n",
      "Iteration: 19700, Loss: 0.7159320116043091 for layer [False]\n",
      "Iteration: 19800, Loss: 1.0966538190841675 for layer [False]\n",
      "Iteration: 19900, Loss: 1.053892731666565 for layer [False]\n",
      "Iteration: 20000, Loss: 0.6970365047454834 for layer [False]\n",
      "Iteration: 20100, Loss: 0.33882468938827515 for layer [False]\n",
      "Iteration: 20200, Loss: 0.00020385773677844554 for layer [ True]\n",
      "Iteration: 20300, Loss: 0.735572338104248 for layer [False]\n",
      "Iteration: 20400, Loss: 3.140977423754521e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 6.673343159491196e-05 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.000531094498001039 for layer [ True]\n",
      "Iteration: 20700, Loss: 0.7473122477531433 for layer [False]\n",
      "Iteration: 20800, Loss: 8.580621215514839e-05 for layer [ True]\n",
      "Iteration: 20900, Loss: 2.6521178369875997e-05 for layer [ True]\n",
      "Iteration: 21000, Loss: 0.5335941314697266 for layer [False]\n",
      "Iteration: 21100, Loss: 0.8298884630203247 for layer [False]\n",
      "Iteration: 21200, Loss: 3.167752947774716e-05 for layer [ True]\n",
      "Iteration: 21300, Loss: 0.7295470237731934 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00019018526654690504 for layer [ True]\n",
      "Iteration: 21500, Loss: 0.9618741273880005 for layer [False]\n",
      "Iteration: 21600, Loss: 1.0342440605163574 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0001163077395176515 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00019708949548657984 for layer [ True]\n",
      "Iteration: 21900, Loss: 0.5617921352386475 for layer [False]\n",
      "Iteration: 22000, Loss: 0.625350832939148 for layer [False]\n",
      "Iteration: 22100, Loss: 7.04058475093916e-05 for layer [ True]\n",
      "Iteration: 22200, Loss: 1.737226739351172e-05 for layer [ True]\n",
      "Iteration: 22300, Loss: 0.8385133743286133 for layer [False]\n",
      "Iteration: 22400, Loss: 1.2001656293869019 for layer [False]\n",
      "Iteration: 22500, Loss: 0.42246711254119873 for layer [False]\n",
      "Iteration: 22600, Loss: 7.057750190142542e-05 for layer [ True]\n",
      "Iteration: 22700, Loss: 4.549619188765064e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 8.789872663328424e-05 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0007034080917946994 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00013637362280860543 for layer [ True]\n",
      "Iteration: 23100, Loss: 8.51617514854297e-05 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0002105737803503871 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00029804438236169517 for layer [ True]\n",
      "Iteration: 23400, Loss: 5.830830195918679e-05 for layer [ True]\n",
      "Iteration: 23500, Loss: 2.7305724870529957e-05 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0001689369382802397 for layer [ True]\n",
      "Iteration: 23700, Loss: 0.7123891115188599 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006064266199246049 for layer [ True]\n",
      "Iteration: 23900, Loss: 5.4678574088029563e-05 for layer [ True]\n",
      "Iteration: 24000, Loss: 3.8381247577490285e-05 for layer [ True]\n",
      "Iteration: 24100, Loss: 3.1854273402132094e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 0.7903411984443665 for layer [False]\n",
      "Iteration: 24300, Loss: 0.6015961170196533 for layer [False]\n",
      "Iteration: 24400, Loss: 0.5168365240097046 for layer [False]\n",
      "Iteration: 24500, Loss: 7.769075455144048e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 3.056591594940983e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 1.4607082605361938 for layer [False]\n",
      "Iteration: 24800, Loss: 0.7519532442092896 for layer [False]\n",
      "Iteration: 24900, Loss: 4.037343751406297e-05 for layer [ True]\n",
      "Step 0 | Loss: 0.574181\n",
      "Step 100 | Loss: 0.013194\n",
      "Step 200 | Loss: 0.009996\n",
      "Step 300 | Loss: 0.008637\n",
      "Step 400 | Loss: 0.007605\n",
      "Step 500 | Loss: 0.006917\n",
      "Step 600 | Loss: 0.006427\n",
      "Step 700 | Loss: 0.005980\n",
      "Step 800 | Loss: 0.005616\n",
      "Step 900 | Loss: 0.005316\n",
      "Step 1000 | Loss: 0.005090\n",
      "Step 1100 | Loss: 0.004892\n",
      "Step 1200 | Loss: 0.004736\n",
      "Step 1300 | Loss: 0.004609\n",
      "Step 1400 | Loss: 0.004491\n",
      "Step 1500 | Loss: 0.004386\n",
      "Step 1600 | Loss: 0.004292\n",
      "Step 1700 | Loss: 0.004196\n",
      "Step 1800 | Loss: 0.004106\n",
      "Step 1900 | Loss: 0.004027\n",
      "Step 2000 | Loss: 0.003948\n",
      "Step 2100 | Loss: 0.003879\n",
      "Step 2200 | Loss: 0.003808\n",
      "Step 2300 | Loss: 0.003741\n",
      "Step 2400 | Loss: 0.003682\n",
      "Step 2500 | Loss: 0.003625\n",
      "Step 2600 | Loss: 0.003573\n",
      "Step 2700 | Loss: 0.003523\n",
      "Step 2800 | Loss: 0.003474\n",
      "Step 2900 | Loss: 0.003430\n",
      "Step 3000 | Loss: 0.003394\n",
      "Step 3100 | Loss: 0.003351\n",
      "Step 3200 | Loss: 0.003307\n",
      "Step 3300 | Loss: 0.003267\n",
      "Step 3400 | Loss: 0.003226\n",
      "Step 3500 | Loss: 0.003181\n",
      "Step 3600 | Loss: 0.003141\n",
      "Step 3700 | Loss: 0.003103\n",
      "Step 3800 | Loss: 0.003064\n",
      "Step 3900 | Loss: 0.003029\n",
      "Step 4000 | Loss: 0.002987\n",
      "Step 4100 | Loss: 0.002946\n",
      "Step 4200 | Loss: 0.002909\n",
      "Step 4300 | Loss: 0.002871\n",
      "Step 4400 | Loss: 0.002831\n",
      "Step 4500 | Loss: 0.002788\n",
      "Step 4600 | Loss: 0.002747\n",
      "Step 4700 | Loss: 0.002706\n",
      "Step 4800 | Loss: 0.002672\n",
      "Step 4900 | Loss: 0.002631\n",
      "Step 5000 | Loss: 0.002591\n",
      "Step 5100 | Loss: 0.002553\n",
      "Step 5200 | Loss: 0.002513\n",
      "Step 5300 | Loss: 0.002478\n",
      "Step 5400 | Loss: 0.002444\n",
      "Step 5500 | Loss: 0.002409\n",
      "Step 5600 | Loss: 0.002374\n",
      "Step 5700 | Loss: 0.002346\n",
      "Step 5800 | Loss: 0.002317\n",
      "Step 5900 | Loss: 0.002287\n",
      "Step 6000 | Loss: 0.002260\n",
      "Step 6100 | Loss: 0.002228\n",
      "Step 6200 | Loss: 0.002196\n",
      "Step 6300 | Loss: 0.002168\n",
      "Step 6400 | Loss: 0.002139\n",
      "Step 6500 | Loss: 0.002111\n",
      "Step 6600 | Loss: 0.002086\n",
      "Step 6700 | Loss: 0.002057\n",
      "Step 6800 | Loss: 0.002032\n",
      "Step 6900 | Loss: 0.002007\n",
      "Step 7000 | Loss: 0.001977\n",
      "Step 7100 | Loss: 0.001947\n",
      "Step 7200 | Loss: 0.001920\n",
      "Step 7300 | Loss: 0.001894\n",
      "Step 7400 | Loss: 0.001871\n",
      "Step 7500 | Loss: 0.001851\n",
      "Step 7600 | Loss: 0.001831\n",
      "Step 7700 | Loss: 0.001814\n",
      "Step 7800 | Loss: 0.001797\n",
      "Step 7900 | Loss: 0.001778\n",
      "Step 8000 | Loss: 0.001757\n",
      "Step 8100 | Loss: 0.001739\n",
      "Step 8200 | Loss: 0.001722\n",
      "Step 8300 | Loss: 0.001707\n",
      "Step 8400 | Loss: 0.001692\n",
      "Step 8500 | Loss: 0.001677\n",
      "Step 8600 | Loss: 0.001662\n",
      "Step 8700 | Loss: 0.001647\n",
      "Step 8800 | Loss: 0.001634\n",
      "Step 8900 | Loss: 0.001620\n",
      "Step 9000 | Loss: 0.001607\n",
      "Step 9100 | Loss: 0.001596\n",
      "Step 9200 | Loss: 0.001585\n",
      "Step 9300 | Loss: 0.001575\n",
      "Step 9400 | Loss: 0.001564\n",
      "Step 9500 | Loss: 0.001554\n",
      "Step 9600 | Loss: 0.001545\n",
      "Step 9700 | Loss: 0.001535\n",
      "Step 9800 | Loss: 0.001524\n",
      "Step 9900 | Loss: 0.001515\n",
      "Step 10000 | Loss: 0.001505\n",
      "Step 10100 | Loss: 0.001498\n",
      "Step 10200 | Loss: 0.001491\n",
      "Step 10300 | Loss: 0.001484\n",
      "Step 10400 | Loss: 0.001478\n",
      "Step 10500 | Loss: 0.001474\n",
      "Step 10600 | Loss: 0.001468\n",
      "Step 10700 | Loss: 0.001463\n",
      "Step 10800 | Loss: 0.001458\n",
      "Step 10900 | Loss: 0.001453\n",
      "Step 11000 | Loss: 0.001448\n",
      "Step 11100 | Loss: 0.001444\n",
      "Step 11200 | Loss: 0.001440\n",
      "Step 11300 | Loss: 0.001436\n",
      "Step 11400 | Loss: 0.001433\n",
      "Step 11500 | Loss: 0.001429\n",
      "Step 11600 | Loss: 0.001425\n",
      "Step 11700 | Loss: 0.001422\n",
      "Step 11800 | Loss: 0.001418\n",
      "Step 11900 | Loss: 0.001415\n",
      "Step 12000 | Loss: 0.001412\n",
      "Step 12100 | Loss: 0.001409\n",
      "Step 12200 | Loss: 0.001405\n",
      "Step 12300 | Loss: 0.001403\n",
      "Step 12400 | Loss: 0.001399\n",
      "Step 12500 | Loss: 0.001396\n",
      "Step 12600 | Loss: 0.001392\n",
      "Step 12700 | Loss: 0.001389\n",
      "Step 12800 | Loss: 0.001385\n",
      "Step 12900 | Loss: 0.001382\n",
      "Step 13000 | Loss: 0.001379\n",
      "Step 13100 | Loss: 0.001376\n",
      "Step 13200 | Loss: 0.001373\n",
      "Step 13300 | Loss: 0.001369\n",
      "Step 13400 | Loss: 0.001367\n",
      "Step 13500 | Loss: 0.001364\n",
      "Step 13600 | Loss: 0.001361\n",
      "Step 13700 | Loss: 0.001357\n",
      "Step 13800 | Loss: 0.001354\n",
      "Step 13900 | Loss: 0.001351\n",
      "Step 14000 | Loss: 0.001347\n",
      "Step 14100 | Loss: 0.001344\n",
      "Step 14200 | Loss: 0.001341\n",
      "Step 14300 | Loss: 0.001339\n",
      "Step 14400 | Loss: 0.001336\n",
      "Step 14500 | Loss: 0.001333\n",
      "Step 14600 | Loss: 0.001331\n",
      "Step 14700 | Loss: 0.001328\n",
      "Step 14800 | Loss: 0.001326\n",
      "Step 14900 | Loss: 0.001324\n",
      "Step 15000 | Loss: 0.001322\n",
      "Step 15100 | Loss: 0.001319\n",
      "Step 15200 | Loss: 0.001318\n",
      "Step 15300 | Loss: 0.001315\n",
      "Step 15400 | Loss: 0.001313\n",
      "Step 15500 | Loss: 0.001311\n",
      "Step 15600 | Loss: 0.001309\n",
      "Step 15700 | Loss: 0.001307\n",
      "Step 15800 | Loss: 0.001305\n",
      "Step 15900 | Loss: 0.001303\n",
      "Step 16000 | Loss: 0.001302\n",
      "Step 16100 | Loss: 0.001300\n",
      "Step 16200 | Loss: 0.001299\n",
      "Step 16300 | Loss: 0.001297\n",
      "Step 16400 | Loss: 0.001295\n",
      "Step 16500 | Loss: 0.001294\n",
      "Step 16600 | Loss: 0.001292\n",
      "Step 16700 | Loss: 0.001291\n",
      "Step 16800 | Loss: 0.001289\n",
      "Step 16900 | Loss: 0.001288\n",
      "Step 17000 | Loss: 0.001286\n",
      "Step 17100 | Loss: 0.001285\n",
      "Step 17200 | Loss: 0.001283\n",
      "Step 17300 | Loss: 0.001282\n",
      "Step 17400 | Loss: 0.001281\n",
      "Step 17500 | Loss: 0.001279\n",
      "Step 17600 | Loss: 0.001278\n",
      "Step 17700 | Loss: 0.001277\n",
      "Step 17800 | Loss: 0.001275\n",
      "Step 17900 | Loss: 0.001274\n",
      "Step 18000 | Loss: 0.001273\n",
      "Step 18100 | Loss: 0.001272\n",
      "Step 18200 | Loss: 0.001271\n",
      "Step 18300 | Loss: 0.001270\n",
      "Step 18400 | Loss: 0.001269\n",
      "Step 18500 | Loss: 0.001268\n",
      "Step 18600 | Loss: 0.001267\n",
      "Step 18700 | Loss: 0.001266\n",
      "Step 18800 | Loss: 0.001265\n",
      "Step 18900 | Loss: 0.001264\n",
      "Step 19000 | Loss: 0.001263\n",
      "Step 19100 | Loss: 0.001262\n",
      "Step 19200 | Loss: 0.001261\n",
      "Step 19300 | Loss: 0.001260\n",
      "Step 19400 | Loss: 0.001258\n",
      "Step 19500 | Loss: 0.001257\n",
      "Step 19600 | Loss: 0.001256\n",
      "Step 19700 | Loss: 0.001254\n",
      "Step 19800 | Loss: 0.001253\n",
      "Step 19900 | Loss: 0.001252\n"
     ]
    }
   ],
   "source": [
    "# learn the parameters\n",
    "student_params = random_params(di, Nh, do, key4)\n",
    "flat_student_params, student_unravel_fn = ravel_pytree(student_params)\n",
    "\n",
    "no_of_iters = 20000\n",
    "K = 2\n",
    "\n",
    "adam_losses = adam_training(no_of_iters, student_params, x, y, learning_rate=1e-3)\n",
    "sofo_losses = nn_SOFO(K, no_of_iters, student_params, x, y, learning_rate=1)\n",
    "sofo_eigs_losses = nn_SOFO_eigs2(K, no_of_iters, student_params, x, y, layers, approx_freq=100000, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "952dc752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2154.142578125 for layer [False]\n",
      "Iteration: 100, Loss: 320.3658752441406 for layer [ True]\n",
      "Iteration: 200, Loss: 2116.405029296875 for layer [False]\n",
      "Iteration: 300, Loss: 1807.4527587890625 for layer [False]\n",
      "Iteration: 400, Loss: 219.60540771484375 for layer [ True]\n",
      "Iteration: 500, Loss: 280.37646484375 for layer [ True]\n",
      "Iteration: 600, Loss: 1610.39306640625 for layer [False]\n",
      "Iteration: 700, Loss: 188.9257354736328 for layer [ True]\n",
      "Iteration: 800, Loss: 1365.0474853515625 for layer [False]\n",
      "Iteration: 900, Loss: 1153.5601806640625 for layer [False]\n",
      "Iteration: 1000, Loss: 108.21387481689453 for layer [ True]\n",
      "Iteration: 1100, Loss: 1027.761474609375 for layer [False]\n",
      "Iteration: 1200, Loss: 1284.729248046875 for layer [False]\n",
      "Iteration: 1300, Loss: 1093.1422119140625 for layer [False]\n",
      "Iteration: 1400, Loss: 128.2809600830078 for layer [ True]\n",
      "Iteration: 1500, Loss: 134.0095672607422 for layer [ True]\n",
      "Iteration: 1600, Loss: 84.76541900634766 for layer [ True]\n",
      "Iteration: 1700, Loss: 111.27171325683594 for layer [ True]\n",
      "Iteration: 1800, Loss: 737.9534301757812 for layer [False]\n",
      "Iteration: 1900, Loss: 81.28598022460938 for layer [ True]\n",
      "Iteration: 2000, Loss: 67.10264587402344 for layer [ True]\n",
      "Iteration: 2100, Loss: 90.61642456054688 for layer [ True]\n",
      "Iteration: 2200, Loss: 73.36328125 for layer [ True]\n",
      "Iteration: 2300, Loss: 506.45355224609375 for layer [False]\n",
      "Iteration: 2400, Loss: 64.9726791381836 for layer [ True]\n",
      "Iteration: 2500, Loss: 67.24933624267578 for layer [ True]\n",
      "Iteration: 2600, Loss: 58.77504348754883 for layer [ True]\n",
      "Iteration: 2700, Loss: 48.25952911376953 for layer [ True]\n",
      "Iteration: 2800, Loss: 366.41558837890625 for layer [False]\n",
      "Iteration: 2900, Loss: 303.40667724609375 for layer [False]\n",
      "Iteration: 3000, Loss: 39.8793830871582 for layer [ True]\n",
      "Iteration: 3100, Loss: 283.8625183105469 for layer [False]\n",
      "Iteration: 3200, Loss: 282.12896728515625 for layer [False]\n",
      "Iteration: 3300, Loss: 225.00013732910156 for layer [False]\n",
      "Iteration: 3400, Loss: 26.186601638793945 for layer [ True]\n",
      "Iteration: 3500, Loss: 251.69235229492188 for layer [False]\n",
      "Iteration: 3600, Loss: 28.497718811035156 for layer [ True]\n",
      "Iteration: 3700, Loss: 209.24681091308594 for layer [False]\n",
      "Iteration: 3800, Loss: 152.6968231201172 for layer [False]\n",
      "Iteration: 3900, Loss: 19.671966552734375 for layer [ True]\n",
      "Iteration: 4000, Loss: 22.01123809814453 for layer [ True]\n",
      "Iteration: 4100, Loss: 14.184374809265137 for layer [ True]\n",
      "Iteration: 4200, Loss: 25.390180587768555 for layer [ True]\n",
      "Iteration: 4300, Loss: 18.619386672973633 for layer [ True]\n",
      "Iteration: 4400, Loss: 14.810619354248047 for layer [ True]\n",
      "Iteration: 4500, Loss: 20.50114631652832 for layer [ True]\n",
      "Iteration: 4600, Loss: 12.37463092803955 for layer [ True]\n",
      "Iteration: 4700, Loss: 79.6865463256836 for layer [False]\n",
      "Iteration: 4800, Loss: 53.43505859375 for layer [False]\n",
      "Iteration: 4900, Loss: 62.565032958984375 for layer [False]\n",
      "Iteration: 5000, Loss: 12.214137077331543 for layer [ True]\n",
      "Iteration: 5100, Loss: 8.74173355102539 for layer [ True]\n",
      "Iteration: 5200, Loss: 47.468387603759766 for layer [False]\n",
      "Iteration: 5300, Loss: 11.407903671264648 for layer [ True]\n",
      "Iteration: 5400, Loss: 42.93750762939453 for layer [False]\n",
      "Iteration: 5500, Loss: 9.96094799041748 for layer [ True]\n",
      "Iteration: 5600, Loss: 10.548088073730469 for layer [ True]\n",
      "Iteration: 5700, Loss: 30.341684341430664 for layer [False]\n",
      "Iteration: 5800, Loss: 28.526100158691406 for layer [False]\n",
      "Iteration: 5900, Loss: 21.791927337646484 for layer [False]\n",
      "Iteration: 6000, Loss: 5.696066856384277 for layer [ True]\n",
      "Iteration: 6100, Loss: 19.772249221801758 for layer [False]\n",
      "Iteration: 6200, Loss: 18.69793701171875 for layer [False]\n",
      "Iteration: 6300, Loss: 14.400754928588867 for layer [False]\n",
      "Iteration: 6400, Loss: 6.557624816894531 for layer [ True]\n",
      "Iteration: 6500, Loss: 2.842574119567871 for layer [ True]\n",
      "Iteration: 6600, Loss: 3.5096664428710938 for layer [ True]\n",
      "Iteration: 6700, Loss: 10.188230514526367 for layer [False]\n",
      "Iteration: 6800, Loss: 2.988476514816284 for layer [ True]\n",
      "Iteration: 6900, Loss: 8.292176246643066 for layer [False]\n",
      "Iteration: 7000, Loss: 7.194338798522949 for layer [False]\n",
      "Iteration: 7100, Loss: 8.202593803405762 for layer [False]\n",
      "Iteration: 7200, Loss: 5.201351642608643 for layer [False]\n",
      "Iteration: 7300, Loss: 2.468937397003174 for layer [ True]\n",
      "Iteration: 7400, Loss: 3.690342664718628 for layer [False]\n",
      "Iteration: 7500, Loss: 2.597797393798828 for layer [ True]\n",
      "Iteration: 7600, Loss: 1.8546028137207031 for layer [ True]\n",
      "Iteration: 7700, Loss: 3.0553557872772217 for layer [False]\n",
      "Iteration: 7800, Loss: 3.400352716445923 for layer [False]\n",
      "Iteration: 7900, Loss: 1.5281834602355957 for layer [ True]\n",
      "Iteration: 8000, Loss: 1.9503498077392578 for layer [ True]\n",
      "Iteration: 8100, Loss: 1.2520660161972046 for layer [ True]\n",
      "Iteration: 8200, Loss: 0.8390539884567261 for layer [ True]\n",
      "Iteration: 8300, Loss: 2.9994161128997803 for layer [False]\n",
      "Iteration: 8400, Loss: 0.8199017643928528 for layer [ True]\n",
      "Iteration: 8500, Loss: 2.41536283493042 for layer [False]\n",
      "Iteration: 8600, Loss: 6.4105377197265625 for layer [False]\n",
      "Iteration: 8700, Loss: 0.9582894444465637 for layer [ True]\n",
      "Iteration: 8800, Loss: 2.617842674255371 for layer [False]\n",
      "Iteration: 8900, Loss: 5.39314603805542 for layer [False]\n",
      "Iteration: 9000, Loss: 0.6057276725769043 for layer [ True]\n",
      "Iteration: 9100, Loss: 0.5860365033149719 for layer [ True]\n",
      "Iteration: 9200, Loss: 0.8173987865447998 for layer [ True]\n",
      "Iteration: 9300, Loss: 3.7518203258514404 for layer [False]\n",
      "Iteration: 9400, Loss: 2.209899663925171 for layer [False]\n",
      "Iteration: 9500, Loss: 0.4702199697494507 for layer [ True]\n",
      "Iteration: 9600, Loss: 1.8642594814300537 for layer [False]\n",
      "Iteration: 9700, Loss: 0.33050066232681274 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.2891194522380829 for layer [ True]\n",
      "Iteration: 9900, Loss: 1.258974552154541 for layer [False]\n",
      "Iteration: 10000, Loss: 1.9156758785247803 for layer [False]\n",
      "Iteration: 10100, Loss: 0.22627359628677368 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.15722544491291046 for layer [ True]\n",
      "Iteration: 10300, Loss: 2.362316608428955 for layer [False]\n",
      "Iteration: 10400, Loss: 0.23295892775058746 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.1461116373538971 for layer [ True]\n",
      "Iteration: 10600, Loss: 0.9516761302947998 for layer [False]\n",
      "Iteration: 10700, Loss: 0.18890272080898285 for layer [ True]\n",
      "Iteration: 10800, Loss: 0.9391874670982361 for layer [False]\n",
      "Iteration: 10900, Loss: 0.11191034317016602 for layer [ True]\n",
      "Iteration: 11000, Loss: 0.9551820158958435 for layer [False]\n",
      "Iteration: 11100, Loss: 1.4636400938034058 for layer [False]\n",
      "Iteration: 11200, Loss: 0.09689855575561523 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.0742497593164444 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.09207139909267426 for layer [ True]\n",
      "Iteration: 11500, Loss: 0.9374807476997375 for layer [False]\n",
      "Iteration: 11600, Loss: 0.09497162699699402 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.04741775244474411 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.038542281836271286 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.057178299874067307 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.037242475897073746 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.04987320676445961 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.028094371780753136 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.027766644954681396 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.027684420347213745 for layer [ True]\n",
      "Iteration: 12500, Loss: 1.525556206703186 for layer [False]\n",
      "Iteration: 12600, Loss: 0.5856668949127197 for layer [False]\n",
      "Iteration: 12700, Loss: 1.1143746376037598 for layer [False]\n",
      "Iteration: 12800, Loss: 0.022220894694328308 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.0199004877358675 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.010511478409171104 for layer [ True]\n",
      "Iteration: 13100, Loss: 1.4012906551361084 for layer [False]\n",
      "Iteration: 13200, Loss: 0.012161067686975002 for layer [ True]\n",
      "Iteration: 13300, Loss: 0.7471807599067688 for layer [False]\n",
      "Iteration: 13400, Loss: 0.007710193283855915 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.007644232362508774 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.007672654930502176 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.004385360516607761 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.005861536134034395 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.00557800754904747 for layer [ True]\n",
      "Iteration: 14000, Loss: 0.5147806406021118 for layer [False]\n",
      "Iteration: 14100, Loss: 0.0038334503769874573 for layer [ True]\n",
      "Iteration: 14200, Loss: 0.7057147026062012 for layer [False]\n",
      "Iteration: 14300, Loss: 0.0018963116453960538 for layer [ True]\n",
      "Iteration: 14400, Loss: 1.5726064443588257 for layer [False]\n",
      "Iteration: 14500, Loss: 0.5947601795196533 for layer [False]\n",
      "Iteration: 14600, Loss: 0.4636284410953522 for layer [False]\n",
      "Iteration: 14700, Loss: 0.8715588450431824 for layer [False]\n",
      "Iteration: 14800, Loss: 0.8396347761154175 for layer [False]\n",
      "Iteration: 14900, Loss: 0.6266114711761475 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0008248815429396927 for layer [ True]\n",
      "Iteration: 15100, Loss: 0.4791894853115082 for layer [False]\n",
      "Iteration: 15200, Loss: 0.9978339076042175 for layer [False]\n",
      "Iteration: 15300, Loss: 0.8470824956893921 for layer [False]\n",
      "Iteration: 15400, Loss: 0.8372082114219666 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0006880882428959012 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0004037729522679001 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.0006763717392459512 for layer [ True]\n",
      "Iteration: 15800, Loss: 0.8453736901283264 for layer [False]\n",
      "Iteration: 15900, Loss: 0.00026489453739486635 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0003340980038046837 for layer [ True]\n",
      "Iteration: 16100, Loss: 0.84527188539505 for layer [False]\n",
      "Iteration: 16200, Loss: 0.00014917150838300586 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0001324680633842945 for layer [ True]\n",
      "Iteration: 16400, Loss: 1.2322152853012085 for layer [False]\n",
      "Iteration: 16500, Loss: 0.676017165184021 for layer [False]\n",
      "Iteration: 16600, Loss: 0.35184040665626526 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0004508834972511977 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.00028444486088119447 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.00013029322144575417 for layer [ True]\n",
      "Iteration: 17000, Loss: 9.988583042286336e-05 for layer [ True]\n",
      "Iteration: 17100, Loss: 0.9925986528396606 for layer [False]\n",
      "Iteration: 17200, Loss: 1.0717570781707764 for layer [False]\n",
      "Iteration: 17300, Loss: 4.811460530618206e-05 for layer [ True]\n",
      "Iteration: 17400, Loss: 1.0585379600524902 for layer [False]\n",
      "Iteration: 17500, Loss: 1.4598990678787231 for layer [False]\n",
      "Iteration: 17600, Loss: 0.6067089438438416 for layer [False]\n",
      "Iteration: 17700, Loss: 9.718145156512037e-05 for layer [ True]\n",
      "Iteration: 17800, Loss: 0.8297550678253174 for layer [False]\n",
      "Iteration: 17900, Loss: 1.2125191688537598 for layer [False]\n",
      "Iteration: 18000, Loss: 0.8563672304153442 for layer [False]\n",
      "Iteration: 18100, Loss: 5.992120350128971e-05 for layer [ True]\n",
      "Iteration: 18200, Loss: 7.659063703613356e-05 for layer [ True]\n",
      "Iteration: 18300, Loss: 1.119579792022705 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00010010354890255257 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.00010502195073058829 for layer [ True]\n",
      "Iteration: 18600, Loss: 0.8573132753372192 for layer [False]\n",
      "Iteration: 18700, Loss: 0.6344069242477417 for layer [False]\n",
      "Iteration: 18800, Loss: 1.097229242324829 for layer [False]\n",
      "Iteration: 18900, Loss: 0.6600701808929443 for layer [False]\n",
      "Iteration: 19000, Loss: 7.737700070720166e-05 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.3262161016464233 for layer [False]\n",
      "Iteration: 19200, Loss: 0.00022999798238743097 for layer [ True]\n",
      "Iteration: 19300, Loss: 0.8191765546798706 for layer [False]\n",
      "Iteration: 19400, Loss: 0.9584453105926514 for layer [False]\n",
      "Iteration: 19500, Loss: 0.00034067020169459283 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0002031076146522537 for layer [ True]\n",
      "Iteration: 19700, Loss: 0.7159320116043091 for layer [False]\n",
      "Iteration: 19800, Loss: 1.0966538190841675 for layer [False]\n",
      "Iteration: 19900, Loss: 1.053892731666565 for layer [False]\n",
      "Iteration: 20000, Loss: 0.6970365047454834 for layer [False]\n",
      "Iteration: 20100, Loss: 0.33882468938827515 for layer [False]\n",
      "Iteration: 20200, Loss: 0.00020385773677844554 for layer [ True]\n",
      "Iteration: 20300, Loss: 0.735572338104248 for layer [False]\n",
      "Iteration: 20400, Loss: 3.140977423754521e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 6.673343159491196e-05 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.000531094498001039 for layer [ True]\n",
      "Iteration: 20700, Loss: 0.7473122477531433 for layer [False]\n",
      "Iteration: 20800, Loss: 8.580621215514839e-05 for layer [ True]\n",
      "Iteration: 20900, Loss: 2.6521178369875997e-05 for layer [ True]\n",
      "Iteration: 21000, Loss: 0.5335941314697266 for layer [False]\n",
      "Iteration: 21100, Loss: 0.8298884630203247 for layer [False]\n",
      "Iteration: 21200, Loss: 3.167752947774716e-05 for layer [ True]\n",
      "Iteration: 21300, Loss: 0.7295470237731934 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00019018526654690504 for layer [ True]\n",
      "Iteration: 21500, Loss: 0.9618741273880005 for layer [False]\n",
      "Iteration: 21600, Loss: 1.0342440605163574 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0001163077395176515 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00019708949548657984 for layer [ True]\n",
      "Iteration: 21900, Loss: 0.5617921352386475 for layer [False]\n",
      "Iteration: 22000, Loss: 0.625350832939148 for layer [False]\n",
      "Iteration: 22100, Loss: 7.04058475093916e-05 for layer [ True]\n",
      "Iteration: 22200, Loss: 1.737226739351172e-05 for layer [ True]\n",
      "Iteration: 22300, Loss: 0.8385133743286133 for layer [False]\n",
      "Iteration: 22400, Loss: 1.2001656293869019 for layer [False]\n",
      "Iteration: 22500, Loss: 0.42246711254119873 for layer [False]\n",
      "Iteration: 22600, Loss: 7.057750190142542e-05 for layer [ True]\n",
      "Iteration: 22700, Loss: 4.549619188765064e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 8.789872663328424e-05 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0007034080917946994 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00013637362280860543 for layer [ True]\n",
      "Iteration: 23100, Loss: 8.51617514854297e-05 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0002105737803503871 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00029804438236169517 for layer [ True]\n",
      "Iteration: 23400, Loss: 5.830830195918679e-05 for layer [ True]\n",
      "Iteration: 23500, Loss: 2.7305724870529957e-05 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0001689369382802397 for layer [ True]\n",
      "Iteration: 23700, Loss: 0.7123891115188599 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006064266199246049 for layer [ True]\n",
      "Iteration: 23900, Loss: 5.4678574088029563e-05 for layer [ True]\n",
      "Iteration: 24000, Loss: 3.8381247577490285e-05 for layer [ True]\n",
      "Iteration: 24100, Loss: 3.1854273402132094e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 0.7903411984443665 for layer [False]\n",
      "Iteration: 24300, Loss: 0.6015961170196533 for layer [False]\n",
      "Iteration: 24400, Loss: 0.5168365240097046 for layer [False]\n",
      "Iteration: 24500, Loss: 7.769075455144048e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 3.056591594940983e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 1.4607082605361938 for layer [False]\n",
      "Iteration: 24800, Loss: 0.7519532442092896 for layer [False]\n",
      "Iteration: 24900, Loss: 4.037343751406297e-05 for layer [ True]\n",
      "Step 0 | Loss: 0.574181\n",
      "Step 100 | Loss: 0.013194\n",
      "Step 200 | Loss: 0.009996\n",
      "Step 300 | Loss: 0.008637\n",
      "Step 400 | Loss: 0.007605\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2152.37109375 for layer [False]\n",
      "Iteration: 100, Loss: 338.2296447753906 for layer [ True]\n",
      "Iteration: 200, Loss: 2075.4501953125 for layer [False]\n",
      "Iteration: 300, Loss: 1826.7960205078125 for layer [False]\n",
      "Iteration: 400, Loss: 224.20474243164062 for layer [ True]\n",
      "Iteration: 500, Loss: 286.818603515625 for layer [ True]\n",
      "Iteration: 600, Loss: 1601.749755859375 for layer [False]\n",
      "Iteration: 700, Loss: 186.44998168945312 for layer [ True]\n",
      "Iteration: 800, Loss: 1354.751953125 for layer [False]\n",
      "Iteration: 900, Loss: 1165.2724609375 for layer [False]\n",
      "Iteration: 1000, Loss: 122.00373840332031 for layer [ True]\n",
      "Iteration: 1100, Loss: 1067.476806640625 for layer [False]\n",
      "Iteration: 1200, Loss: 1292.0040283203125 for layer [False]\n",
      "Iteration: 1300, Loss: 1120.135009765625 for layer [False]\n",
      "Iteration: 1400, Loss: 138.4385528564453 for layer [ True]\n",
      "Iteration: 1500, Loss: 148.51980590820312 for layer [ True]\n",
      "Iteration: 1600, Loss: 91.1556625366211 for layer [ True]\n",
      "Iteration: 1700, Loss: 119.22136688232422 for layer [ True]\n",
      "Iteration: 1800, Loss: 756.4970092773438 for layer [False]\n",
      "Iteration: 1900, Loss: 78.84435272216797 for layer [ True]\n",
      "Iteration: 2000, Loss: 69.5843276977539 for layer [ True]\n",
      "Iteration: 2100, Loss: 101.11011505126953 for layer [ True]\n",
      "Iteration: 2200, Loss: 74.06761169433594 for layer [ True]\n",
      "Iteration: 2300, Loss: 533.0057983398438 for layer [False]\n",
      "Iteration: 2400, Loss: 63.766117095947266 for layer [ True]\n",
      "Iteration: 2500, Loss: 72.5238037109375 for layer [ True]\n",
      "Iteration: 2600, Loss: 63.70721435546875 for layer [ True]\n",
      "Iteration: 2700, Loss: 52.15715026855469 for layer [ True]\n",
      "Iteration: 2800, Loss: 380.7044982910156 for layer [False]\n",
      "Iteration: 2900, Loss: 321.5110168457031 for layer [False]\n",
      "Iteration: 3000, Loss: 41.36016082763672 for layer [ True]\n",
      "Iteration: 3100, Loss: 282.12286376953125 for layer [False]\n",
      "Iteration: 3200, Loss: 292.069580078125 for layer [False]\n",
      "Iteration: 3300, Loss: 208.446533203125 for layer [False]\n",
      "Iteration: 3400, Loss: 29.450729370117188 for layer [ True]\n",
      "Iteration: 3500, Loss: 260.8702392578125 for layer [False]\n",
      "Iteration: 3600, Loss: 30.413801193237305 for layer [ True]\n",
      "Iteration: 3700, Loss: 204.887939453125 for layer [False]\n",
      "Iteration: 3800, Loss: 150.25567626953125 for layer [False]\n",
      "Iteration: 3900, Loss: 20.963775634765625 for layer [ True]\n",
      "Iteration: 4000, Loss: 25.735519409179688 for layer [ True]\n",
      "Iteration: 4100, Loss: 18.523042678833008 for layer [ True]\n",
      "Iteration: 4200, Loss: 31.778377532958984 for layer [ True]\n",
      "Iteration: 4300, Loss: 21.423742294311523 for layer [ True]\n",
      "Iteration: 4400, Loss: 17.63212776184082 for layer [ True]\n",
      "Iteration: 4500, Loss: 23.128137588500977 for layer [ True]\n",
      "Iteration: 4600, Loss: 13.26041030883789 for layer [ True]\n",
      "Iteration: 4700, Loss: 88.64836883544922 for layer [False]\n",
      "Iteration: 4800, Loss: 58.09370422363281 for layer [False]\n",
      "Iteration: 4900, Loss: 70.80606842041016 for layer [False]\n",
      "Iteration: 5000, Loss: 15.864405632019043 for layer [ True]\n",
      "Iteration: 5100, Loss: 10.063727378845215 for layer [ True]\n",
      "Iteration: 5200, Loss: 52.681644439697266 for layer [False]\n",
      "Iteration: 5300, Loss: 12.850775718688965 for layer [ True]\n",
      "Iteration: 5400, Loss: 41.60028839111328 for layer [False]\n",
      "Iteration: 5500, Loss: 10.519906044006348 for layer [ True]\n",
      "Iteration: 5600, Loss: 11.061214447021484 for layer [ True]\n",
      "Iteration: 5700, Loss: 34.63673782348633 for layer [False]\n",
      "Iteration: 5800, Loss: 30.655563354492188 for layer [False]\n",
      "Iteration: 5900, Loss: 23.24751091003418 for layer [False]\n",
      "Iteration: 6000, Loss: 5.7736406326293945 for layer [ True]\n",
      "Iteration: 6100, Loss: 19.046369552612305 for layer [False]\n",
      "Iteration: 6200, Loss: 18.106266021728516 for layer [False]\n",
      "Iteration: 6300, Loss: 14.683737754821777 for layer [False]\n",
      "Iteration: 6400, Loss: 7.059682369232178 for layer [ True]\n",
      "Iteration: 6500, Loss: 2.8882267475128174 for layer [ True]\n",
      "Iteration: 6600, Loss: 4.355186462402344 for layer [ True]\n",
      "Iteration: 6700, Loss: 12.046910285949707 for layer [False]\n",
      "Iteration: 6800, Loss: 2.6913254261016846 for layer [ True]\n",
      "Iteration: 6900, Loss: 10.44027328491211 for layer [False]\n",
      "Iteration: 7000, Loss: 7.474589824676514 for layer [False]\n",
      "Iteration: 7100, Loss: 11.267382621765137 for layer [False]\n",
      "Iteration: 7200, Loss: 5.232687950134277 for layer [False]\n",
      "Iteration: 7300, Loss: 3.08201265335083 for layer [ True]\n",
      "Iteration: 7400, Loss: 4.271537780761719 for layer [False]\n",
      "Iteration: 7500, Loss: 2.4639732837677 for layer [ True]\n",
      "Iteration: 7600, Loss: 2.0041584968566895 for layer [ True]\n",
      "Iteration: 7700, Loss: 3.676499605178833 for layer [False]\n",
      "Iteration: 7800, Loss: 5.175117492675781 for layer [False]\n",
      "Iteration: 7900, Loss: 2.0400140285491943 for layer [ True]\n",
      "Iteration: 8000, Loss: 2.1523828506469727 for layer [ True]\n",
      "Iteration: 8100, Loss: 1.3815991878509521 for layer [ True]\n",
      "Iteration: 8200, Loss: 1.0855886936187744 for layer [ True]\n",
      "Iteration: 8300, Loss: 3.962480306625366 for layer [False]\n",
      "Iteration: 8400, Loss: 0.9951134920120239 for layer [ True]\n",
      "Iteration: 8500, Loss: 1.7677358388900757 for layer [False]\n",
      "Iteration: 8600, Loss: 2.4023609161376953 for layer [False]\n",
      "Iteration: 8700, Loss: 0.9532453417778015 for layer [ True]\n",
      "Iteration: 8800, Loss: 2.0496127605438232 for layer [False]\n",
      "Iteration: 8900, Loss: 3.7916018962860107 for layer [False]\n",
      "Iteration: 9000, Loss: 0.5065168738365173 for layer [ True]\n",
      "Iteration: 9100, Loss: 0.6989020109176636 for layer [ True]\n",
      "Iteration: 9200, Loss: 0.8825612664222717 for layer [ True]\n",
      "Iteration: 9300, Loss: 1.9088642597198486 for layer [False]\n",
      "Iteration: 9400, Loss: 1.9458189010620117 for layer [False]\n",
      "Iteration: 9500, Loss: 0.5088539719581604 for layer [ True]\n",
      "Iteration: 9600, Loss: 1.9140853881835938 for layer [False]\n",
      "Iteration: 9700, Loss: 0.3969486951828003 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.3058930039405823 for layer [ True]\n",
      "Iteration: 9900, Loss: 1.480708122253418 for layer [False]\n",
      "Iteration: 10000, Loss: 1.3020925521850586 for layer [False]\n",
      "Iteration: 10100, Loss: 0.30027782917022705 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.1762765645980835 for layer [ True]\n",
      "Iteration: 10300, Loss: 2.84130859375 for layer [False]\n",
      "Iteration: 10400, Loss: 0.2290678769350052 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.15864905714988708 for layer [ True]\n",
      "Iteration: 10600, Loss: 1.1644065380096436 for layer [False]\n",
      "Iteration: 10700, Loss: 0.20339547097682953 for layer [ True]\n",
      "Iteration: 10800, Loss: 0.8400834202766418 for layer [False]\n",
      "Iteration: 10900, Loss: 0.16111205518245697 for layer [ True]\n",
      "Iteration: 11000, Loss: 1.611637830734253 for layer [False]\n",
      "Iteration: 11100, Loss: 0.9842439293861389 for layer [False]\n",
      "Iteration: 11200, Loss: 0.11945701390504837 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.09447019547224045 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.10818168520927429 for layer [ True]\n",
      "Iteration: 11500, Loss: 0.8007693290710449 for layer [False]\n",
      "Iteration: 11600, Loss: 0.11423952132463455 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.06771061569452286 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.05099744349718094 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.07291898876428604 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.05063261836767197 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.05217750370502472 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.03637447953224182 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.04326827824115753 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.038769397884607315 for layer [ True]\n",
      "Iteration: 12500, Loss: 0.9826870560646057 for layer [False]\n",
      "Iteration: 12600, Loss: 1.2347623109817505 for layer [False]\n",
      "Iteration: 12700, Loss: 0.7440332770347595 for layer [False]\n",
      "Iteration: 12800, Loss: 0.0321684367954731 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.02555478736758232 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.018331408500671387 for layer [ True]\n",
      "Iteration: 13100, Loss: 1.4474345445632935 for layer [False]\n",
      "Iteration: 13200, Loss: 0.015803469344973564 for layer [ True]\n",
      "Iteration: 13300, Loss: 0.6418529152870178 for layer [False]\n",
      "Iteration: 13400, Loss: 0.011458083987236023 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.011634597554802895 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.009690569713711739 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.005328318569809198 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.01085562352091074 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.007105861324816942 for layer [ True]\n",
      "Iteration: 14000, Loss: 0.4454057514667511 for layer [False]\n",
      "Iteration: 14100, Loss: 0.010136991739273071 for layer [ True]\n",
      "Iteration: 14200, Loss: 1.839711308479309 for layer [False]\n",
      "Iteration: 14300, Loss: 0.003528611036017537 for layer [ True]\n",
      "Iteration: 14400, Loss: 1.1084606647491455 for layer [False]\n",
      "Iteration: 14500, Loss: 0.7083503007888794 for layer [False]\n",
      "Iteration: 14600, Loss: 0.8528679609298706 for layer [False]\n",
      "Iteration: 14700, Loss: 0.6957640647888184 for layer [False]\n",
      "Iteration: 14800, Loss: 1.5397748947143555 for layer [False]\n",
      "Iteration: 14900, Loss: 0.7497293949127197 for layer [False]\n",
      "Iteration: 15000, Loss: 0.001213211566209793 for layer [ True]\n",
      "Iteration: 15100, Loss: 0.8628861308097839 for layer [False]\n",
      "Iteration: 15200, Loss: 0.7151625752449036 for layer [False]\n",
      "Iteration: 15300, Loss: 0.4950467050075531 for layer [False]\n",
      "Iteration: 15400, Loss: 0.820027768611908 for layer [False]\n",
      "Iteration: 15500, Loss: 0.000593674776609987 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0008597826235927641 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.0007916513131931424 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.0259181261062622 for layer [False]\n",
      "Iteration: 15900, Loss: 0.00037543452344834805 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0009059930453076959 for layer [ True]\n",
      "Iteration: 16100, Loss: 1.236565113067627 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0002035736251855269 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.00034652077010832727 for layer [ True]\n",
      "Iteration: 16400, Loss: 0.5696351528167725 for layer [False]\n",
      "Iteration: 16500, Loss: 2.007195472717285 for layer [False]\n",
      "Iteration: 16600, Loss: 1.163205623626709 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0003418192209210247 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.00041908619459718466 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0001605707366252318 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.00021073620882816613 for layer [ True]\n",
      "Iteration: 17100, Loss: 0.38800692558288574 for layer [False]\n",
      "Iteration: 17200, Loss: 0.5071049928665161 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0002720697084441781 for layer [ True]\n",
      "Iteration: 17400, Loss: 0.7733837962150574 for layer [False]\n",
      "Iteration: 17500, Loss: 1.5516167879104614 for layer [False]\n",
      "Iteration: 17600, Loss: 0.693369448184967 for layer [False]\n",
      "Iteration: 17700, Loss: 0.00020978233078494668 for layer [ True]\n",
      "Iteration: 17800, Loss: 0.93829345703125 for layer [False]\n",
      "Iteration: 17900, Loss: 1.0111091136932373 for layer [False]\n",
      "Iteration: 18000, Loss: 1.025541067123413 for layer [False]\n",
      "Iteration: 18100, Loss: 9.602236968930811e-05 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.00016473415598738939 for layer [ True]\n",
      "Iteration: 18300, Loss: 1.3908592462539673 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00013149534061085433 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.00016857559967320412 for layer [ True]\n",
      "Iteration: 18600, Loss: 0.663684070110321 for layer [False]\n",
      "Iteration: 18700, Loss: 0.8281406164169312 for layer [False]\n",
      "Iteration: 18800, Loss: 0.8999329209327698 for layer [False]\n",
      "Iteration: 18900, Loss: 0.4835238456726074 for layer [False]\n",
      "Iteration: 19000, Loss: 5.398986832005903e-05 for layer [ True]\n",
      "Iteration: 19100, Loss: 0.6681758165359497 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0002840621455106884 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.5153348445892334 for layer [False]\n",
      "Iteration: 19400, Loss: 1.1557053327560425 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0001383865746902302 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00017565302550792694 for layer [ True]\n",
      "Iteration: 19700, Loss: 0.9587163925170898 for layer [False]\n",
      "Iteration: 19800, Loss: 0.5955385565757751 for layer [False]\n",
      "Iteration: 19900, Loss: 1.034040927886963 for layer [False]\n",
      "Iteration: 20000, Loss: 1.0077342987060547 for layer [False]\n",
      "Iteration: 20100, Loss: 0.9022243022918701 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0007468494004569948 for layer [ True]\n",
      "Iteration: 20300, Loss: 0.9280635118484497 for layer [False]\n",
      "Iteration: 20400, Loss: 1.0683173059078399e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 2.2845362764201127e-05 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0010546429548412561 for layer [ True]\n",
      "Iteration: 20700, Loss: 0.7790906429290771 for layer [False]\n",
      "Iteration: 20800, Loss: 1.4595804714190308e-05 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.00028671399923041463 for layer [ True]\n",
      "Iteration: 21000, Loss: 0.8835265040397644 for layer [False]\n",
      "Iteration: 21100, Loss: 0.8645020127296448 for layer [False]\n",
      "Iteration: 21200, Loss: 4.401174373924732e-05 for layer [ True]\n",
      "Iteration: 21300, Loss: 0.7327548265457153 for layer [False]\n",
      "Iteration: 21400, Loss: 5.9891277487622574e-05 for layer [ True]\n",
      "Iteration: 21500, Loss: 0.6973826289176941 for layer [False]\n",
      "Iteration: 21600, Loss: 0.6597161889076233 for layer [False]\n",
      "Iteration: 21700, Loss: 4.98989102197811e-05 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00017970247427001595 for layer [ True]\n",
      "Iteration: 21900, Loss: 0.48143500089645386 for layer [False]\n",
      "Iteration: 22000, Loss: 1.4421130418777466 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00016229708853643388 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0001379480236209929 for layer [ True]\n",
      "Iteration: 22300, Loss: 0.7626033425331116 for layer [False]\n",
      "Iteration: 22400, Loss: 1.0543010234832764 for layer [False]\n",
      "Iteration: 22500, Loss: 0.5469990968704224 for layer [False]\n",
      "Iteration: 22600, Loss: 8.355233876500279e-05 for layer [ True]\n",
      "Iteration: 22700, Loss: 1.517811597295804e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 7.986523996805772e-05 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.00046045935596339405 for layer [ True]\n",
      "Iteration: 23000, Loss: 3.700043089338578e-05 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00025685448781587183 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00021731038577854633 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00010106734407600015 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00010180754179600626 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00011984179582213983 for layer [ True]\n",
      "Iteration: 23600, Loss: 8.136081305565313e-05 for layer [ True]\n",
      "Iteration: 23700, Loss: 0.987506091594696 for layer [False]\n",
      "Iteration: 23800, Loss: 0.00023751384287606925 for layer [ True]\n",
      "Iteration: 23900, Loss: 3.793308133026585e-05 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0002601813175715506 for layer [ True]\n",
      "Iteration: 24100, Loss: 2.3009091819403693e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 0.4651130437850952 for layer [False]\n",
      "Iteration: 24300, Loss: 0.5458223223686218 for layer [False]\n",
      "Iteration: 24400, Loss: 1.0889549255371094 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0002939500263892114 for layer [ True]\n",
      "Iteration: 24600, Loss: 7.01030285199522e-06 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.2928683757781982 for layer [False]\n",
      "Iteration: 24800, Loss: 1.2100046873092651 for layer [False]\n",
      "Iteration: 24900, Loss: 9.908834908856079e-05 for layer [ True]\n",
      "Step 500 | Loss: 0.006917\n",
      "Step 600 | Loss: 0.005629\n",
      "Step 700 | Loss: 0.004876\n",
      "Step 800 | Loss: 0.004342\n",
      "Step 900 | Loss: 0.003992\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2127.548583984375 for layer [False]\n",
      "Iteration: 100, Loss: 350.83709716796875 for layer [ True]\n",
      "Iteration: 200, Loss: 2057.836669921875 for layer [False]\n",
      "Iteration: 300, Loss: 1785.402099609375 for layer [False]\n",
      "Iteration: 400, Loss: 224.36578369140625 for layer [ True]\n",
      "Iteration: 500, Loss: 287.7435607910156 for layer [ True]\n",
      "Iteration: 600, Loss: 1571.88525390625 for layer [False]\n",
      "Iteration: 700, Loss: 197.53622436523438 for layer [ True]\n",
      "Iteration: 800, Loss: 1338.5030517578125 for layer [False]\n",
      "Iteration: 900, Loss: 1139.5838623046875 for layer [False]\n",
      "Iteration: 1000, Loss: 123.01700592041016 for layer [ True]\n",
      "Iteration: 1100, Loss: 1053.08203125 for layer [False]\n",
      "Iteration: 1200, Loss: 1254.4017333984375 for layer [False]\n",
      "Iteration: 1300, Loss: 1101.412109375 for layer [False]\n",
      "Iteration: 1400, Loss: 145.58677673339844 for layer [ True]\n",
      "Iteration: 1500, Loss: 170.03468322753906 for layer [ True]\n",
      "Iteration: 1600, Loss: 94.98219299316406 for layer [ True]\n",
      "Iteration: 1700, Loss: 117.11181640625 for layer [ True]\n",
      "Iteration: 1800, Loss: 730.7282104492188 for layer [False]\n",
      "Iteration: 1900, Loss: 80.7620620727539 for layer [ True]\n",
      "Iteration: 2000, Loss: 78.65277862548828 for layer [ True]\n",
      "Iteration: 2100, Loss: 114.05471801757812 for layer [ True]\n",
      "Iteration: 2200, Loss: 81.63566589355469 for layer [ True]\n",
      "Iteration: 2300, Loss: 526.03173828125 for layer [False]\n",
      "Iteration: 2400, Loss: 77.9642333984375 for layer [ True]\n",
      "Iteration: 2500, Loss: 104.62456512451172 for layer [ True]\n",
      "Iteration: 2600, Loss: 72.26728057861328 for layer [ True]\n",
      "Iteration: 2700, Loss: 67.75050354003906 for layer [ True]\n",
      "Iteration: 2800, Loss: 375.6290588378906 for layer [False]\n",
      "Iteration: 2900, Loss: 318.16241455078125 for layer [False]\n",
      "Iteration: 3000, Loss: 53.25640106201172 for layer [ True]\n",
      "Iteration: 3100, Loss: 267.47406005859375 for layer [False]\n",
      "Iteration: 3200, Loss: 277.0267333984375 for layer [False]\n",
      "Iteration: 3300, Loss: 196.61648559570312 for layer [False]\n",
      "Iteration: 3400, Loss: 47.40576934814453 for layer [ True]\n",
      "Iteration: 3500, Loss: 250.21534729003906 for layer [False]\n",
      "Iteration: 3600, Loss: 39.7706413269043 for layer [ True]\n",
      "Iteration: 3700, Loss: 195.1658172607422 for layer [False]\n",
      "Iteration: 3800, Loss: 145.7013397216797 for layer [False]\n",
      "Iteration: 3900, Loss: 26.953176498413086 for layer [ True]\n",
      "Iteration: 4000, Loss: 33.97476577758789 for layer [ True]\n",
      "Iteration: 4100, Loss: 24.65652847290039 for layer [ True]\n",
      "Iteration: 4200, Loss: 45.62024688720703 for layer [ True]\n",
      "Iteration: 4300, Loss: 30.511884689331055 for layer [ True]\n",
      "Iteration: 4400, Loss: 22.704862594604492 for layer [ True]\n",
      "Iteration: 4500, Loss: 33.617740631103516 for layer [ True]\n",
      "Iteration: 4600, Loss: 21.145038604736328 for layer [ True]\n",
      "Iteration: 4700, Loss: 84.38288879394531 for layer [False]\n",
      "Iteration: 4800, Loss: 57.22874450683594 for layer [False]\n",
      "Iteration: 4900, Loss: 69.01171112060547 for layer [False]\n",
      "Iteration: 5000, Loss: 20.57362174987793 for layer [ True]\n",
      "Iteration: 5100, Loss: 14.045804977416992 for layer [ True]\n",
      "Iteration: 5200, Loss: 51.22099304199219 for layer [False]\n",
      "Iteration: 5300, Loss: 15.221858024597168 for layer [ True]\n",
      "Iteration: 5400, Loss: 38.30047607421875 for layer [False]\n",
      "Iteration: 5500, Loss: 13.67659854888916 for layer [ True]\n",
      "Iteration: 5600, Loss: 15.667912483215332 for layer [ True]\n",
      "Iteration: 5700, Loss: 32.85097122192383 for layer [False]\n",
      "Iteration: 5800, Loss: 29.23931884765625 for layer [False]\n",
      "Iteration: 5900, Loss: 21.802215576171875 for layer [False]\n",
      "Iteration: 6000, Loss: 8.1026611328125 for layer [ True]\n",
      "Iteration: 6100, Loss: 16.414894104003906 for layer [False]\n",
      "Iteration: 6200, Loss: 15.991939544677734 for layer [False]\n",
      "Iteration: 6300, Loss: 14.336188316345215 for layer [False]\n",
      "Iteration: 6400, Loss: 9.628561019897461 for layer [ True]\n",
      "Iteration: 6500, Loss: 4.549300670623779 for layer [ True]\n",
      "Iteration: 6600, Loss: 6.463816165924072 for layer [ True]\n",
      "Iteration: 6700, Loss: 13.247044563293457 for layer [False]\n",
      "Iteration: 6800, Loss: 3.6135733127593994 for layer [ True]\n",
      "Iteration: 6900, Loss: 10.02713680267334 for layer [False]\n",
      "Iteration: 7000, Loss: 6.89958381652832 for layer [False]\n",
      "Iteration: 7100, Loss: 9.91134262084961 for layer [False]\n",
      "Iteration: 7200, Loss: 6.094607830047607 for layer [False]\n",
      "Iteration: 7300, Loss: 4.035912990570068 for layer [ True]\n",
      "Iteration: 7400, Loss: 5.411121845245361 for layer [False]\n",
      "Iteration: 7500, Loss: 3.2172372341156006 for layer [ True]\n",
      "Iteration: 7600, Loss: 2.551771402359009 for layer [ True]\n",
      "Iteration: 7700, Loss: 5.198600769042969 for layer [False]\n",
      "Iteration: 7800, Loss: 5.464221000671387 for layer [False]\n",
      "Iteration: 7900, Loss: 2.741921901702881 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.0231382846832275 for layer [ True]\n",
      "Iteration: 8100, Loss: 1.717833399772644 for layer [ True]\n",
      "Iteration: 8200, Loss: 1.43180251121521 for layer [ True]\n",
      "Iteration: 8300, Loss: 4.455749034881592 for layer [False]\n",
      "Iteration: 8400, Loss: 1.278320550918579 for layer [ True]\n",
      "Iteration: 8500, Loss: 2.0109448432922363 for layer [False]\n",
      "Iteration: 8600, Loss: 2.606102466583252 for layer [False]\n",
      "Iteration: 8700, Loss: 1.4246716499328613 for layer [ True]\n",
      "Iteration: 8800, Loss: 2.5254716873168945 for layer [False]\n",
      "Iteration: 8900, Loss: 5.164915084838867 for layer [False]\n",
      "Iteration: 9000, Loss: 0.6655421257019043 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.0075637102127075 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.2603157758712769 for layer [ True]\n",
      "Iteration: 9300, Loss: 2.034738302230835 for layer [False]\n",
      "Iteration: 9400, Loss: 2.3843984603881836 for layer [False]\n",
      "Iteration: 9500, Loss: 0.718296468257904 for layer [ True]\n",
      "Iteration: 9600, Loss: 2.5611412525177 for layer [False]\n",
      "Iteration: 9700, Loss: 0.6025049686431885 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.47998151183128357 for layer [ True]\n",
      "Iteration: 9900, Loss: 2.5682907104492188 for layer [False]\n",
      "Iteration: 10000, Loss: 1.6897368431091309 for layer [False]\n",
      "Iteration: 10100, Loss: 0.4693048894405365 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.2655637264251709 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.0764150619506836 for layer [False]\n",
      "Iteration: 10400, Loss: 0.30166080594062805 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.23210935294628143 for layer [ True]\n",
      "Iteration: 10600, Loss: 1.7546955347061157 for layer [False]\n",
      "Iteration: 10700, Loss: 0.2783128023147583 for layer [ True]\n",
      "Iteration: 10800, Loss: 1.9893229007720947 for layer [False]\n",
      "Iteration: 10900, Loss: 0.2465665638446808 for layer [ True]\n",
      "Iteration: 11000, Loss: 2.391914129257202 for layer [False]\n",
      "Iteration: 11100, Loss: 1.8959683179855347 for layer [False]\n",
      "Iteration: 11200, Loss: 0.1758175939321518 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.11600151658058167 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.1485038548707962 for layer [ True]\n",
      "Iteration: 11500, Loss: 1.3081905841827393 for layer [False]\n",
      "Iteration: 11600, Loss: 0.18041303753852844 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.0952857956290245 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.09074286371469498 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.10664805769920349 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.058425143361091614 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.06802317500114441 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.04965559393167496 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.06185257434844971 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.0534830167889595 for layer [ True]\n",
      "Iteration: 12500, Loss: 1.0403915643692017 for layer [False]\n",
      "Iteration: 12600, Loss: 1.6418442726135254 for layer [False]\n",
      "Iteration: 12700, Loss: 1.0548789501190186 for layer [False]\n",
      "Iteration: 12800, Loss: 0.04054434224963188 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.032088883221149445 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.02136983349919319 for layer [ True]\n",
      "Iteration: 13100, Loss: 1.5816439390182495 for layer [False]\n",
      "Iteration: 13200, Loss: 0.02399803325533867 for layer [ True]\n",
      "Iteration: 13300, Loss: 0.8376071453094482 for layer [False]\n",
      "Iteration: 13400, Loss: 0.013400113210082054 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.014659738168120384 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.012188529595732689 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.007172498386353254 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.013856528326869011 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.010319718159735203 for layer [ True]\n",
      "Iteration: 14000, Loss: 0.8118476867675781 for layer [False]\n",
      "Iteration: 14100, Loss: 0.01287656370550394 for layer [ True]\n",
      "Iteration: 14200, Loss: 2.8691928386688232 for layer [False]\n",
      "Iteration: 14300, Loss: 0.004666824359446764 for layer [ True]\n",
      "Iteration: 14400, Loss: 1.5540505647659302 for layer [False]\n",
      "Iteration: 14500, Loss: 0.8477702736854553 for layer [False]\n",
      "Iteration: 14600, Loss: 1.495005488395691 for layer [False]\n",
      "Iteration: 14700, Loss: 1.068074107170105 for layer [False]\n",
      "Iteration: 14800, Loss: 1.4828298091888428 for layer [False]\n",
      "Iteration: 14900, Loss: 1.2340927124023438 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0018415733939036727 for layer [ True]\n",
      "Iteration: 15100, Loss: 0.9023170471191406 for layer [False]\n",
      "Iteration: 15200, Loss: 1.2920368909835815 for layer [False]\n",
      "Iteration: 15300, Loss: 0.9465963244438171 for layer [False]\n",
      "Iteration: 15400, Loss: 0.834652841091156 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0013546347618103027 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0015005950117483735 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.0013019740581512451 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.1069673299789429 for layer [False]\n",
      "Iteration: 15900, Loss: 0.000776484317611903 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0011482973350211978 for layer [ True]\n",
      "Iteration: 16100, Loss: 1.4478423595428467 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0004136526840738952 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0008872441248968244 for layer [ True]\n",
      "Iteration: 16400, Loss: 0.7148151397705078 for layer [False]\n",
      "Iteration: 16500, Loss: 4.05748987197876 for layer [False]\n",
      "Iteration: 16600, Loss: 1.5150022506713867 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0002852840116247535 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0006928373477421701 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.00041663332376629114 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0006189705454744399 for layer [ True]\n",
      "Iteration: 17100, Loss: 0.5496456623077393 for layer [False]\n",
      "Iteration: 17200, Loss: 0.9972916841506958 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0002686384250409901 for layer [ True]\n",
      "Iteration: 17400, Loss: 0.896373450756073 for layer [False]\n",
      "Iteration: 17500, Loss: 1.9544706344604492 for layer [False]\n",
      "Iteration: 17600, Loss: 0.9162369966506958 for layer [False]\n",
      "Iteration: 17700, Loss: 0.00023354605946224183 for layer [ True]\n",
      "Iteration: 17800, Loss: 1.2125746011734009 for layer [False]\n",
      "Iteration: 17900, Loss: 1.7732810974121094 for layer [False]\n",
      "Iteration: 18000, Loss: 1.0241279602050781 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0003057171998079866 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.00037019941373728216 for layer [ True]\n",
      "Iteration: 18300, Loss: 2.5367789268493652 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00015214388258755207 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.00019609883020166308 for layer [ True]\n",
      "Iteration: 18600, Loss: 0.9328628182411194 for layer [False]\n",
      "Iteration: 18700, Loss: 0.8560068607330322 for layer [False]\n",
      "Iteration: 18800, Loss: 1.1219205856323242 for layer [False]\n",
      "Iteration: 18900, Loss: 0.739073634147644 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00022136903135105968 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.1645375490188599 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0002824818075168878 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.6599897146224976 for layer [False]\n",
      "Iteration: 19400, Loss: 1.5198092460632324 for layer [False]\n",
      "Iteration: 19500, Loss: 0.00028446223586797714 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00042322149965912104 for layer [ True]\n",
      "Iteration: 19700, Loss: 1.421236515045166 for layer [False]\n",
      "Iteration: 19800, Loss: 0.8074929714202881 for layer [False]\n",
      "Iteration: 19900, Loss: 0.9510427713394165 for layer [False]\n",
      "Iteration: 20000, Loss: 1.2112948894500732 for layer [False]\n",
      "Iteration: 20100, Loss: 1.3861737251281738 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0005750448326580226 for layer [ True]\n",
      "Iteration: 20300, Loss: 1.382883906364441 for layer [False]\n",
      "Iteration: 20400, Loss: 4.949565845890902e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00011615817493293434 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.000634792959317565 for layer [ True]\n",
      "Iteration: 20700, Loss: 0.8939289450645447 for layer [False]\n",
      "Iteration: 20800, Loss: 5.3986863349564373e-05 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0003003313613589853 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.1633634567260742 for layer [False]\n",
      "Iteration: 21100, Loss: 1.885087251663208 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00029368745163083076 for layer [ True]\n",
      "Iteration: 21300, Loss: 1.2800023555755615 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00016865276847966015 for layer [ True]\n",
      "Iteration: 21500, Loss: 0.6534538865089417 for layer [False]\n",
      "Iteration: 21600, Loss: 1.0963143110275269 for layer [False]\n",
      "Iteration: 21700, Loss: 9.842556755756959e-05 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00033147784415632486 for layer [ True]\n",
      "Iteration: 21900, Loss: 0.6476640701293945 for layer [False]\n",
      "Iteration: 22000, Loss: 1.3752719163894653 for layer [False]\n",
      "Iteration: 22100, Loss: 9.558266901876777e-05 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0004825102514587343 for layer [ True]\n",
      "Iteration: 22300, Loss: 1.1819206476211548 for layer [False]\n",
      "Iteration: 22400, Loss: 1.4116978645324707 for layer [False]\n",
      "Iteration: 22500, Loss: 0.6708651781082153 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00010219673276878893 for layer [ True]\n",
      "Iteration: 22700, Loss: 5.384134783525951e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0002469909086357802 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0008365364046767354 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0002592825621832162 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00028863188344985247 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00014959268446546048 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0001461185165680945 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00023245162446983159 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0001698014821158722 for layer [ True]\n",
      "Iteration: 23600, Loss: 6.592478894162923e-05 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.311540961265564 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0004855807055719197 for layer [ True]\n",
      "Iteration: 23900, Loss: 5.737683750339784e-05 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0004538113425951451 for layer [ True]\n",
      "Iteration: 24100, Loss: 4.9575774028198794e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 0.5427618026733398 for layer [False]\n",
      "Iteration: 24300, Loss: 0.7477845549583435 for layer [False]\n",
      "Iteration: 24400, Loss: 1.271315336227417 for layer [False]\n",
      "Iteration: 24500, Loss: 7.838218152755871e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 8.009965677047148e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.64723539352417 for layer [False]\n",
      "Iteration: 24800, Loss: 1.77873694896698 for layer [False]\n",
      "Iteration: 24900, Loss: 4.0967734094010666e-05 for layer [ True]\n",
      "Step 1000 | Loss: 0.003751\n",
      "Step 1100 | Loss: 0.003405\n",
      "Step 1200 | Loss: 0.003148\n",
      "Step 1300 | Loss: 0.002977\n",
      "Step 1400 | Loss: 0.002825\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2093.600341796875 for layer [False]\n",
      "Iteration: 100, Loss: 342.9135437011719 for layer [ True]\n",
      "Iteration: 200, Loss: 2010.9783935546875 for layer [False]\n",
      "Iteration: 300, Loss: 1759.20751953125 for layer [False]\n",
      "Iteration: 400, Loss: 223.21200561523438 for layer [ True]\n",
      "Iteration: 500, Loss: 283.4387512207031 for layer [ True]\n",
      "Iteration: 600, Loss: 1536.165283203125 for layer [False]\n",
      "Iteration: 700, Loss: 204.19384765625 for layer [ True]\n",
      "Iteration: 800, Loss: 1311.11669921875 for layer [False]\n",
      "Iteration: 900, Loss: 1099.99560546875 for layer [False]\n",
      "Iteration: 1000, Loss: 120.59261322021484 for layer [ True]\n",
      "Iteration: 1100, Loss: 1034.3978271484375 for layer [False]\n",
      "Iteration: 1200, Loss: 1215.3155517578125 for layer [False]\n",
      "Iteration: 1300, Loss: 1075.3043212890625 for layer [False]\n",
      "Iteration: 1400, Loss: 143.6549072265625 for layer [ True]\n",
      "Iteration: 1500, Loss: 181.42047119140625 for layer [ True]\n",
      "Iteration: 1600, Loss: 96.23200988769531 for layer [ True]\n",
      "Iteration: 1700, Loss: 112.3370361328125 for layer [ True]\n",
      "Iteration: 1800, Loss: 712.9916381835938 for layer [False]\n",
      "Iteration: 1900, Loss: 83.02288818359375 for layer [ True]\n",
      "Iteration: 2000, Loss: 78.47724914550781 for layer [ True]\n",
      "Iteration: 2100, Loss: 111.1827621459961 for layer [ True]\n",
      "Iteration: 2200, Loss: 84.42327117919922 for layer [ True]\n",
      "Iteration: 2300, Loss: 505.4851989746094 for layer [False]\n",
      "Iteration: 2400, Loss: 76.6026611328125 for layer [ True]\n",
      "Iteration: 2500, Loss: 112.96803283691406 for layer [ True]\n",
      "Iteration: 2600, Loss: 74.3961181640625 for layer [ True]\n",
      "Iteration: 2700, Loss: 69.73114776611328 for layer [ True]\n",
      "Iteration: 2800, Loss: 365.01007080078125 for layer [False]\n",
      "Iteration: 2900, Loss: 304.6321716308594 for layer [False]\n",
      "Iteration: 3000, Loss: 54.41056442260742 for layer [ True]\n",
      "Iteration: 3100, Loss: 252.06881713867188 for layer [False]\n",
      "Iteration: 3200, Loss: 267.0968933105469 for layer [False]\n",
      "Iteration: 3300, Loss: 184.63336181640625 for layer [False]\n",
      "Iteration: 3400, Loss: 51.59630584716797 for layer [ True]\n",
      "Iteration: 3500, Loss: 235.99891662597656 for layer [False]\n",
      "Iteration: 3600, Loss: 41.33380889892578 for layer [ True]\n",
      "Iteration: 3700, Loss: 183.50152587890625 for layer [False]\n",
      "Iteration: 3800, Loss: 135.52838134765625 for layer [False]\n",
      "Iteration: 3900, Loss: 28.614627838134766 for layer [ True]\n",
      "Iteration: 4000, Loss: 34.768165588378906 for layer [ True]\n",
      "Iteration: 4100, Loss: 27.152496337890625 for layer [ True]\n",
      "Iteration: 4200, Loss: 45.50960159301758 for layer [ True]\n",
      "Iteration: 4300, Loss: 31.769094467163086 for layer [ True]\n",
      "Iteration: 4400, Loss: 22.775537490844727 for layer [ True]\n",
      "Iteration: 4500, Loss: 33.69172668457031 for layer [ True]\n",
      "Iteration: 4600, Loss: 22.363780975341797 for layer [ True]\n",
      "Iteration: 4700, Loss: 78.55548095703125 for layer [False]\n",
      "Iteration: 4800, Loss: 52.18302917480469 for layer [False]\n",
      "Iteration: 4900, Loss: 65.10694885253906 for layer [False]\n",
      "Iteration: 5000, Loss: 23.910486221313477 for layer [ True]\n",
      "Iteration: 5100, Loss: 13.517534255981445 for layer [ True]\n",
      "Iteration: 5200, Loss: 47.42386245727539 for layer [False]\n",
      "Iteration: 5300, Loss: 14.44172191619873 for layer [ True]\n",
      "Iteration: 5400, Loss: 35.04962921142578 for layer [False]\n",
      "Iteration: 5500, Loss: 14.422025680541992 for layer [ True]\n",
      "Iteration: 5600, Loss: 15.637874603271484 for layer [ True]\n",
      "Iteration: 5700, Loss: 30.833837509155273 for layer [False]\n",
      "Iteration: 5800, Loss: 26.88031768798828 for layer [False]\n",
      "Iteration: 5900, Loss: 21.447723388671875 for layer [False]\n",
      "Iteration: 6000, Loss: 7.987937927246094 for layer [ True]\n",
      "Iteration: 6100, Loss: 14.384779930114746 for layer [False]\n",
      "Iteration: 6200, Loss: 14.306586265563965 for layer [False]\n",
      "Iteration: 6300, Loss: 13.98337173461914 for layer [False]\n",
      "Iteration: 6400, Loss: 9.438528060913086 for layer [ True]\n",
      "Iteration: 6500, Loss: 4.170746803283691 for layer [ True]\n",
      "Iteration: 6600, Loss: 5.879081726074219 for layer [ True]\n",
      "Iteration: 6700, Loss: 12.409100532531738 for layer [False]\n",
      "Iteration: 6800, Loss: 3.5539870262145996 for layer [ True]\n",
      "Iteration: 6900, Loss: 12.245145797729492 for layer [False]\n",
      "Iteration: 7000, Loss: 6.841390609741211 for layer [False]\n",
      "Iteration: 7100, Loss: 11.381646156311035 for layer [False]\n",
      "Iteration: 7200, Loss: 6.277690410614014 for layer [False]\n",
      "Iteration: 7300, Loss: 3.9156200885772705 for layer [ True]\n",
      "Iteration: 7400, Loss: 5.057559013366699 for layer [False]\n",
      "Iteration: 7500, Loss: 2.982713460922241 for layer [ True]\n",
      "Iteration: 7600, Loss: 2.817336320877075 for layer [ True]\n",
      "Iteration: 7700, Loss: 5.9240922927856445 for layer [False]\n",
      "Iteration: 7800, Loss: 5.469723224639893 for layer [False]\n",
      "Iteration: 7900, Loss: 2.7165255546569824 for layer [ True]\n",
      "Iteration: 8000, Loss: 2.951510190963745 for layer [ True]\n",
      "Iteration: 8100, Loss: 1.7011083364486694 for layer [ True]\n",
      "Iteration: 8200, Loss: 1.3732125759124756 for layer [ True]\n",
      "Iteration: 8300, Loss: 5.294665336608887 for layer [False]\n",
      "Iteration: 8400, Loss: 1.1797490119934082 for layer [ True]\n",
      "Iteration: 8500, Loss: 3.167226552963257 for layer [False]\n",
      "Iteration: 8600, Loss: 3.0237886905670166 for layer [False]\n",
      "Iteration: 8700, Loss: 1.4195350408554077 for layer [ True]\n",
      "Iteration: 8800, Loss: 3.8907787799835205 for layer [False]\n",
      "Iteration: 8900, Loss: 5.5962419509887695 for layer [False]\n",
      "Iteration: 9000, Loss: 0.7206208109855652 for layer [ True]\n",
      "Iteration: 9100, Loss: 0.9514609575271606 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.2054439783096313 for layer [ True]\n",
      "Iteration: 9300, Loss: 2.754575729370117 for layer [False]\n",
      "Iteration: 9400, Loss: 2.8336782455444336 for layer [False]\n",
      "Iteration: 9500, Loss: 0.7841241955757141 for layer [ True]\n",
      "Iteration: 9600, Loss: 2.999997854232788 for layer [False]\n",
      "Iteration: 9700, Loss: 0.644640326499939 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.5200890302658081 for layer [ True]\n",
      "Iteration: 9900, Loss: 3.3039772510528564 for layer [False]\n",
      "Iteration: 10000, Loss: 2.0940964221954346 for layer [False]\n",
      "Iteration: 10100, Loss: 0.5226057171821594 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.32974398136138916 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.737727642059326 for layer [False]\n",
      "Iteration: 10400, Loss: 0.3357093036174774 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.25392448902130127 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.0089142322540283 for layer [False]\n",
      "Iteration: 10700, Loss: 0.31600263714790344 for layer [ True]\n",
      "Iteration: 10800, Loss: 2.4749443531036377 for layer [False]\n",
      "Iteration: 10900, Loss: 0.25571513175964355 for layer [ True]\n",
      "Iteration: 11000, Loss: 2.6968603134155273 for layer [False]\n",
      "Iteration: 11100, Loss: 2.290858268737793 for layer [False]\n",
      "Iteration: 11200, Loss: 0.19065183401107788 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.1435222625732422 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.18801815807819366 for layer [ True]\n",
      "Iteration: 11500, Loss: 1.877362608909607 for layer [False]\n",
      "Iteration: 11600, Loss: 0.2019246369600296 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.11194740980863571 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.1176948994398117 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.1254127472639084 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.061716478317976 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.08334033936262131 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.06351366639137268 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.05955752730369568 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.06584221869707108 for layer [ True]\n",
      "Iteration: 12500, Loss: 1.6494745016098022 for layer [False]\n",
      "Iteration: 12600, Loss: 1.8744845390319824 for layer [False]\n",
      "Iteration: 12700, Loss: 1.3623073101043701 for layer [False]\n",
      "Iteration: 12800, Loss: 0.04332499951124191 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.03567545488476753 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.02986970916390419 for layer [ True]\n",
      "Iteration: 13100, Loss: 1.8465923070907593 for layer [False]\n",
      "Iteration: 13200, Loss: 0.028449833393096924 for layer [ True]\n",
      "Iteration: 13300, Loss: 1.1451858282089233 for layer [False]\n",
      "Iteration: 13400, Loss: 0.01792200468480587 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.019252685829997063 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.014741942286491394 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.008576443418860435 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.01401454582810402 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.010905216448009014 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.0893638134002686 for layer [False]\n",
      "Iteration: 14100, Loss: 0.015025983564555645 for layer [ True]\n",
      "Iteration: 14200, Loss: 3.5507707595825195 for layer [False]\n",
      "Iteration: 14300, Loss: 0.005248023197054863 for layer [ True]\n",
      "Iteration: 14400, Loss: 1.9209011793136597 for layer [False]\n",
      "Iteration: 14500, Loss: 1.174035906791687 for layer [False]\n",
      "Iteration: 14600, Loss: 1.8837330341339111 for layer [False]\n",
      "Iteration: 14700, Loss: 1.1618757247924805 for layer [False]\n",
      "Iteration: 14800, Loss: 1.9617284536361694 for layer [False]\n",
      "Iteration: 14900, Loss: 1.806741714477539 for layer [False]\n",
      "Iteration: 15000, Loss: 0.002250443445518613 for layer [ True]\n",
      "Iteration: 15100, Loss: 1.181706190109253 for layer [False]\n",
      "Iteration: 15200, Loss: 1.8247753381729126 for layer [False]\n",
      "Iteration: 15300, Loss: 1.3047235012054443 for layer [False]\n",
      "Iteration: 15400, Loss: 1.2056246995925903 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0011270242976024747 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0018052029190585017 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.0015160782495513558 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.5165618658065796 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0009587385575287044 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0010158759541809559 for layer [ True]\n",
      "Iteration: 16100, Loss: 1.9281823635101318 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0006852869992144406 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0006215858738869429 for layer [ True]\n",
      "Iteration: 16400, Loss: 0.9078993797302246 for layer [False]\n",
      "Iteration: 16500, Loss: 4.983184814453125 for layer [False]\n",
      "Iteration: 16600, Loss: 1.8089736700057983 for layer [False]\n",
      "Iteration: 16700, Loss: 0.00039982644375413656 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.001426152652129531 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0005232738913036883 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0005572098307311535 for layer [ True]\n",
      "Iteration: 17100, Loss: 0.6489855051040649 for layer [False]\n",
      "Iteration: 17200, Loss: 1.4423329830169678 for layer [False]\n",
      "Iteration: 17300, Loss: 0.00017202897288370878 for layer [ True]\n",
      "Iteration: 17400, Loss: 1.1753501892089844 for layer [False]\n",
      "Iteration: 17500, Loss: 2.5966527462005615 for layer [False]\n",
      "Iteration: 17600, Loss: 1.3292008638381958 for layer [False]\n",
      "Iteration: 17700, Loss: 0.00020040132221765816 for layer [ True]\n",
      "Iteration: 17800, Loss: 1.5640466213226318 for layer [False]\n",
      "Iteration: 17900, Loss: 2.2252769470214844 for layer [False]\n",
      "Iteration: 18000, Loss: 1.3647053241729736 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0003184312954545021 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.000331869930960238 for layer [ True]\n",
      "Iteration: 18300, Loss: 2.8326897621154785 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00016742806474212557 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0003211906587239355 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.1903656721115112 for layer [False]\n",
      "Iteration: 18700, Loss: 1.4260739088058472 for layer [False]\n",
      "Iteration: 18800, Loss: 1.1167114973068237 for layer [False]\n",
      "Iteration: 18900, Loss: 0.9768784046173096 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00012903037713840604 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.3661144971847534 for layer [False]\n",
      "Iteration: 19200, Loss: 0.00040910180541686714 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.8456122875213623 for layer [False]\n",
      "Iteration: 19400, Loss: 1.8357545137405396 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005005353596061468 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00030211370903998613 for layer [ True]\n",
      "Iteration: 19700, Loss: 1.5436965227127075 for layer [False]\n",
      "Iteration: 19800, Loss: 0.8288828730583191 for layer [False]\n",
      "Iteration: 19900, Loss: 1.2214711904525757 for layer [False]\n",
      "Iteration: 20000, Loss: 1.4444465637207031 for layer [False]\n",
      "Iteration: 20100, Loss: 1.9330781698226929 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0006589671247638762 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.0622384548187256 for layer [False]\n",
      "Iteration: 20400, Loss: 6.922792817931622e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00012111179967178032 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0002765385725069791 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.22748863697052 for layer [False]\n",
      "Iteration: 20800, Loss: 7.173870108090341e-05 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0003154515288770199 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.9150837659835815 for layer [False]\n",
      "Iteration: 21100, Loss: 2.607700824737549 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0004103636019863188 for layer [ True]\n",
      "Iteration: 21300, Loss: 1.7744535207748413 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00011352422734489664 for layer [ True]\n",
      "Iteration: 21500, Loss: 0.7292611002922058 for layer [False]\n",
      "Iteration: 21600, Loss: 1.2116254568099976 for layer [False]\n",
      "Iteration: 21700, Loss: 4.5252570998854935e-05 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00029125873697921634 for layer [ True]\n",
      "Iteration: 21900, Loss: 0.9576637148857117 for layer [False]\n",
      "Iteration: 22000, Loss: 1.5304478406906128 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00017272135301027447 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00027453602524474263 for layer [ True]\n",
      "Iteration: 22300, Loss: 1.7123641967773438 for layer [False]\n",
      "Iteration: 22400, Loss: 1.5446292161941528 for layer [False]\n",
      "Iteration: 22500, Loss: 0.8778567314147949 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00010546661360422149 for layer [ True]\n",
      "Iteration: 22700, Loss: 8.772593719186261e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0002845791750587523 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0012010806240141392 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00029829706181772053 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.000804482726380229 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0001033381195156835 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0001638797839405015 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002485924633219838 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00010009108518715948 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0002797962224576622 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.6834543943405151 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0009595187730155885 for layer [ True]\n",
      "Iteration: 23900, Loss: 9.490946104051545e-05 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00019272307690698653 for layer [ True]\n",
      "Iteration: 24100, Loss: 4.119249933864921e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 0.7864646911621094 for layer [False]\n",
      "Iteration: 24300, Loss: 1.2128843069076538 for layer [False]\n",
      "Iteration: 24400, Loss: 1.9024033546447754 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00012834154767915606 for layer [ True]\n",
      "Iteration: 24600, Loss: 5.803766907774843e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.901273727416992 for layer [False]\n",
      "Iteration: 24800, Loss: 2.394946813583374 for layer [False]\n",
      "Iteration: 24900, Loss: 4.067679401487112e-05 for layer [ True]\n",
      "Step 1500 | Loss: 0.002710\n",
      "Step 1600 | Loss: 0.002498\n",
      "Step 1700 | Loss: 0.002316\n",
      "Step 1800 | Loss: 0.002125\n",
      "Step 1900 | Loss: 0.001980\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2089.05029296875 for layer [False]\n",
      "Iteration: 100, Loss: 322.85748291015625 for layer [ True]\n",
      "Iteration: 200, Loss: 2022.7525634765625 for layer [False]\n",
      "Iteration: 300, Loss: 1763.0931396484375 for layer [False]\n",
      "Iteration: 400, Loss: 222.76434326171875 for layer [ True]\n",
      "Iteration: 500, Loss: 296.9650573730469 for layer [ True]\n",
      "Iteration: 600, Loss: 1541.113525390625 for layer [False]\n",
      "Iteration: 700, Loss: 202.95362854003906 for layer [ True]\n",
      "Iteration: 800, Loss: 1314.0626220703125 for layer [False]\n",
      "Iteration: 900, Loss: 1087.6793212890625 for layer [False]\n",
      "Iteration: 1000, Loss: 127.7682876586914 for layer [ True]\n",
      "Iteration: 1100, Loss: 1028.647705078125 for layer [False]\n",
      "Iteration: 1200, Loss: 1197.8446044921875 for layer [False]\n",
      "Iteration: 1300, Loss: 1077.4935302734375 for layer [False]\n",
      "Iteration: 1400, Loss: 160.46786499023438 for layer [ True]\n",
      "Iteration: 1500, Loss: 205.63796997070312 for layer [ True]\n",
      "Iteration: 1600, Loss: 99.77019500732422 for layer [ True]\n",
      "Iteration: 1700, Loss: 111.81446075439453 for layer [ True]\n",
      "Iteration: 1800, Loss: 710.6351318359375 for layer [False]\n",
      "Iteration: 1900, Loss: 87.64530181884766 for layer [ True]\n",
      "Iteration: 2000, Loss: 79.0141372680664 for layer [ True]\n",
      "Iteration: 2100, Loss: 120.21533203125 for layer [ True]\n",
      "Iteration: 2200, Loss: 89.4066162109375 for layer [ True]\n",
      "Iteration: 2300, Loss: 501.363037109375 for layer [False]\n",
      "Iteration: 2400, Loss: 80.48629760742188 for layer [ True]\n",
      "Iteration: 2500, Loss: 117.53778839111328 for layer [ True]\n",
      "Iteration: 2600, Loss: 85.75946044921875 for layer [ True]\n",
      "Iteration: 2700, Loss: 69.3970947265625 for layer [ True]\n",
      "Iteration: 2800, Loss: 368.1373596191406 for layer [False]\n",
      "Iteration: 2900, Loss: 298.84222412109375 for layer [False]\n",
      "Iteration: 3000, Loss: 57.73663330078125 for layer [ True]\n",
      "Iteration: 3100, Loss: 252.19198608398438 for layer [False]\n",
      "Iteration: 3200, Loss: 264.5238342285156 for layer [False]\n",
      "Iteration: 3300, Loss: 184.29185485839844 for layer [False]\n",
      "Iteration: 3400, Loss: 51.93555450439453 for layer [ True]\n",
      "Iteration: 3500, Loss: 231.2784881591797 for layer [False]\n",
      "Iteration: 3600, Loss: 44.04766082763672 for layer [ True]\n",
      "Iteration: 3700, Loss: 178.98126220703125 for layer [False]\n",
      "Iteration: 3800, Loss: 135.44102478027344 for layer [False]\n",
      "Iteration: 3900, Loss: 30.13120460510254 for layer [ True]\n",
      "Iteration: 4000, Loss: 36.65984344482422 for layer [ True]\n",
      "Iteration: 4100, Loss: 31.79658317565918 for layer [ True]\n",
      "Iteration: 4200, Loss: 51.91434860229492 for layer [ True]\n",
      "Iteration: 4300, Loss: 35.190372467041016 for layer [ True]\n",
      "Iteration: 4400, Loss: 27.50748062133789 for layer [ True]\n",
      "Iteration: 4500, Loss: 35.537960052490234 for layer [ True]\n",
      "Iteration: 4600, Loss: 25.57821273803711 for layer [ True]\n",
      "Iteration: 4700, Loss: 78.65316009521484 for layer [False]\n",
      "Iteration: 4800, Loss: 50.47544860839844 for layer [False]\n",
      "Iteration: 4900, Loss: 64.79295349121094 for layer [False]\n",
      "Iteration: 5000, Loss: 32.86976623535156 for layer [ True]\n",
      "Iteration: 5100, Loss: 15.039420127868652 for layer [ True]\n",
      "Iteration: 5200, Loss: 46.761268615722656 for layer [False]\n",
      "Iteration: 5300, Loss: 14.94711971282959 for layer [ True]\n",
      "Iteration: 5400, Loss: 34.39218521118164 for layer [False]\n",
      "Iteration: 5500, Loss: 16.45840835571289 for layer [ True]\n",
      "Iteration: 5600, Loss: 17.476444244384766 for layer [ True]\n",
      "Iteration: 5700, Loss: 30.079574584960938 for layer [False]\n",
      "Iteration: 5800, Loss: 26.171953201293945 for layer [False]\n",
      "Iteration: 5900, Loss: 20.49109649658203 for layer [False]\n",
      "Iteration: 6000, Loss: 9.333404541015625 for layer [ True]\n",
      "Iteration: 6100, Loss: 14.07471752166748 for layer [False]\n",
      "Iteration: 6200, Loss: 13.664679527282715 for layer [False]\n",
      "Iteration: 6300, Loss: 13.375480651855469 for layer [False]\n",
      "Iteration: 6400, Loss: 9.963041305541992 for layer [ True]\n",
      "Iteration: 6500, Loss: 4.785112380981445 for layer [ True]\n",
      "Iteration: 6600, Loss: 6.02368688583374 for layer [ True]\n",
      "Iteration: 6700, Loss: 11.701011657714844 for layer [False]\n",
      "Iteration: 6800, Loss: 3.613896608352661 for layer [ True]\n",
      "Iteration: 6900, Loss: 14.563529968261719 for layer [False]\n",
      "Iteration: 7000, Loss: 7.255125522613525 for layer [False]\n",
      "Iteration: 7100, Loss: 10.48762035369873 for layer [False]\n",
      "Iteration: 7200, Loss: 5.548681735992432 for layer [False]\n",
      "Iteration: 7300, Loss: 3.863999128341675 for layer [ True]\n",
      "Iteration: 7400, Loss: 4.766903400421143 for layer [False]\n",
      "Iteration: 7500, Loss: 3.0805535316467285 for layer [ True]\n",
      "Iteration: 7600, Loss: 3.1370692253112793 for layer [ True]\n",
      "Iteration: 7700, Loss: 6.025259971618652 for layer [False]\n",
      "Iteration: 7800, Loss: 5.439289569854736 for layer [False]\n",
      "Iteration: 7900, Loss: 3.312617301940918 for layer [ True]\n",
      "Iteration: 8000, Loss: 2.7963716983795166 for layer [ True]\n",
      "Iteration: 8100, Loss: 1.9610767364501953 for layer [ True]\n",
      "Iteration: 8200, Loss: 1.504486083984375 for layer [ True]\n",
      "Iteration: 8300, Loss: 5.440508842468262 for layer [False]\n",
      "Iteration: 8400, Loss: 1.3506132364273071 for layer [ True]\n",
      "Iteration: 8500, Loss: 3.387348175048828 for layer [False]\n",
      "Iteration: 8600, Loss: 3.041295289993286 for layer [False]\n",
      "Iteration: 8700, Loss: 1.7186377048492432 for layer [ True]\n",
      "Iteration: 8800, Loss: 3.3829944133758545 for layer [False]\n",
      "Iteration: 8900, Loss: 4.654504299163818 for layer [False]\n",
      "Iteration: 9000, Loss: 0.8240480422973633 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.0116504430770874 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.3317043781280518 for layer [ True]\n",
      "Iteration: 9300, Loss: 2.799984931945801 for layer [False]\n",
      "Iteration: 9400, Loss: 2.6289820671081543 for layer [False]\n",
      "Iteration: 9500, Loss: 0.9368438720703125 for layer [ True]\n",
      "Iteration: 9600, Loss: 2.7096524238586426 for layer [False]\n",
      "Iteration: 9700, Loss: 0.6862688064575195 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.580694854259491 for layer [ True]\n",
      "Iteration: 9900, Loss: 3.444918155670166 for layer [False]\n",
      "Iteration: 10000, Loss: 1.8099428415298462 for layer [False]\n",
      "Iteration: 10100, Loss: 0.6225955486297607 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.3994855284690857 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.3534021377563477 for layer [False]\n",
      "Iteration: 10400, Loss: 0.35798370838165283 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.33052217960357666 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.1465768814086914 for layer [False]\n",
      "Iteration: 10700, Loss: 0.3934547007083893 for layer [ True]\n",
      "Iteration: 10800, Loss: 2.4267735481262207 for layer [False]\n",
      "Iteration: 10900, Loss: 0.28742516040802 for layer [ True]\n",
      "Iteration: 11000, Loss: 2.308382034301758 for layer [False]\n",
      "Iteration: 11100, Loss: 2.4115097522735596 for layer [False]\n",
      "Iteration: 11200, Loss: 0.21218569576740265 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.17072521150112152 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.23096922039985657 for layer [ True]\n",
      "Iteration: 11500, Loss: 1.6425440311431885 for layer [False]\n",
      "Iteration: 11600, Loss: 0.22935479879379272 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.1177375391125679 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.13277168571949005 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.129140242934227 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.06907534599304199 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.07984956353902817 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.07671378552913666 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.05237121880054474 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.08116817474365234 for layer [ True]\n",
      "Iteration: 12500, Loss: 1.8236417770385742 for layer [False]\n",
      "Iteration: 12600, Loss: 2.338130235671997 for layer [False]\n",
      "Iteration: 12700, Loss: 1.3554606437683105 for layer [False]\n",
      "Iteration: 12800, Loss: 0.04834606498479843 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.03781185299158096 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.037715647369623184 for layer [ True]\n",
      "Iteration: 13100, Loss: 1.6653977632522583 for layer [False]\n",
      "Iteration: 13200, Loss: 0.0339956097304821 for layer [ True]\n",
      "Iteration: 13300, Loss: 1.446521520614624 for layer [False]\n",
      "Iteration: 13400, Loss: 0.022587114945054054 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.02794567681849003 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.01942424476146698 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.009551986120641232 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.014978005550801754 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.012622017413377762 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.2795077562332153 for layer [False]\n",
      "Iteration: 14100, Loss: 0.016332218423485756 for layer [ True]\n",
      "Iteration: 14200, Loss: 3.2269887924194336 for layer [False]\n",
      "Iteration: 14300, Loss: 0.006678356789052486 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.264359712600708 for layer [False]\n",
      "Iteration: 14500, Loss: 1.1519049406051636 for layer [False]\n",
      "Iteration: 14600, Loss: 1.6269240379333496 for layer [False]\n",
      "Iteration: 14700, Loss: 1.2788764238357544 for layer [False]\n",
      "Iteration: 14800, Loss: 1.8170596361160278 for layer [False]\n",
      "Iteration: 14900, Loss: 1.8845298290252686 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0028443043120205402 for layer [ True]\n",
      "Iteration: 15100, Loss: 1.1835367679595947 for layer [False]\n",
      "Iteration: 15200, Loss: 2.617596387863159 for layer [False]\n",
      "Iteration: 15300, Loss: 1.5997921228408813 for layer [False]\n",
      "Iteration: 15400, Loss: 1.2480504512786865 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0014735412551090121 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0019980785436928272 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.0035170926712453365 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.4589505195617676 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0010313093662261963 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0011855557095259428 for layer [ True]\n",
      "Iteration: 16100, Loss: 1.9169281721115112 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0007163912523537874 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0007129489677026868 for layer [ True]\n",
      "Iteration: 16400, Loss: 0.8664709329605103 for layer [False]\n",
      "Iteration: 16500, Loss: 3.8823821544647217 for layer [False]\n",
      "Iteration: 16600, Loss: 1.5217350721359253 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0008243023185059428 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0014907928416505456 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.000530876568518579 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0006688581197522581 for layer [ True]\n",
      "Iteration: 17100, Loss: 0.8467811942100525 for layer [False]\n",
      "Iteration: 17200, Loss: 1.6025376319885254 for layer [False]\n",
      "Iteration: 17300, Loss: 0.00034684903221204877 for layer [ True]\n",
      "Iteration: 17400, Loss: 1.2173980474472046 for layer [False]\n",
      "Iteration: 17500, Loss: 2.800278663635254 for layer [False]\n",
      "Iteration: 17600, Loss: 1.4999479055404663 for layer [False]\n",
      "Iteration: 17700, Loss: 0.00019257969688624144 for layer [ True]\n",
      "Iteration: 17800, Loss: 1.517221212387085 for layer [False]\n",
      "Iteration: 17900, Loss: 2.302755117416382 for layer [False]\n",
      "Iteration: 18000, Loss: 1.26531183719635 for layer [False]\n",
      "Iteration: 18100, Loss: 0.00033774159965105355 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.000503410876262933 for layer [ True]\n",
      "Iteration: 18300, Loss: 2.8362693786621094 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00018573302077129483 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0005373434978537261 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.257739782333374 for layer [False]\n",
      "Iteration: 18700, Loss: 1.3759558200836182 for layer [False]\n",
      "Iteration: 18800, Loss: 1.0001699924468994 for layer [False]\n",
      "Iteration: 18900, Loss: 0.8585317730903625 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00013448130630422384 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.2791223526000977 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0004171178152319044 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.7712960243225098 for layer [False]\n",
      "Iteration: 19400, Loss: 1.8313966989517212 for layer [False]\n",
      "Iteration: 19500, Loss: 0.00034973942092619836 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00031535676680505276 for layer [ True]\n",
      "Iteration: 19700, Loss: 1.3133906126022339 for layer [False]\n",
      "Iteration: 19800, Loss: 0.8574855327606201 for layer [False]\n",
      "Iteration: 19900, Loss: 0.9193267822265625 for layer [False]\n",
      "Iteration: 20000, Loss: 1.2975152730941772 for layer [False]\n",
      "Iteration: 20100, Loss: 2.0847151279449463 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0009854640811681747 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.0167853832244873 for layer [False]\n",
      "Iteration: 20400, Loss: 8.965546294348314e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0002484738652128726 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0001954920298885554 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.0452773571014404 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00011768964759539813 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0005476743099279702 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.5905643701553345 for layer [False]\n",
      "Iteration: 21100, Loss: 2.9236690998077393 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0004263165465090424 for layer [ True]\n",
      "Iteration: 21300, Loss: 1.8081215620040894 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0009428129415027797 for layer [ True]\n",
      "Iteration: 21500, Loss: 0.7264201045036316 for layer [False]\n",
      "Iteration: 21600, Loss: 1.3307456970214844 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00015858343977015465 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0002747600374277681 for layer [ True]\n",
      "Iteration: 21900, Loss: 1.0037544965744019 for layer [False]\n",
      "Iteration: 22000, Loss: 1.3569847345352173 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00020166403555776924 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00021650781854987144 for layer [ True]\n",
      "Iteration: 22300, Loss: 1.8735482692718506 for layer [False]\n",
      "Iteration: 22400, Loss: 1.9087934494018555 for layer [False]\n",
      "Iteration: 22500, Loss: 0.8509693145751953 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0003244811960030347 for layer [ True]\n",
      "Iteration: 22700, Loss: 7.333618850680068e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0005651786923408508 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0006888983771204948 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.000666406296659261 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0004008384421467781 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00012932546087540686 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00041240479913540184 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00010967566049657762 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00012760017125401646 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00015029274800326675 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.5260316133499146 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0005484135472215712 for layer [ True]\n",
      "Iteration: 23900, Loss: 8.767521649133414e-05 for layer [ True]\n",
      "Iteration: 24000, Loss: 8.66464979480952e-05 for layer [ True]\n",
      "Iteration: 24100, Loss: 9.385175508214161e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 1.093949556350708 for layer [False]\n",
      "Iteration: 24300, Loss: 1.3560469150543213 for layer [False]\n",
      "Iteration: 24400, Loss: 2.120950222015381 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00026918452931568027 for layer [ True]\n",
      "Iteration: 24600, Loss: 6.081326864659786e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.286982536315918 for layer [False]\n",
      "Iteration: 24800, Loss: 2.4926462173461914 for layer [False]\n",
      "Iteration: 24900, Loss: 9.328982559964061e-05 for layer [ True]\n",
      "Step 2000 | Loss: 0.001848\n",
      "Step 2100 | Loss: 0.001676\n",
      "Step 2200 | Loss: 0.001555\n",
      "Step 2300 | Loss: 0.001451\n",
      "Step 2400 | Loss: 0.001370\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2059.67431640625 for layer [False]\n",
      "Iteration: 100, Loss: 319.31414794921875 for layer [ True]\n",
      "Iteration: 200, Loss: 2011.5987548828125 for layer [False]\n",
      "Iteration: 300, Loss: 1726.9210205078125 for layer [False]\n",
      "Iteration: 400, Loss: 223.26646423339844 for layer [ True]\n",
      "Iteration: 500, Loss: 304.4851379394531 for layer [ True]\n",
      "Iteration: 600, Loss: 1532.546875 for layer [False]\n",
      "Iteration: 700, Loss: 211.19786071777344 for layer [ True]\n",
      "Iteration: 800, Loss: 1283.7169189453125 for layer [False]\n",
      "Iteration: 900, Loss: 1060.647216796875 for layer [False]\n",
      "Iteration: 1000, Loss: 135.24855041503906 for layer [ True]\n",
      "Iteration: 1100, Loss: 1001.9976196289062 for layer [False]\n",
      "Iteration: 1200, Loss: 1163.989013671875 for layer [False]\n",
      "Iteration: 1300, Loss: 1068.373291015625 for layer [False]\n",
      "Iteration: 1400, Loss: 180.2010955810547 for layer [ True]\n",
      "Iteration: 1500, Loss: 227.18222045898438 for layer [ True]\n",
      "Iteration: 1600, Loss: 106.82538604736328 for layer [ True]\n",
      "Iteration: 1700, Loss: 109.82662200927734 for layer [ True]\n",
      "Iteration: 1800, Loss: 691.0689086914062 for layer [False]\n",
      "Iteration: 1900, Loss: 94.12285614013672 for layer [ True]\n",
      "Iteration: 2000, Loss: 82.36395263671875 for layer [ True]\n",
      "Iteration: 2100, Loss: 130.64027404785156 for layer [ True]\n",
      "Iteration: 2200, Loss: 94.42532348632812 for layer [ True]\n",
      "Iteration: 2300, Loss: 491.7795715332031 for layer [False]\n",
      "Iteration: 2400, Loss: 86.12676239013672 for layer [ True]\n",
      "Iteration: 2500, Loss: 134.56626892089844 for layer [ True]\n",
      "Iteration: 2600, Loss: 92.89008331298828 for layer [ True]\n",
      "Iteration: 2700, Loss: 71.06629943847656 for layer [ True]\n",
      "Iteration: 2800, Loss: 360.11279296875 for layer [False]\n",
      "Iteration: 2900, Loss: 289.2651062011719 for layer [False]\n",
      "Iteration: 3000, Loss: 64.32566833496094 for layer [ True]\n",
      "Iteration: 3100, Loss: 245.46424865722656 for layer [False]\n",
      "Iteration: 3200, Loss: 255.78085327148438 for layer [False]\n",
      "Iteration: 3300, Loss: 178.4835968017578 for layer [False]\n",
      "Iteration: 3400, Loss: 57.421722412109375 for layer [ True]\n",
      "Iteration: 3500, Loss: 219.63641357421875 for layer [False]\n",
      "Iteration: 3600, Loss: 47.62347412109375 for layer [ True]\n",
      "Iteration: 3700, Loss: 166.56491088867188 for layer [False]\n",
      "Iteration: 3800, Loss: 131.6322784423828 for layer [False]\n",
      "Iteration: 3900, Loss: 32.41604232788086 for layer [ True]\n",
      "Iteration: 4000, Loss: 39.901771545410156 for layer [ True]\n",
      "Iteration: 4100, Loss: 38.31396484375 for layer [ True]\n",
      "Iteration: 4200, Loss: 60.91902160644531 for layer [ True]\n",
      "Iteration: 4300, Loss: 39.04123306274414 for layer [ True]\n",
      "Iteration: 4400, Loss: 32.7617073059082 for layer [ True]\n",
      "Iteration: 4500, Loss: 40.54692459106445 for layer [ True]\n",
      "Iteration: 4600, Loss: 29.38295555114746 for layer [ True]\n",
      "Iteration: 4700, Loss: 75.5188217163086 for layer [False]\n",
      "Iteration: 4800, Loss: 47.58869171142578 for layer [False]\n",
      "Iteration: 4900, Loss: 61.37423324584961 for layer [False]\n",
      "Iteration: 5000, Loss: 40.88112258911133 for layer [ True]\n",
      "Iteration: 5100, Loss: 17.266725540161133 for layer [ True]\n",
      "Iteration: 5200, Loss: 43.75891876220703 for layer [False]\n",
      "Iteration: 5300, Loss: 17.582738876342773 for layer [ True]\n",
      "Iteration: 5400, Loss: 31.639671325683594 for layer [False]\n",
      "Iteration: 5500, Loss: 19.56182098388672 for layer [ True]\n",
      "Iteration: 5600, Loss: 20.36808967590332 for layer [ True]\n",
      "Iteration: 5700, Loss: 26.63345718383789 for layer [False]\n",
      "Iteration: 5800, Loss: 24.534536361694336 for layer [False]\n",
      "Iteration: 5900, Loss: 17.98158836364746 for layer [False]\n",
      "Iteration: 6000, Loss: 10.886083602905273 for layer [ True]\n",
      "Iteration: 6100, Loss: 14.322453498840332 for layer [False]\n",
      "Iteration: 6200, Loss: 13.079285621643066 for layer [False]\n",
      "Iteration: 6300, Loss: 12.627891540527344 for layer [False]\n",
      "Iteration: 6400, Loss: 12.316765785217285 for layer [ True]\n",
      "Iteration: 6500, Loss: 6.001819133758545 for layer [ True]\n",
      "Iteration: 6600, Loss: 7.171562194824219 for layer [ True]\n",
      "Iteration: 6700, Loss: 11.892635345458984 for layer [False]\n",
      "Iteration: 6800, Loss: 4.023566722869873 for layer [ True]\n",
      "Iteration: 6900, Loss: 16.14596939086914 for layer [False]\n",
      "Iteration: 7000, Loss: 7.584415912628174 for layer [False]\n",
      "Iteration: 7100, Loss: 9.688376426696777 for layer [False]\n",
      "Iteration: 7200, Loss: 6.301326751708984 for layer [False]\n",
      "Iteration: 7300, Loss: 4.0363616943359375 for layer [ True]\n",
      "Iteration: 7400, Loss: 5.637941360473633 for layer [False]\n",
      "Iteration: 7500, Loss: 3.5960798263549805 for layer [ True]\n",
      "Iteration: 7600, Loss: 3.7367656230926514 for layer [ True]\n",
      "Iteration: 7700, Loss: 7.203513145446777 for layer [False]\n",
      "Iteration: 7800, Loss: 6.094625949859619 for layer [False]\n",
      "Iteration: 7900, Loss: 4.104407787322998 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.0907106399536133 for layer [ True]\n",
      "Iteration: 8100, Loss: 2.327228307723999 for layer [ True]\n",
      "Iteration: 8200, Loss: 1.8434303998947144 for layer [ True]\n",
      "Iteration: 8300, Loss: 6.475906848907471 for layer [False]\n",
      "Iteration: 8400, Loss: 1.5375534296035767 for layer [ True]\n",
      "Iteration: 8500, Loss: 4.326355457305908 for layer [False]\n",
      "Iteration: 8600, Loss: 4.004822731018066 for layer [False]\n",
      "Iteration: 8700, Loss: 1.9477403163909912 for layer [ True]\n",
      "Iteration: 8800, Loss: 3.5269854068756104 for layer [False]\n",
      "Iteration: 8900, Loss: 4.584963321685791 for layer [False]\n",
      "Iteration: 9000, Loss: 0.9544891715049744 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.151868462562561 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.4851418733596802 for layer [ True]\n",
      "Iteration: 9300, Loss: 3.304504871368408 for layer [False]\n",
      "Iteration: 9400, Loss: 3.342552661895752 for layer [False]\n",
      "Iteration: 9500, Loss: 1.0921374559402466 for layer [ True]\n",
      "Iteration: 9600, Loss: 2.67996883392334 for layer [False]\n",
      "Iteration: 9700, Loss: 0.760443389415741 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.637109637260437 for layer [ True]\n",
      "Iteration: 9900, Loss: 4.97423791885376 for layer [False]\n",
      "Iteration: 10000, Loss: 2.06577467918396 for layer [False]\n",
      "Iteration: 10100, Loss: 0.8085031509399414 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.47830089926719666 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.314962387084961 for layer [False]\n",
      "Iteration: 10400, Loss: 0.40995705127716064 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.40929293632507324 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.498997688293457 for layer [False]\n",
      "Iteration: 10700, Loss: 0.4478692412376404 for layer [ True]\n",
      "Iteration: 10800, Loss: 2.6352431774139404 for layer [False]\n",
      "Iteration: 10900, Loss: 0.32685843110084534 for layer [ True]\n",
      "Iteration: 11000, Loss: 2.548637628555298 for layer [False]\n",
      "Iteration: 11100, Loss: 2.4470760822296143 for layer [False]\n",
      "Iteration: 11200, Loss: 0.23667758703231812 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.22152547538280487 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.26426810026168823 for layer [ True]\n",
      "Iteration: 11500, Loss: 1.7935765981674194 for layer [False]\n",
      "Iteration: 11600, Loss: 0.25022533535957336 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.12437188625335693 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.16741876304149628 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.15092435479164124 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.07962720841169357 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.10048794746398926 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.09447057545185089 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.0604986697435379 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.0926918014883995 for layer [ True]\n",
      "Iteration: 12500, Loss: 2.3858914375305176 for layer [False]\n",
      "Iteration: 12600, Loss: 3.0526161193847656 for layer [False]\n",
      "Iteration: 12700, Loss: 1.9794864654541016 for layer [False]\n",
      "Iteration: 12800, Loss: 0.061289943754673004 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.047178447246551514 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.04763364791870117 for layer [ True]\n",
      "Iteration: 13100, Loss: 2.0366551876068115 for layer [False]\n",
      "Iteration: 13200, Loss: 0.03800707682967186 for layer [ True]\n",
      "Iteration: 13300, Loss: 2.565941333770752 for layer [False]\n",
      "Iteration: 13400, Loss: 0.02757124789059162 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.03401635214686394 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.02584391087293625 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.010654858313500881 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.018120430409908295 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.015226047486066818 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.5411720275878906 for layer [False]\n",
      "Iteration: 14100, Loss: 0.018893079832196236 for layer [ True]\n",
      "Iteration: 14200, Loss: 3.7050817012786865 for layer [False]\n",
      "Iteration: 14300, Loss: 0.008464423008263111 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.841217041015625 for layer [False]\n",
      "Iteration: 14500, Loss: 1.3256237506866455 for layer [False]\n",
      "Iteration: 14600, Loss: 1.789867877960205 for layer [False]\n",
      "Iteration: 14700, Loss: 2.0462987422943115 for layer [False]\n",
      "Iteration: 14800, Loss: 1.8697075843811035 for layer [False]\n",
      "Iteration: 14900, Loss: 2.4181206226348877 for layer [False]\n",
      "Iteration: 15000, Loss: 0.004038776736706495 for layer [ True]\n",
      "Iteration: 15100, Loss: 1.4410640001296997 for layer [False]\n",
      "Iteration: 15200, Loss: 3.9240050315856934 for layer [False]\n",
      "Iteration: 15300, Loss: 2.453104257583618 for layer [False]\n",
      "Iteration: 15400, Loss: 1.6671236753463745 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0019632752519100904 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.002545929281041026 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004386933520436287 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.6852500438690186 for layer [False]\n",
      "Iteration: 15900, Loss: 0.001285096281208098 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0017951404443010688 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.194305181503296 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0007570780580863357 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0010759186698123813 for layer [ True]\n",
      "Iteration: 16400, Loss: 1.0285872220993042 for layer [False]\n",
      "Iteration: 16500, Loss: 3.498112916946411 for layer [False]\n",
      "Iteration: 16600, Loss: 1.6126869916915894 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0011896805372089148 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007335682166740298 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0006352905766107142 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008787274709902704 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.5777064561843872 for layer [False]\n",
      "Iteration: 17200, Loss: 1.9594168663024902 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006144486251287162 for layer [ True]\n",
      "Iteration: 17400, Loss: 1.7016106843948364 for layer [False]\n",
      "Iteration: 17500, Loss: 4.266750812530518 for layer [False]\n",
      "Iteration: 17600, Loss: 1.832375407218933 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0002891488838940859 for layer [ True]\n",
      "Iteration: 17800, Loss: 1.9240515232086182 for layer [False]\n",
      "Iteration: 17900, Loss: 2.4524197578430176 for layer [False]\n",
      "Iteration: 18000, Loss: 1.741231918334961 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0004009322728961706 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.00028894716524519026 for layer [ True]\n",
      "Iteration: 18300, Loss: 3.5287649631500244 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00018378920503892004 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0006679650978185236 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.7111622095108032 for layer [False]\n",
      "Iteration: 18700, Loss: 1.880150556564331 for layer [False]\n",
      "Iteration: 18800, Loss: 1.4188423156738281 for layer [False]\n",
      "Iteration: 18900, Loss: 0.8565492033958435 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00017244336777366698 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.3970791101455688 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0006672618328593671 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.7946234941482544 for layer [False]\n",
      "Iteration: 19400, Loss: 2.4484879970550537 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0004280140274204314 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0002988674968946725 for layer [ True]\n",
      "Iteration: 19700, Loss: 1.5552313327789307 for layer [False]\n",
      "Iteration: 19800, Loss: 1.2921359539031982 for layer [False]\n",
      "Iteration: 19900, Loss: 0.996714174747467 for layer [False]\n",
      "Iteration: 20000, Loss: 1.3781201839447021 for layer [False]\n",
      "Iteration: 20100, Loss: 2.4791312217712402 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0008553181542083621 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.102536916732788 for layer [False]\n",
      "Iteration: 20400, Loss: 9.621647768653929e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00012005730968667194 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.00014320477203000337 for layer [ True]\n",
      "Iteration: 20700, Loss: 0.9969978332519531 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0001100273584597744 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0005081568378955126 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.520335078239441 for layer [False]\n",
      "Iteration: 21100, Loss: 3.6570005416870117 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0007399250171147287 for layer [ True]\n",
      "Iteration: 21300, Loss: 2.4708893299102783 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00019356893608346581 for layer [ True]\n",
      "Iteration: 21500, Loss: 1.0115176439285278 for layer [False]\n",
      "Iteration: 21600, Loss: 1.9116299152374268 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0001668592740315944 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0002722953795455396 for layer [ True]\n",
      "Iteration: 21900, Loss: 1.2220790386199951 for layer [False]\n",
      "Iteration: 22000, Loss: 1.489196538925171 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00010797675349749625 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00018679030472412705 for layer [ True]\n",
      "Iteration: 22300, Loss: 2.7362942695617676 for layer [False]\n",
      "Iteration: 22400, Loss: 2.6080780029296875 for layer [False]\n",
      "Iteration: 22500, Loss: 1.0013597011566162 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0001249532651854679 for layer [ True]\n",
      "Iteration: 22700, Loss: 5.886624785489403e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0003689305449370295 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.00018279776850249618 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0001923320087371394 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00088474404765293 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0003215032338630408 for layer [ True]\n",
      "Iteration: 23300, Loss: 6.0081609262852e-05 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002388396969763562 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00013573658361565322 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00046131826820783317 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.5357109308242798 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006051387754268944 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00012185709056211635 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00014411086158361286 for layer [ True]\n",
      "Iteration: 24100, Loss: 8.731771958991885e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 1.8719719648361206 for layer [False]\n",
      "Iteration: 24300, Loss: 1.7420083284378052 for layer [False]\n",
      "Iteration: 24400, Loss: 2.7075047492980957 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00027383610722608864 for layer [ True]\n",
      "Iteration: 24600, Loss: 4.4949152652407065e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.4127817153930664 for layer [False]\n",
      "Iteration: 24800, Loss: 2.925997018814087 for layer [False]\n",
      "Iteration: 24900, Loss: 9.8402590083424e-05 for layer [ True]\n",
      "Step 2500 | Loss: 0.001293\n",
      "Step 2600 | Loss: 0.001164\n",
      "Step 2700 | Loss: 0.001059\n",
      "Step 2800 | Loss: 0.000993\n",
      "Step 2900 | Loss: 0.000942\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2029.7308349609375 for layer [False]\n",
      "Iteration: 100, Loss: 314.34625244140625 for layer [ True]\n",
      "Iteration: 200, Loss: 2004.2440185546875 for layer [False]\n",
      "Iteration: 300, Loss: 1675.1826171875 for layer [False]\n",
      "Iteration: 400, Loss: 221.8058319091797 for layer [ True]\n",
      "Iteration: 500, Loss: 304.6843566894531 for layer [ True]\n",
      "Iteration: 600, Loss: 1522.457275390625 for layer [False]\n",
      "Iteration: 700, Loss: 212.27256774902344 for layer [ True]\n",
      "Iteration: 800, Loss: 1254.946533203125 for layer [False]\n",
      "Iteration: 900, Loss: 1040.5692138671875 for layer [False]\n",
      "Iteration: 1000, Loss: 139.05580139160156 for layer [ True]\n",
      "Iteration: 1100, Loss: 974.0043334960938 for layer [False]\n",
      "Iteration: 1200, Loss: 1132.2049560546875 for layer [False]\n",
      "Iteration: 1300, Loss: 1048.1658935546875 for layer [False]\n",
      "Iteration: 1400, Loss: 193.49647521972656 for layer [ True]\n",
      "Iteration: 1500, Loss: 245.659912109375 for layer [ True]\n",
      "Iteration: 1600, Loss: 112.50495147705078 for layer [ True]\n",
      "Iteration: 1700, Loss: 111.21258544921875 for layer [ True]\n",
      "Iteration: 1800, Loss: 660.5921020507812 for layer [False]\n",
      "Iteration: 1900, Loss: 97.32345581054688 for layer [ True]\n",
      "Iteration: 2000, Loss: 86.69622802734375 for layer [ True]\n",
      "Iteration: 2100, Loss: 138.16334533691406 for layer [ True]\n",
      "Iteration: 2200, Loss: 96.18912506103516 for layer [ True]\n",
      "Iteration: 2300, Loss: 481.9162902832031 for layer [False]\n",
      "Iteration: 2400, Loss: 94.36833953857422 for layer [ True]\n",
      "Iteration: 2500, Loss: 146.61961364746094 for layer [ True]\n",
      "Iteration: 2600, Loss: 100.43196105957031 for layer [ True]\n",
      "Iteration: 2700, Loss: 72.79827117919922 for layer [ True]\n",
      "Iteration: 2800, Loss: 347.3017883300781 for layer [False]\n",
      "Iteration: 2900, Loss: 280.5737609863281 for layer [False]\n",
      "Iteration: 3000, Loss: 67.7579574584961 for layer [ True]\n",
      "Iteration: 3100, Loss: 243.5504608154297 for layer [False]\n",
      "Iteration: 3200, Loss: 250.129150390625 for layer [False]\n",
      "Iteration: 3300, Loss: 173.24574279785156 for layer [False]\n",
      "Iteration: 3400, Loss: 59.29337692260742 for layer [ True]\n",
      "Iteration: 3500, Loss: 210.94259643554688 for layer [False]\n",
      "Iteration: 3600, Loss: 48.949642181396484 for layer [ True]\n",
      "Iteration: 3700, Loss: 160.9735565185547 for layer [False]\n",
      "Iteration: 3800, Loss: 129.95712280273438 for layer [False]\n",
      "Iteration: 3900, Loss: 33.805335998535156 for layer [ True]\n",
      "Iteration: 4000, Loss: 42.1425666809082 for layer [ True]\n",
      "Iteration: 4100, Loss: 41.66007614135742 for layer [ True]\n",
      "Iteration: 4200, Loss: 63.55747985839844 for layer [ True]\n",
      "Iteration: 4300, Loss: 40.33255386352539 for layer [ True]\n",
      "Iteration: 4400, Loss: 34.82181167602539 for layer [ True]\n",
      "Iteration: 4500, Loss: 42.1974983215332 for layer [ True]\n",
      "Iteration: 4600, Loss: 30.393844604492188 for layer [ True]\n",
      "Iteration: 4700, Loss: 73.89161682128906 for layer [False]\n",
      "Iteration: 4800, Loss: 46.23923873901367 for layer [False]\n",
      "Iteration: 4900, Loss: 58.57413101196289 for layer [False]\n",
      "Iteration: 5000, Loss: 43.431243896484375 for layer [ True]\n",
      "Iteration: 5100, Loss: 18.079862594604492 for layer [ True]\n",
      "Iteration: 5200, Loss: 41.49565505981445 for layer [False]\n",
      "Iteration: 5300, Loss: 17.926748275756836 for layer [ True]\n",
      "Iteration: 5400, Loss: 29.581132888793945 for layer [False]\n",
      "Iteration: 5500, Loss: 19.82467269897461 for layer [ True]\n",
      "Iteration: 5600, Loss: 21.792972564697266 for layer [ True]\n",
      "Iteration: 5700, Loss: 25.46964454650879 for layer [False]\n",
      "Iteration: 5800, Loss: 22.670209884643555 for layer [False]\n",
      "Iteration: 5900, Loss: 16.454792022705078 for layer [False]\n",
      "Iteration: 6000, Loss: 11.240446090698242 for layer [ True]\n",
      "Iteration: 6100, Loss: 14.740890502929688 for layer [False]\n",
      "Iteration: 6200, Loss: 13.092574119567871 for layer [False]\n",
      "Iteration: 6300, Loss: 12.049814224243164 for layer [False]\n",
      "Iteration: 6400, Loss: 12.977417945861816 for layer [ True]\n",
      "Iteration: 6500, Loss: 6.365627288818359 for layer [ True]\n",
      "Iteration: 6600, Loss: 7.37575101852417 for layer [ True]\n",
      "Iteration: 6700, Loss: 12.55561351776123 for layer [False]\n",
      "Iteration: 6800, Loss: 3.944200277328491 for layer [ True]\n",
      "Iteration: 6900, Loss: 17.441049575805664 for layer [False]\n",
      "Iteration: 7000, Loss: 8.223341941833496 for layer [False]\n",
      "Iteration: 7100, Loss: 9.98720645904541 for layer [False]\n",
      "Iteration: 7200, Loss: 7.402614593505859 for layer [False]\n",
      "Iteration: 7300, Loss: 4.005343914031982 for layer [ True]\n",
      "Iteration: 7400, Loss: 6.806521892547607 for layer [False]\n",
      "Iteration: 7500, Loss: 3.8133604526519775 for layer [ True]\n",
      "Iteration: 7600, Loss: 4.005219459533691 for layer [ True]\n",
      "Iteration: 7700, Loss: 9.444793701171875 for layer [False]\n",
      "Iteration: 7800, Loss: 7.331334114074707 for layer [False]\n",
      "Iteration: 7900, Loss: 4.438059329986572 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.19467830657959 for layer [ True]\n",
      "Iteration: 8100, Loss: 2.580765962600708 for layer [ True]\n",
      "Iteration: 8200, Loss: 1.9501327276229858 for layer [ True]\n",
      "Iteration: 8300, Loss: 7.367646217346191 for layer [False]\n",
      "Iteration: 8400, Loss: 1.6023993492126465 for layer [ True]\n",
      "Iteration: 8500, Loss: 5.058292865753174 for layer [False]\n",
      "Iteration: 8600, Loss: 4.125765800476074 for layer [False]\n",
      "Iteration: 8700, Loss: 2.072098731994629 for layer [ True]\n",
      "Iteration: 8800, Loss: 3.7926793098449707 for layer [False]\n",
      "Iteration: 8900, Loss: 5.8455810546875 for layer [False]\n",
      "Iteration: 9000, Loss: 1.0846517086029053 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.2010602951049805 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.5598604679107666 for layer [ True]\n",
      "Iteration: 9300, Loss: 4.036020278930664 for layer [False]\n",
      "Iteration: 9400, Loss: 4.013850688934326 for layer [False]\n",
      "Iteration: 9500, Loss: 1.1889086961746216 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.1187801361083984 for layer [False]\n",
      "Iteration: 9700, Loss: 0.8377633690834045 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.6779788136482239 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.077295780181885 for layer [False]\n",
      "Iteration: 10000, Loss: 2.3830177783966064 for layer [False]\n",
      "Iteration: 10100, Loss: 0.882074236869812 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.5300542116165161 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.277805805206299 for layer [False]\n",
      "Iteration: 10400, Loss: 0.4421404302120209 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.43244054913520813 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.9460155963897705 for layer [False]\n",
      "Iteration: 10700, Loss: 0.4620799124240875 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.0872621536254883 for layer [False]\n",
      "Iteration: 10900, Loss: 0.3470642864704132 for layer [ True]\n",
      "Iteration: 11000, Loss: 2.9609498977661133 for layer [False]\n",
      "Iteration: 11100, Loss: 2.846531391143799 for layer [False]\n",
      "Iteration: 11200, Loss: 0.2570646107196808 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.25885647535324097 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.28147321939468384 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.0032832622528076 for layer [False]\n",
      "Iteration: 11600, Loss: 0.2613191604614258 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.13956354558467865 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.187746062874794 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.1813749223947525 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.09104692190885544 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.12482943385839462 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1126880943775177 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.0723871961236 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.09934235364198685 for layer [ True]\n",
      "Iteration: 12500, Loss: 3.2724528312683105 for layer [False]\n",
      "Iteration: 12600, Loss: 3.808168649673462 for layer [False]\n",
      "Iteration: 12700, Loss: 2.8164618015289307 for layer [False]\n",
      "Iteration: 12800, Loss: 0.07080630958080292 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.055557817220687866 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.05301486328244209 for layer [ True]\n",
      "Iteration: 13100, Loss: 2.768725633621216 for layer [False]\n",
      "Iteration: 13200, Loss: 0.043753206729888916 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.7167723178863525 for layer [False]\n",
      "Iteration: 13400, Loss: 0.03219917416572571 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.04108550772070885 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.030998658388853073 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.012341470457613468 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.022287627682089806 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.019172847270965576 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.717612385749817 for layer [False]\n",
      "Iteration: 14100, Loss: 0.021528489887714386 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.402247428894043 for layer [False]\n",
      "Iteration: 14300, Loss: 0.009093222208321095 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.3696553707122803 for layer [False]\n",
      "Iteration: 14500, Loss: 2.030195951461792 for layer [False]\n",
      "Iteration: 14600, Loss: 1.998498797416687 for layer [False]\n",
      "Iteration: 14700, Loss: 2.6588029861450195 for layer [False]\n",
      "Iteration: 14800, Loss: 2.2804019451141357 for layer [False]\n",
      "Iteration: 14900, Loss: 2.8125529289245605 for layer [False]\n",
      "Iteration: 15000, Loss: 0.00503417057916522 for layer [ True]\n",
      "Iteration: 15100, Loss: 1.9297218322753906 for layer [False]\n",
      "Iteration: 15200, Loss: 5.298212051391602 for layer [False]\n",
      "Iteration: 15300, Loss: 3.486259937286377 for layer [False]\n",
      "Iteration: 15400, Loss: 1.9641085863113403 for layer [False]\n",
      "Iteration: 15500, Loss: 0.002271458972245455 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0029546183068305254 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.00397317111492157 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.9816073179244995 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0015008212067186832 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0022999714128673077 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.68188738822937 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0008333217701874673 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0013909165281802416 for layer [ True]\n",
      "Iteration: 16400, Loss: 1.4710462093353271 for layer [False]\n",
      "Iteration: 16500, Loss: 3.7851641178131104 for layer [False]\n",
      "Iteration: 16600, Loss: 2.0403244495391846 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014620497822761536 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0006614665617235005 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.00072180584538728 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009258094942197204 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.885237693786621 for layer [False]\n",
      "Iteration: 17200, Loss: 2.4521799087524414 for layer [False]\n",
      "Iteration: 17300, Loss: 0.00047946113045327365 for layer [ True]\n",
      "Iteration: 17400, Loss: 2.3550937175750732 for layer [False]\n",
      "Iteration: 17500, Loss: 5.691524028778076 for layer [False]\n",
      "Iteration: 17600, Loss: 2.2483229637145996 for layer [False]\n",
      "Iteration: 17700, Loss: 0.000364849460311234 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.4272096157073975 for layer [False]\n",
      "Iteration: 17900, Loss: 2.9360601902008057 for layer [False]\n",
      "Iteration: 18000, Loss: 2.0208654403686523 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0003476338752079755 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.00035446698893792927 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.392541885375977 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00028694988577626646 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0009423433803021908 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.0840861797332764 for layer [False]\n",
      "Iteration: 18700, Loss: 2.287400960922241 for layer [False]\n",
      "Iteration: 18800, Loss: 2.0158255100250244 for layer [False]\n",
      "Iteration: 18900, Loss: 1.0427076816558838 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0001481803774368018 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.6871817111968994 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0009423231240361929 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.886315107345581 for layer [False]\n",
      "Iteration: 19400, Loss: 3.2579588890075684 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005738371983170509 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0002935044467449188 for layer [ True]\n",
      "Iteration: 19700, Loss: 2.1186482906341553 for layer [False]\n",
      "Iteration: 19800, Loss: 1.8518253564834595 for layer [False]\n",
      "Iteration: 19900, Loss: 1.193108320236206 for layer [False]\n",
      "Iteration: 20000, Loss: 1.4651347398757935 for layer [False]\n",
      "Iteration: 20100, Loss: 3.036104679107666 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0005275343428365886 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.316857099533081 for layer [False]\n",
      "Iteration: 20400, Loss: 0.00010741417645476758 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00021369761088863015 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.001105829724110663 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.2438104152679443 for layer [False]\n",
      "Iteration: 20800, Loss: 6.201502401381731e-05 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0007869211258366704 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.5062569379806519 for layer [False]\n",
      "Iteration: 21100, Loss: 4.368299961090088 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0006367372698150575 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.409868001937866 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00027588530792854726 for layer [ True]\n",
      "Iteration: 21500, Loss: 1.2694576978683472 for layer [False]\n",
      "Iteration: 21600, Loss: 2.4557015895843506 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0002116024843417108 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.000195485248696059 for layer [ True]\n",
      "Iteration: 21900, Loss: 1.650266170501709 for layer [False]\n",
      "Iteration: 22000, Loss: 1.9169998168945312 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0005960013950243592 for layer [ True]\n",
      "Iteration: 22200, Loss: 5.2919582230970263e-05 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.6648473739624023 for layer [False]\n",
      "Iteration: 22400, Loss: 3.123957395553589 for layer [False]\n",
      "Iteration: 22500, Loss: 1.2137041091918945 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00016734504606574774 for layer [ True]\n",
      "Iteration: 22700, Loss: 9.129828686127439e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0005371699808165431 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.00073037832044065 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00035086963907815516 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.000452027830760926 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00034132684231735766 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0002112917136400938 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002192940009990707 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00024853384820744395 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00014302905765362084 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.825365424156189 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006758438539691269 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00013078645861241966 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0001624138094484806 for layer [ True]\n",
      "Iteration: 24100, Loss: 9.962548938347027e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 2.6017508506774902 for layer [False]\n",
      "Iteration: 24300, Loss: 2.227954626083374 for layer [False]\n",
      "Iteration: 24400, Loss: 2.9565744400024414 for layer [False]\n",
      "Iteration: 24500, Loss: 5.8295063354307786e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00015712116146460176 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.9148778915405273 for layer [False]\n",
      "Iteration: 24800, Loss: 3.2801156044006348 for layer [False]\n",
      "Iteration: 24900, Loss: 9.253260213881731e-05 for layer [ True]\n",
      "Step 3000 | Loss: 0.000908\n",
      "Step 3100 | Loss: 0.000871\n",
      "Step 3200 | Loss: 0.000846\n",
      "Step 3300 | Loss: 0.000815\n",
      "Step 3400 | Loss: 0.000782\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2011.292724609375 for layer [False]\n",
      "Iteration: 100, Loss: 311.4947204589844 for layer [ True]\n",
      "Iteration: 200, Loss: 2001.5987548828125 for layer [False]\n",
      "Iteration: 300, Loss: 1644.57861328125 for layer [False]\n",
      "Iteration: 400, Loss: 227.29075622558594 for layer [ True]\n",
      "Iteration: 500, Loss: 312.1256408691406 for layer [ True]\n",
      "Iteration: 600, Loss: 1512.466552734375 for layer [False]\n",
      "Iteration: 700, Loss: 214.86134338378906 for layer [ True]\n",
      "Iteration: 800, Loss: 1231.1336669921875 for layer [False]\n",
      "Iteration: 900, Loss: 1032.2095947265625 for layer [False]\n",
      "Iteration: 1000, Loss: 143.5330352783203 for layer [ True]\n",
      "Iteration: 1100, Loss: 954.2953491210938 for layer [False]\n",
      "Iteration: 1200, Loss: 1117.3531494140625 for layer [False]\n",
      "Iteration: 1300, Loss: 1033.5230712890625 for layer [False]\n",
      "Iteration: 1400, Loss: 205.0828857421875 for layer [ True]\n",
      "Iteration: 1500, Loss: 265.61083984375 for layer [ True]\n",
      "Iteration: 1600, Loss: 117.6317138671875 for layer [ True]\n",
      "Iteration: 1700, Loss: 113.00118255615234 for layer [ True]\n",
      "Iteration: 1800, Loss: 648.0892944335938 for layer [False]\n",
      "Iteration: 1900, Loss: 101.22087860107422 for layer [ True]\n",
      "Iteration: 2000, Loss: 89.41868591308594 for layer [ True]\n",
      "Iteration: 2100, Loss: 144.3939666748047 for layer [ True]\n",
      "Iteration: 2200, Loss: 96.8109359741211 for layer [ True]\n",
      "Iteration: 2300, Loss: 477.6775817871094 for layer [False]\n",
      "Iteration: 2400, Loss: 102.3074951171875 for layer [ True]\n",
      "Iteration: 2500, Loss: 153.22581481933594 for layer [ True]\n",
      "Iteration: 2600, Loss: 106.51729583740234 for layer [ True]\n",
      "Iteration: 2700, Loss: 75.43600463867188 for layer [ True]\n",
      "Iteration: 2800, Loss: 342.42584228515625 for layer [False]\n",
      "Iteration: 2900, Loss: 274.65972900390625 for layer [False]\n",
      "Iteration: 3000, Loss: 69.63715362548828 for layer [ True]\n",
      "Iteration: 3100, Loss: 241.11497497558594 for layer [False]\n",
      "Iteration: 3200, Loss: 246.37625122070312 for layer [False]\n",
      "Iteration: 3300, Loss: 169.23800659179688 for layer [False]\n",
      "Iteration: 3400, Loss: 60.83808517456055 for layer [ True]\n",
      "Iteration: 3500, Loss: 206.79946899414062 for layer [False]\n",
      "Iteration: 3600, Loss: 50.04063034057617 for layer [ True]\n",
      "Iteration: 3700, Loss: 153.57931518554688 for layer [False]\n",
      "Iteration: 3800, Loss: 126.57020568847656 for layer [False]\n",
      "Iteration: 3900, Loss: 35.27741241455078 for layer [ True]\n",
      "Iteration: 4000, Loss: 43.715457916259766 for layer [ True]\n",
      "Iteration: 4100, Loss: 44.52228927612305 for layer [ True]\n",
      "Iteration: 4200, Loss: 67.10232543945312 for layer [ True]\n",
      "Iteration: 4300, Loss: 41.2368049621582 for layer [ True]\n",
      "Iteration: 4400, Loss: 35.91746139526367 for layer [ True]\n",
      "Iteration: 4500, Loss: 44.06982421875 for layer [ True]\n",
      "Iteration: 4600, Loss: 31.831344604492188 for layer [ True]\n",
      "Iteration: 4700, Loss: 71.70443725585938 for layer [False]\n",
      "Iteration: 4800, Loss: 44.237205505371094 for layer [False]\n",
      "Iteration: 4900, Loss: 56.13279724121094 for layer [False]\n",
      "Iteration: 5000, Loss: 46.00722885131836 for layer [ True]\n",
      "Iteration: 5100, Loss: 18.746692657470703 for layer [ True]\n",
      "Iteration: 5200, Loss: 39.861656188964844 for layer [False]\n",
      "Iteration: 5300, Loss: 18.802583694458008 for layer [ True]\n",
      "Iteration: 5400, Loss: 28.442138671875 for layer [False]\n",
      "Iteration: 5500, Loss: 20.577388763427734 for layer [ True]\n",
      "Iteration: 5600, Loss: 22.91992950439453 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.11058807373047 for layer [False]\n",
      "Iteration: 5800, Loss: 22.372228622436523 for layer [False]\n",
      "Iteration: 5900, Loss: 15.083083152770996 for layer [False]\n",
      "Iteration: 6000, Loss: 11.52608871459961 for layer [ True]\n",
      "Iteration: 6100, Loss: 15.578797340393066 for layer [False]\n",
      "Iteration: 6200, Loss: 14.15322208404541 for layer [False]\n",
      "Iteration: 6300, Loss: 11.797707557678223 for layer [False]\n",
      "Iteration: 6400, Loss: 13.404550552368164 for layer [ True]\n",
      "Iteration: 6500, Loss: 6.709799766540527 for layer [ True]\n",
      "Iteration: 6600, Loss: 7.654422283172607 for layer [ True]\n",
      "Iteration: 6700, Loss: 11.982147216796875 for layer [False]\n",
      "Iteration: 6800, Loss: 4.078560829162598 for layer [ True]\n",
      "Iteration: 6900, Loss: 21.204126358032227 for layer [False]\n",
      "Iteration: 7000, Loss: 8.46915340423584 for layer [False]\n",
      "Iteration: 7100, Loss: 10.070472717285156 for layer [False]\n",
      "Iteration: 7200, Loss: 8.848932266235352 for layer [False]\n",
      "Iteration: 7300, Loss: 4.082871437072754 for layer [ True]\n",
      "Iteration: 7400, Loss: 8.536575317382812 for layer [False]\n",
      "Iteration: 7500, Loss: 3.9040794372558594 for layer [ True]\n",
      "Iteration: 7600, Loss: 4.286270618438721 for layer [ True]\n",
      "Iteration: 7700, Loss: 9.560138702392578 for layer [False]\n",
      "Iteration: 7800, Loss: 7.284992694854736 for layer [False]\n",
      "Iteration: 7900, Loss: 4.61985445022583 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.2785274982452393 for layer [ True]\n",
      "Iteration: 8100, Loss: 2.8097259998321533 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.1117606163024902 for layer [ True]\n",
      "Iteration: 8300, Loss: 8.711033821105957 for layer [False]\n",
      "Iteration: 8400, Loss: 1.6468026638031006 for layer [ True]\n",
      "Iteration: 8500, Loss: 5.8373847007751465 for layer [False]\n",
      "Iteration: 8600, Loss: 4.2411909103393555 for layer [False]\n",
      "Iteration: 8700, Loss: 2.232429265975952 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.116152286529541 for layer [False]\n",
      "Iteration: 8900, Loss: 6.5606842041015625 for layer [False]\n",
      "Iteration: 9000, Loss: 1.1865047216415405 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.2724508047103882 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.6311715841293335 for layer [ True]\n",
      "Iteration: 9300, Loss: 4.393806457519531 for layer [False]\n",
      "Iteration: 9400, Loss: 4.590845584869385 for layer [False]\n",
      "Iteration: 9500, Loss: 1.2569148540496826 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.087446928024292 for layer [False]\n",
      "Iteration: 9700, Loss: 0.9042956233024597 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.7090551853179932 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.149904251098633 for layer [False]\n",
      "Iteration: 10000, Loss: 2.542977809906006 for layer [False]\n",
      "Iteration: 10100, Loss: 0.9730958938598633 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.5888763070106506 for layer [ True]\n",
      "Iteration: 10300, Loss: 2.9653751850128174 for layer [False]\n",
      "Iteration: 10400, Loss: 0.45296511054039 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.46871820092201233 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.8844454288482666 for layer [False]\n",
      "Iteration: 10700, Loss: 0.4870421886444092 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.0040581226348877 for layer [False]\n",
      "Iteration: 10900, Loss: 0.38650912046432495 for layer [ True]\n",
      "Iteration: 11000, Loss: 3.4324629306793213 for layer [False]\n",
      "Iteration: 11100, Loss: 2.5401251316070557 for layer [False]\n",
      "Iteration: 11200, Loss: 0.28099700808525085 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.2965151071548462 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.29146233201026917 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.0767250061035156 for layer [False]\n",
      "Iteration: 11600, Loss: 0.27730777859687805 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.15257425606250763 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.20628947019577026 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.19690866768360138 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.09590163826942444 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.13896521925926208 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.12356230616569519 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.07758564502000809 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.10467870533466339 for layer [ True]\n",
      "Iteration: 12500, Loss: 3.5046489238739014 for layer [False]\n",
      "Iteration: 12600, Loss: 3.8068830966949463 for layer [False]\n",
      "Iteration: 12700, Loss: 3.1435208320617676 for layer [False]\n",
      "Iteration: 12800, Loss: 0.07547587901353836 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.060717757791280746 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.056020502001047134 for layer [ True]\n",
      "Iteration: 13100, Loss: 2.973043441772461 for layer [False]\n",
      "Iteration: 13200, Loss: 0.046211447566747665 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.159446716308594 for layer [False]\n",
      "Iteration: 13400, Loss: 0.03440333530306816 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.04378129914402962 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.03387147933244705 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.012895054183900356 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.025295207276940346 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.021411413326859474 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.7476366758346558 for layer [False]\n",
      "Iteration: 14100, Loss: 0.02295711264014244 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.594008922576904 for layer [False]\n",
      "Iteration: 14300, Loss: 0.009306487627327442 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.141411781311035 for layer [False]\n",
      "Iteration: 14500, Loss: 2.0327699184417725 for layer [False]\n",
      "Iteration: 14600, Loss: 1.9144681692123413 for layer [False]\n",
      "Iteration: 14700, Loss: 2.739287853240967 for layer [False]\n",
      "Iteration: 14800, Loss: 2.510599374771118 for layer [False]\n",
      "Iteration: 14900, Loss: 3.2112298011779785 for layer [False]\n",
      "Iteration: 15000, Loss: 0.005389100406318903 for layer [ True]\n",
      "Iteration: 15100, Loss: 2.378002166748047 for layer [False]\n",
      "Iteration: 15200, Loss: 5.635707855224609 for layer [False]\n",
      "Iteration: 15300, Loss: 3.716557264328003 for layer [False]\n",
      "Iteration: 15400, Loss: 1.986202359199524 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0024189369287341833 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0031727224122732878 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.0036711362190544605 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.0696027278900146 for layer [False]\n",
      "Iteration: 15900, Loss: 0.001715041697025299 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0024850869085639715 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.5984854698181152 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0008631503442302346 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0015781159745529294 for layer [ True]\n",
      "Iteration: 16400, Loss: 1.8564471006393433 for layer [False]\n",
      "Iteration: 16500, Loss: 3.9049460887908936 for layer [False]\n",
      "Iteration: 16600, Loss: 2.3022191524505615 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015666794497519732 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0006312891491688788 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0008222508477047086 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.000862190208863467 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.9097100496292114 for layer [False]\n",
      "Iteration: 17200, Loss: 2.365753650665283 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0004863507638219744 for layer [ True]\n",
      "Iteration: 17400, Loss: 2.8928136825561523 for layer [False]\n",
      "Iteration: 17500, Loss: 6.150445938110352 for layer [False]\n",
      "Iteration: 17600, Loss: 2.3581082820892334 for layer [False]\n",
      "Iteration: 17700, Loss: 0.00046346543240360916 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.7316229343414307 for layer [False]\n",
      "Iteration: 17900, Loss: 2.7838127613067627 for layer [False]\n",
      "Iteration: 18000, Loss: 2.0498569011688232 for layer [False]\n",
      "Iteration: 18100, Loss: 0.000725330610293895 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.000470257451524958 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.373231887817383 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0003499793529044837 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0006340542458929121 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.0384981632232666 for layer [False]\n",
      "Iteration: 18700, Loss: 2.5020384788513184 for layer [False]\n",
      "Iteration: 18800, Loss: 2.4677960872650146 for layer [False]\n",
      "Iteration: 18900, Loss: 1.0458662509918213 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00017975547234527767 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.83397376537323 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0013162994291633368 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.887745976448059 for layer [False]\n",
      "Iteration: 19400, Loss: 3.544436454772949 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005389289581216872 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003605071979109198 for layer [ True]\n",
      "Iteration: 19700, Loss: 2.3050355911254883 for layer [False]\n",
      "Iteration: 19800, Loss: 2.1070406436920166 for layer [False]\n",
      "Iteration: 19900, Loss: 1.5290316343307495 for layer [False]\n",
      "Iteration: 20000, Loss: 1.506668210029602 for layer [False]\n",
      "Iteration: 20100, Loss: 3.3466975688934326 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0009107356891036034 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.3215014934539795 for layer [False]\n",
      "Iteration: 20400, Loss: 5.9295496612321585e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00016561252414248884 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.00019370074733160436 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.3586618900299072 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0006681910599581897 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0019259750843048096 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.404142141342163 for layer [False]\n",
      "Iteration: 21100, Loss: 4.660133361816406 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0005458265659399331 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.9592959880828857 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00022120542416814715 for layer [ True]\n",
      "Iteration: 21500, Loss: 1.6853073835372925 for layer [False]\n",
      "Iteration: 21600, Loss: 2.6706042289733887 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00018082468886859715 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00024410717014688998 for layer [ True]\n",
      "Iteration: 21900, Loss: 1.9516546726226807 for layer [False]\n",
      "Iteration: 22000, Loss: 2.2783944606781006 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00016576114285271615 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00058459717547521 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.050833225250244 for layer [False]\n",
      "Iteration: 22400, Loss: 3.289896249771118 for layer [False]\n",
      "Iteration: 22500, Loss: 1.2060343027114868 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00013404521450866014 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.0001561366079840809 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00011774080485338345 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0005243855412118137 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0002568351337686181 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00031859625596553087 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0005500167608261108 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0002539226843509823 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00016997079364955425 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.000501549628097564 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0001897039037430659 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.927197813987732 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0008312229765579104 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0002227638615295291 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00029803108191117644 for layer [ True]\n",
      "Iteration: 24100, Loss: 7.093272142810747e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.0167903900146484 for layer [False]\n",
      "Iteration: 24300, Loss: 2.4165477752685547 for layer [False]\n",
      "Iteration: 24400, Loss: 3.1411595344543457 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00013705425953958184 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00010858220775844529 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.8190248012542725 for layer [False]\n",
      "Iteration: 24800, Loss: 3.2079646587371826 for layer [False]\n",
      "Iteration: 24900, Loss: 9.296814823755994e-05 for layer [ True]\n",
      "Step 3500 | Loss: 0.000745\n",
      "Step 3600 | Loss: 0.000714\n",
      "Step 3700 | Loss: 0.000691\n",
      "Step 3800 | Loss: 0.000671\n",
      "Step 3900 | Loss: 0.000652\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2016.86572265625 for layer [False]\n",
      "Iteration: 100, Loss: 306.22613525390625 for layer [ True]\n",
      "Iteration: 200, Loss: 2007.3758544921875 for layer [False]\n",
      "Iteration: 300, Loss: 1651.7557373046875 for layer [False]\n",
      "Iteration: 400, Loss: 231.84152221679688 for layer [ True]\n",
      "Iteration: 500, Loss: 317.93975830078125 for layer [ True]\n",
      "Iteration: 600, Loss: 1515.9727783203125 for layer [False]\n",
      "Iteration: 700, Loss: 217.14222717285156 for layer [ True]\n",
      "Iteration: 800, Loss: 1237.52734375 for layer [False]\n",
      "Iteration: 900, Loss: 1047.190185546875 for layer [False]\n",
      "Iteration: 1000, Loss: 147.23155212402344 for layer [ True]\n",
      "Iteration: 1100, Loss: 956.002685546875 for layer [False]\n",
      "Iteration: 1200, Loss: 1121.103759765625 for layer [False]\n",
      "Iteration: 1300, Loss: 1036.9764404296875 for layer [False]\n",
      "Iteration: 1400, Loss: 217.5194549560547 for layer [ True]\n",
      "Iteration: 1500, Loss: 286.69989013671875 for layer [ True]\n",
      "Iteration: 1600, Loss: 121.5702133178711 for layer [ True]\n",
      "Iteration: 1700, Loss: 114.50080871582031 for layer [ True]\n",
      "Iteration: 1800, Loss: 652.658935546875 for layer [False]\n",
      "Iteration: 1900, Loss: 105.7879867553711 for layer [ True]\n",
      "Iteration: 2000, Loss: 92.36052703857422 for layer [ True]\n",
      "Iteration: 2100, Loss: 150.48666381835938 for layer [ True]\n",
      "Iteration: 2200, Loss: 99.84622955322266 for layer [ True]\n",
      "Iteration: 2300, Loss: 483.69183349609375 for layer [False]\n",
      "Iteration: 2400, Loss: 110.67398834228516 for layer [ True]\n",
      "Iteration: 2500, Loss: 161.97366333007812 for layer [ True]\n",
      "Iteration: 2600, Loss: 112.35633850097656 for layer [ True]\n",
      "Iteration: 2700, Loss: 79.07513427734375 for layer [ True]\n",
      "Iteration: 2800, Loss: 344.5886535644531 for layer [False]\n",
      "Iteration: 2900, Loss: 277.1624450683594 for layer [False]\n",
      "Iteration: 3000, Loss: 72.28267669677734 for layer [ True]\n",
      "Iteration: 3100, Loss: 243.04013061523438 for layer [False]\n",
      "Iteration: 3200, Loss: 248.7010040283203 for layer [False]\n",
      "Iteration: 3300, Loss: 170.45318603515625 for layer [False]\n",
      "Iteration: 3400, Loss: 63.20859146118164 for layer [ True]\n",
      "Iteration: 3500, Loss: 208.40126037597656 for layer [False]\n",
      "Iteration: 3600, Loss: 52.0655517578125 for layer [ True]\n",
      "Iteration: 3700, Loss: 154.30905151367188 for layer [False]\n",
      "Iteration: 3800, Loss: 128.3832550048828 for layer [False]\n",
      "Iteration: 3900, Loss: 37.0360107421875 for layer [ True]\n",
      "Iteration: 4000, Loss: 45.60639190673828 for layer [ True]\n",
      "Iteration: 4100, Loss: 46.88727951049805 for layer [ True]\n",
      "Iteration: 4200, Loss: 69.78961944580078 for layer [ True]\n",
      "Iteration: 4300, Loss: 43.02457046508789 for layer [ True]\n",
      "Iteration: 4400, Loss: 36.87804412841797 for layer [ True]\n",
      "Iteration: 4500, Loss: 46.196224212646484 for layer [ True]\n",
      "Iteration: 4600, Loss: 33.25440979003906 for layer [ True]\n",
      "Iteration: 4700, Loss: 72.08123016357422 for layer [False]\n",
      "Iteration: 4800, Loss: 44.17277145385742 for layer [False]\n",
      "Iteration: 4900, Loss: 56.87320327758789 for layer [False]\n",
      "Iteration: 5000, Loss: 47.95806121826172 for layer [ True]\n",
      "Iteration: 5100, Loss: 19.544477462768555 for layer [ True]\n",
      "Iteration: 5200, Loss: 40.7053337097168 for layer [False]\n",
      "Iteration: 5300, Loss: 19.61857795715332 for layer [ True]\n",
      "Iteration: 5400, Loss: 28.82831573486328 for layer [False]\n",
      "Iteration: 5500, Loss: 21.053781509399414 for layer [ True]\n",
      "Iteration: 5600, Loss: 23.8505802154541 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.957477569580078 for layer [False]\n",
      "Iteration: 5800, Loss: 22.69379425048828 for layer [False]\n",
      "Iteration: 5900, Loss: 14.446731567382812 for layer [False]\n",
      "Iteration: 6000, Loss: 12.00296401977539 for layer [ True]\n",
      "Iteration: 6100, Loss: 16.301904678344727 for layer [False]\n",
      "Iteration: 6200, Loss: 14.527236938476562 for layer [False]\n",
      "Iteration: 6300, Loss: 12.094972610473633 for layer [False]\n",
      "Iteration: 6400, Loss: 13.86369514465332 for layer [ True]\n",
      "Iteration: 6500, Loss: 7.0317606925964355 for layer [ True]\n",
      "Iteration: 6600, Loss: 7.996341228485107 for layer [ True]\n",
      "Iteration: 6700, Loss: 12.45669937133789 for layer [False]\n",
      "Iteration: 6800, Loss: 4.176562309265137 for layer [ True]\n",
      "Iteration: 6900, Loss: 23.46424674987793 for layer [False]\n",
      "Iteration: 7000, Loss: 7.813206195831299 for layer [False]\n",
      "Iteration: 7100, Loss: 10.249974250793457 for layer [False]\n",
      "Iteration: 7200, Loss: 9.29534912109375 for layer [False]\n",
      "Iteration: 7300, Loss: 4.179320335388184 for layer [ True]\n",
      "Iteration: 7400, Loss: 8.764548301696777 for layer [False]\n",
      "Iteration: 7500, Loss: 4.071505546569824 for layer [ True]\n",
      "Iteration: 7600, Loss: 4.574717044830322 for layer [ True]\n",
      "Iteration: 7700, Loss: 8.772054672241211 for layer [False]\n",
      "Iteration: 7800, Loss: 7.262658596038818 for layer [False]\n",
      "Iteration: 7900, Loss: 4.752602577209473 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.3299334049224854 for layer [ True]\n",
      "Iteration: 8100, Loss: 2.9778008460998535 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.219700336456299 for layer [ True]\n",
      "Iteration: 8300, Loss: 8.776174545288086 for layer [False]\n",
      "Iteration: 8400, Loss: 1.7142480611801147 for layer [ True]\n",
      "Iteration: 8500, Loss: 5.806959629058838 for layer [False]\n",
      "Iteration: 8600, Loss: 4.161338806152344 for layer [False]\n",
      "Iteration: 8700, Loss: 2.3466954231262207 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.114783763885498 for layer [False]\n",
      "Iteration: 8900, Loss: 6.429121017456055 for layer [False]\n",
      "Iteration: 9000, Loss: 1.2578465938568115 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.3304522037506104 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.7112581729888916 for layer [ True]\n",
      "Iteration: 9300, Loss: 4.213366508483887 for layer [False]\n",
      "Iteration: 9400, Loss: 4.357982158660889 for layer [False]\n",
      "Iteration: 9500, Loss: 1.3036245107650757 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.067017078399658 for layer [False]\n",
      "Iteration: 9700, Loss: 0.9604495167732239 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.7352538704872131 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.64891242980957 for layer [False]\n",
      "Iteration: 10000, Loss: 2.450195074081421 for layer [False]\n",
      "Iteration: 10100, Loss: 1.0271186828613281 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.6207355260848999 for layer [ True]\n",
      "Iteration: 10300, Loss: 2.732142925262451 for layer [False]\n",
      "Iteration: 10400, Loss: 0.4689176082611084 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.48359161615371704 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.7586095333099365 for layer [False]\n",
      "Iteration: 10700, Loss: 0.5162818431854248 for layer [ True]\n",
      "Iteration: 10800, Loss: 2.8097293376922607 for layer [False]\n",
      "Iteration: 10900, Loss: 0.413013219833374 for layer [ True]\n",
      "Iteration: 11000, Loss: 3.6358747482299805 for layer [False]\n",
      "Iteration: 11100, Loss: 2.1655688285827637 for layer [False]\n",
      "Iteration: 11200, Loss: 0.29131507873535156 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.3226154148578644 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.30929163098335266 for layer [ True]\n",
      "Iteration: 11500, Loss: 1.9121525287628174 for layer [False]\n",
      "Iteration: 11600, Loss: 0.2918987572193146 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.1630578190088272 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.21866005659103394 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2106362283229828 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.10479992628097534 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.1473664939403534 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.13192540407180786 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.08326399326324463 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.11161816120147705 for layer [ True]\n",
      "Iteration: 12500, Loss: 3.2776308059692383 for layer [False]\n",
      "Iteration: 12600, Loss: 3.529494285583496 for layer [False]\n",
      "Iteration: 12700, Loss: 3.022817373275757 for layer [False]\n",
      "Iteration: 12800, Loss: 0.07863333821296692 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.06699672341346741 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.05825066938996315 for layer [ True]\n",
      "Iteration: 13100, Loss: 2.675434112548828 for layer [False]\n",
      "Iteration: 13200, Loss: 0.05002840235829353 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.6291539669036865 for layer [False]\n",
      "Iteration: 13400, Loss: 0.03733459487557411 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.046689048409461975 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.03618510067462921 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.013165700249373913 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.028471793979406357 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.023357145488262177 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.582434058189392 for layer [False]\n",
      "Iteration: 14100, Loss: 0.023479044437408447 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.224276542663574 for layer [False]\n",
      "Iteration: 14300, Loss: 0.009792538359761238 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.6619884967803955 for layer [False]\n",
      "Iteration: 14500, Loss: 1.8042194843292236 for layer [False]\n",
      "Iteration: 14600, Loss: 1.8074311017990112 for layer [False]\n",
      "Iteration: 14700, Loss: 2.40212082862854 for layer [False]\n",
      "Iteration: 14800, Loss: 2.345320701599121 for layer [False]\n",
      "Iteration: 14900, Loss: 2.955122947692871 for layer [False]\n",
      "Iteration: 15000, Loss: 0.005584131460636854 for layer [ True]\n",
      "Iteration: 15100, Loss: 2.4064857959747314 for layer [False]\n",
      "Iteration: 15200, Loss: 5.189691543579102 for layer [False]\n",
      "Iteration: 15300, Loss: 3.18683123588562 for layer [False]\n",
      "Iteration: 15400, Loss: 1.5847244262695312 for layer [False]\n",
      "Iteration: 15500, Loss: 0.002625707071274519 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0034207352437078953 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.003794540883973241 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.8522605895996094 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0018468863563612103 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0026498856022953987 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.379927396774292 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0009418546105735004 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0017704257043078542 for layer [ True]\n",
      "Iteration: 16400, Loss: 1.7997243404388428 for layer [False]\n",
      "Iteration: 16500, Loss: 3.6511189937591553 for layer [False]\n",
      "Iteration: 16600, Loss: 2.305997848510742 for layer [False]\n",
      "Iteration: 16700, Loss: 0.001681148656643927 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.00057600176660344 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009303176775574684 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.00085123919416219 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.6065722703933716 for layer [False]\n",
      "Iteration: 17200, Loss: 2.064526081085205 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0005512291099876165 for layer [ True]\n",
      "Iteration: 17400, Loss: 2.699371814727783 for layer [False]\n",
      "Iteration: 17500, Loss: 5.664662837982178 for layer [False]\n",
      "Iteration: 17600, Loss: 2.31472110748291 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0005582363228313625 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.509114980697632 for layer [False]\n",
      "Iteration: 17900, Loss: 2.4782330989837646 for layer [False]\n",
      "Iteration: 18000, Loss: 1.7916916608810425 for layer [False]\n",
      "Iteration: 18100, Loss: 0.000638163008261472 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0004143980040680617 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.185143947601318 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00040821731090545654 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008512012427672744 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.7221490144729614 for layer [False]\n",
      "Iteration: 18700, Loss: 2.4970130920410156 for layer [False]\n",
      "Iteration: 18800, Loss: 2.4781529903411865 for layer [False]\n",
      "Iteration: 18900, Loss: 0.9615437984466553 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00017495789506938308 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.7488070726394653 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0015650062123313546 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.6854033470153809 for layer [False]\n",
      "Iteration: 19400, Loss: 3.4447741508483887 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005202294560149312 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0004044943198096007 for layer [ True]\n",
      "Iteration: 19700, Loss: 2.4048266410827637 for layer [False]\n",
      "Iteration: 19800, Loss: 2.166921377182007 for layer [False]\n",
      "Iteration: 19900, Loss: 1.589001178741455 for layer [False]\n",
      "Iteration: 20000, Loss: 1.4538205862045288 for layer [False]\n",
      "Iteration: 20100, Loss: 3.0317769050598145 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0010807077633216977 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.0147171020507812 for layer [False]\n",
      "Iteration: 20400, Loss: 4.96989960083738e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00021852752252016217 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.00032285935594700277 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.270558476448059 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0005948495236225426 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0017560896230861545 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.2079662084579468 for layer [False]\n",
      "Iteration: 21100, Loss: 4.367678642272949 for layer [False]\n",
      "Iteration: 21200, Loss: 0.000968039152212441 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.6136269569396973 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00023107843298930675 for layer [ True]\n",
      "Iteration: 21500, Loss: 1.7756032943725586 for layer [False]\n",
      "Iteration: 21600, Loss: 2.489964485168457 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003937031142413616 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0004895249148830771 for layer [ True]\n",
      "Iteration: 21900, Loss: 1.8659940958023071 for layer [False]\n",
      "Iteration: 22000, Loss: 2.2416815757751465 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0004238960682414472 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0001060625072568655 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.573734998703003 for layer [False]\n",
      "Iteration: 22400, Loss: 2.922312021255493 for layer [False]\n",
      "Iteration: 22500, Loss: 1.1321104764938354 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00015053266542963684 for layer [ True]\n",
      "Iteration: 22700, Loss: 7.600893877679482e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0005865820567123592 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0006688306457363069 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.000739291834179312 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0003945821081288159 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00024894686066545546 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00026291501126252115 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00016340063302777708 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0005407141870819032 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0001451846765121445 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.804423451423645 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0010218087118119001 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00022947047546040267 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00029113958589732647 for layer [ True]\n",
      "Iteration: 24100, Loss: 6.645262328675017e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 2.889516830444336 for layer [False]\n",
      "Iteration: 24300, Loss: 2.1223175525665283 for layer [False]\n",
      "Iteration: 24400, Loss: 2.8028340339660645 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0002341909275855869 for layer [ True]\n",
      "Iteration: 24600, Loss: 4.7331519454019144e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.68282413482666 for layer [False]\n",
      "Iteration: 24800, Loss: 2.864610195159912 for layer [False]\n",
      "Iteration: 24900, Loss: 0.0001274831738555804 for layer [ True]\n",
      "Step 4000 | Loss: 0.000636\n",
      "Step 4100 | Loss: 0.000619\n",
      "Step 4200 | Loss: 0.000597\n",
      "Step 4300 | Loss: 0.000569\n",
      "Step 4400 | Loss: 0.000552\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 2006.0377197265625 for layer [False]\n",
      "Iteration: 100, Loss: 306.66375732421875 for layer [ True]\n",
      "Iteration: 200, Loss: 2001.4605712890625 for layer [False]\n",
      "Iteration: 300, Loss: 1637.4075927734375 for layer [False]\n",
      "Iteration: 400, Loss: 234.4344024658203 for layer [ True]\n",
      "Iteration: 500, Loss: 315.7635192871094 for layer [ True]\n",
      "Iteration: 600, Loss: 1506.368408203125 for layer [False]\n",
      "Iteration: 700, Loss: 221.13400268554688 for layer [ True]\n",
      "Iteration: 800, Loss: 1230.715087890625 for layer [False]\n",
      "Iteration: 900, Loss: 1049.3175048828125 for layer [False]\n",
      "Iteration: 1000, Loss: 145.4154052734375 for layer [ True]\n",
      "Iteration: 1100, Loss: 945.1356811523438 for layer [False]\n",
      "Iteration: 1200, Loss: 1109.338623046875 for layer [False]\n",
      "Iteration: 1300, Loss: 1031.5863037109375 for layer [False]\n",
      "Iteration: 1400, Loss: 222.14427185058594 for layer [ True]\n",
      "Iteration: 1500, Loss: 294.16229248046875 for layer [ True]\n",
      "Iteration: 1600, Loss: 124.82405090332031 for layer [ True]\n",
      "Iteration: 1700, Loss: 114.73316192626953 for layer [ True]\n",
      "Iteration: 1800, Loss: 645.767578125 for layer [False]\n",
      "Iteration: 1900, Loss: 108.39373016357422 for layer [ True]\n",
      "Iteration: 2000, Loss: 95.0074462890625 for layer [ True]\n",
      "Iteration: 2100, Loss: 153.7770233154297 for layer [ True]\n",
      "Iteration: 2200, Loss: 102.76744842529297 for layer [ True]\n",
      "Iteration: 2300, Loss: 481.0000915527344 for layer [False]\n",
      "Iteration: 2400, Loss: 116.4697265625 for layer [ True]\n",
      "Iteration: 2500, Loss: 171.09678649902344 for layer [ True]\n",
      "Iteration: 2600, Loss: 115.06820678710938 for layer [ True]\n",
      "Iteration: 2700, Loss: 81.43693542480469 for layer [ True]\n",
      "Iteration: 2800, Loss: 341.93621826171875 for layer [False]\n",
      "Iteration: 2900, Loss: 275.4187316894531 for layer [False]\n",
      "Iteration: 3000, Loss: 75.36421203613281 for layer [ True]\n",
      "Iteration: 3100, Loss: 240.3324737548828 for layer [False]\n",
      "Iteration: 3200, Loss: 246.8572998046875 for layer [False]\n",
      "Iteration: 3300, Loss: 167.76382446289062 for layer [False]\n",
      "Iteration: 3400, Loss: 66.8938980102539 for layer [ True]\n",
      "Iteration: 3500, Loss: 204.08181762695312 for layer [False]\n",
      "Iteration: 3600, Loss: 53.79419708251953 for layer [ True]\n",
      "Iteration: 3700, Loss: 153.1244659423828 for layer [False]\n",
      "Iteration: 3800, Loss: 126.81209564208984 for layer [False]\n",
      "Iteration: 3900, Loss: 38.65644836425781 for layer [ True]\n",
      "Iteration: 4000, Loss: 47.4002571105957 for layer [ True]\n",
      "Iteration: 4100, Loss: 49.562435150146484 for layer [ True]\n",
      "Iteration: 4200, Loss: 72.45578002929688 for layer [ True]\n",
      "Iteration: 4300, Loss: 44.10942459106445 for layer [ True]\n",
      "Iteration: 4400, Loss: 38.20785140991211 for layer [ True]\n",
      "Iteration: 4500, Loss: 48.40013122558594 for layer [ True]\n",
      "Iteration: 4600, Loss: 35.51960372924805 for layer [ True]\n",
      "Iteration: 4700, Loss: 70.67552947998047 for layer [False]\n",
      "Iteration: 4800, Loss: 43.36198043823242 for layer [False]\n",
      "Iteration: 4900, Loss: 56.76457214355469 for layer [False]\n",
      "Iteration: 5000, Loss: 50.66493606567383 for layer [ True]\n",
      "Iteration: 5100, Loss: 20.528459548950195 for layer [ True]\n",
      "Iteration: 5200, Loss: 39.98051071166992 for layer [False]\n",
      "Iteration: 5300, Loss: 20.32033920288086 for layer [ True]\n",
      "Iteration: 5400, Loss: 28.23250961303711 for layer [False]\n",
      "Iteration: 5500, Loss: 21.894620895385742 for layer [ True]\n",
      "Iteration: 5600, Loss: 25.043336868286133 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.90924835205078 for layer [False]\n",
      "Iteration: 5800, Loss: 22.43644142150879 for layer [False]\n",
      "Iteration: 5900, Loss: 14.238373756408691 for layer [False]\n",
      "Iteration: 6000, Loss: 12.513053894042969 for layer [ True]\n",
      "Iteration: 6100, Loss: 17.401811599731445 for layer [False]\n",
      "Iteration: 6200, Loss: 14.788151741027832 for layer [False]\n",
      "Iteration: 6300, Loss: 12.557819366455078 for layer [False]\n",
      "Iteration: 6400, Loss: 14.384586334228516 for layer [ True]\n",
      "Iteration: 6500, Loss: 7.496316909790039 for layer [ True]\n",
      "Iteration: 6600, Loss: 8.36887264251709 for layer [ True]\n",
      "Iteration: 6700, Loss: 12.955512046813965 for layer [False]\n",
      "Iteration: 6800, Loss: 4.315415859222412 for layer [ True]\n",
      "Iteration: 6900, Loss: 25.18321418762207 for layer [False]\n",
      "Iteration: 7000, Loss: 7.7692766189575195 for layer [False]\n",
      "Iteration: 7100, Loss: 10.330829620361328 for layer [False]\n",
      "Iteration: 7200, Loss: 10.45910358428955 for layer [False]\n",
      "Iteration: 7300, Loss: 4.2834248542785645 for layer [ True]\n",
      "Iteration: 7400, Loss: 9.57042407989502 for layer [False]\n",
      "Iteration: 7500, Loss: 4.226177215576172 for layer [ True]\n",
      "Iteration: 7600, Loss: 4.845213890075684 for layer [ True]\n",
      "Iteration: 7700, Loss: 9.267717361450195 for layer [False]\n",
      "Iteration: 7800, Loss: 7.688482284545898 for layer [False]\n",
      "Iteration: 7900, Loss: 4.964804649353027 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.446889877319336 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.1011312007904053 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.334423303604126 for layer [ True]\n",
      "Iteration: 8300, Loss: 8.894803047180176 for layer [False]\n",
      "Iteration: 8400, Loss: 1.761644721031189 for layer [ True]\n",
      "Iteration: 8500, Loss: 6.315437316894531 for layer [False]\n",
      "Iteration: 8600, Loss: 4.332385063171387 for layer [False]\n",
      "Iteration: 8700, Loss: 2.4696195125579834 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.331468105316162 for layer [False]\n",
      "Iteration: 8900, Loss: 6.863678455352783 for layer [False]\n",
      "Iteration: 9000, Loss: 1.3472845554351807 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.4099242687225342 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.7611463069915771 for layer [ True]\n",
      "Iteration: 9300, Loss: 4.433718204498291 for layer [False]\n",
      "Iteration: 9400, Loss: 4.437407493591309 for layer [False]\n",
      "Iteration: 9500, Loss: 1.3818135261535645 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.140721321105957 for layer [False]\n",
      "Iteration: 9700, Loss: 0.9958868026733398 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.7732939720153809 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.517726421356201 for layer [False]\n",
      "Iteration: 10000, Loss: 2.4789133071899414 for layer [False]\n",
      "Iteration: 10100, Loss: 1.0743725299835205 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.6734129190444946 for layer [ True]\n",
      "Iteration: 10300, Loss: 2.8790524005889893 for layer [False]\n",
      "Iteration: 10400, Loss: 0.48396411538124084 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.4937635064125061 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.7971129417419434 for layer [False]\n",
      "Iteration: 10700, Loss: 0.5516591668128967 for layer [ True]\n",
      "Iteration: 10800, Loss: 2.992436408996582 for layer [False]\n",
      "Iteration: 10900, Loss: 0.43701961636543274 for layer [ True]\n",
      "Iteration: 11000, Loss: 4.202069282531738 for layer [False]\n",
      "Iteration: 11100, Loss: 2.196227788925171 for layer [False]\n",
      "Iteration: 11200, Loss: 0.29981058835983276 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.34522145986557007 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3122166693210602 for layer [ True]\n",
      "Iteration: 11500, Loss: 1.8507004976272583 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3019711971282959 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.1671927273273468 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2221982479095459 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2178133726119995 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1040964424610138 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.15409597754478455 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.13685478270053864 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.0857170820236206 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.11431525647640228 for layer [ True]\n",
      "Iteration: 12500, Loss: 3.5372776985168457 for layer [False]\n",
      "Iteration: 12600, Loss: 3.89835786819458 for layer [False]\n",
      "Iteration: 12700, Loss: 3.2515597343444824 for layer [False]\n",
      "Iteration: 12800, Loss: 0.07947079837322235 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.07049679011106491 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.057297319173812866 for layer [ True]\n",
      "Iteration: 13100, Loss: 2.9406185150146484 for layer [False]\n",
      "Iteration: 13200, Loss: 0.05205833166837692 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.6207234859466553 for layer [False]\n",
      "Iteration: 13400, Loss: 0.0366864837706089 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.04710637405514717 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.036688707768917084 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.012452241964638233 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.02953689731657505 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.024377333000302315 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.5910242795944214 for layer [False]\n",
      "Iteration: 14100, Loss: 0.022401029244065285 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.253398418426514 for layer [False]\n",
      "Iteration: 14300, Loss: 0.009432482533156872 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.5749011039733887 for layer [False]\n",
      "Iteration: 14500, Loss: 2.125669002532959 for layer [False]\n",
      "Iteration: 14600, Loss: 1.8825695514678955 for layer [False]\n",
      "Iteration: 14700, Loss: 2.3873252868652344 for layer [False]\n",
      "Iteration: 14800, Loss: 2.5960683822631836 for layer [False]\n",
      "Iteration: 14900, Loss: 3.1148312091827393 for layer [False]\n",
      "Iteration: 15000, Loss: 0.005240344442427158 for layer [ True]\n",
      "Iteration: 15100, Loss: 2.686377763748169 for layer [False]\n",
      "Iteration: 15200, Loss: 5.7068705558776855 for layer [False]\n",
      "Iteration: 15300, Loss: 3.159876585006714 for layer [False]\n",
      "Iteration: 15400, Loss: 1.5151110887527466 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0026678459253162146 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0033090843353420496 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.003847213229164481 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.8869580030441284 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0017580930143594742 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0027604582719504833 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.331878423690796 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0009742220863699913 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0018486425979062915 for layer [ True]\n",
      "Iteration: 16400, Loss: 1.9793486595153809 for layer [False]\n",
      "Iteration: 16500, Loss: 3.699352264404297 for layer [False]\n",
      "Iteration: 16600, Loss: 2.4349958896636963 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015360522083938122 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.000640848942566663 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009398586116731167 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008852804312482476 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.5481488704681396 for layer [False]\n",
      "Iteration: 17200, Loss: 2.2164204120635986 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006834485102444887 for layer [ True]\n",
      "Iteration: 17400, Loss: 2.985215425491333 for layer [False]\n",
      "Iteration: 17500, Loss: 5.855222225189209 for layer [False]\n",
      "Iteration: 17600, Loss: 2.473841905593872 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0007348100189119577 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.6457784175872803 for layer [False]\n",
      "Iteration: 17900, Loss: 2.4058687686920166 for layer [False]\n",
      "Iteration: 18000, Loss: 1.764675259590149 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0005488747847266495 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.00043894717236980796 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.351283550262451 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00035394198494032025 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007981820963323116 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.5979400873184204 for layer [False]\n",
      "Iteration: 18700, Loss: 2.573737621307373 for layer [False]\n",
      "Iteration: 18800, Loss: 2.7158377170562744 for layer [False]\n",
      "Iteration: 18900, Loss: 1.0968348979949951 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00020167551701888442 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.846588134765625 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0014989306218922138 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.7203941345214844 for layer [False]\n",
      "Iteration: 19400, Loss: 3.6896004676818848 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005513273063115776 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00040336212259717286 for layer [ True]\n",
      "Iteration: 19700, Loss: 2.6428751945495605 for layer [False]\n",
      "Iteration: 19800, Loss: 2.414567708969116 for layer [False]\n",
      "Iteration: 19900, Loss: 1.8617218732833862 for layer [False]\n",
      "Iteration: 20000, Loss: 1.4913723468780518 for layer [False]\n",
      "Iteration: 20100, Loss: 3.119541645050049 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0014303026255220175 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.012444257736206 for layer [False]\n",
      "Iteration: 20400, Loss: 6.79531876812689e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00025207563885487616 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0006365370354615152 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.4095147848129272 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00038013121229596436 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0022810210939496756 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.2004077434539795 for layer [False]\n",
      "Iteration: 21100, Loss: 4.878706932067871 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0006000907742418349 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.4887585639953613 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00020138334366492927 for layer [ True]\n",
      "Iteration: 21500, Loss: 1.963083028793335 for layer [False]\n",
      "Iteration: 21600, Loss: 2.446986675262451 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0002289101976202801 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0003082063340116292 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.0635411739349365 for layer [False]\n",
      "Iteration: 22000, Loss: 2.640993595123291 for layer [False]\n",
      "Iteration: 22100, Loss: 8.045845606829971e-05 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00019949182751588523 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.764078378677368 for layer [False]\n",
      "Iteration: 22400, Loss: 2.889008045196533 for layer [False]\n",
      "Iteration: 22500, Loss: 1.1304525136947632 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00022163758694659919 for layer [ True]\n",
      "Iteration: 22700, Loss: 3.966638178098947e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0006255753105506301 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.001126179820857942 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00047925920807756484 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0007975290645845234 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00027081166626885533 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00020528877212200314 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00012735786731354892 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0002862715336959809 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00020843272795900702 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.8737674951553345 for layer [False]\n",
      "Iteration: 23800, Loss: 0.000654840434435755 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00014986173482611775 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0002730510022956878 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.0001323524775216356 for layer [ True]\n",
      "Iteration: 24200, Loss: 2.8329081535339355 for layer [False]\n",
      "Iteration: 24300, Loss: 2.0669753551483154 for layer [False]\n",
      "Iteration: 24400, Loss: 2.8276994228363037 for layer [False]\n",
      "Iteration: 24500, Loss: 4.2556104745017365e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 7.043476944090798e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 2.905714988708496 for layer [False]\n",
      "Iteration: 24800, Loss: 2.7751293182373047 for layer [False]\n",
      "Iteration: 24900, Loss: 9.221887739840895e-05 for layer [ True]\n",
      "Step 4500 | Loss: 0.000539\n",
      "Step 4600 | Loss: 0.000531\n",
      "Step 4700 | Loss: 0.000523\n",
      "Step 4800 | Loss: 0.000515\n",
      "Step 4900 | Loss: 0.000510\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1994.274658203125 for layer [False]\n",
      "Iteration: 100, Loss: 308.67926025390625 for layer [ True]\n",
      "Iteration: 200, Loss: 1990.827880859375 for layer [False]\n",
      "Iteration: 300, Loss: 1621.301513671875 for layer [False]\n",
      "Iteration: 400, Loss: 236.168701171875 for layer [ True]\n",
      "Iteration: 500, Loss: 312.7262878417969 for layer [ True]\n",
      "Iteration: 600, Loss: 1493.3857421875 for layer [False]\n",
      "Iteration: 700, Loss: 223.27413940429688 for layer [ True]\n",
      "Iteration: 800, Loss: 1218.1536865234375 for layer [False]\n",
      "Iteration: 900, Loss: 1045.5240478515625 for layer [False]\n",
      "Iteration: 1000, Loss: 144.69290161132812 for layer [ True]\n",
      "Iteration: 1100, Loss: 934.12109375 for layer [False]\n",
      "Iteration: 1200, Loss: 1100.2847900390625 for layer [False]\n",
      "Iteration: 1300, Loss: 1024.3531494140625 for layer [False]\n",
      "Iteration: 1400, Loss: 223.6653594970703 for layer [ True]\n",
      "Iteration: 1500, Loss: 300.67156982421875 for layer [ True]\n",
      "Iteration: 1600, Loss: 126.6996078491211 for layer [ True]\n",
      "Iteration: 1700, Loss: 115.22335815429688 for layer [ True]\n",
      "Iteration: 1800, Loss: 639.5064086914062 for layer [False]\n",
      "Iteration: 1900, Loss: 110.70825958251953 for layer [ True]\n",
      "Iteration: 2000, Loss: 96.61750030517578 for layer [ True]\n",
      "Iteration: 2100, Loss: 154.99473571777344 for layer [ True]\n",
      "Iteration: 2200, Loss: 104.14718627929688 for layer [ True]\n",
      "Iteration: 2300, Loss: 476.6882629394531 for layer [False]\n",
      "Iteration: 2400, Loss: 120.2587661743164 for layer [ True]\n",
      "Iteration: 2500, Loss: 176.67581176757812 for layer [ True]\n",
      "Iteration: 2600, Loss: 116.59623718261719 for layer [ True]\n",
      "Iteration: 2700, Loss: 83.09577941894531 for layer [ True]\n",
      "Iteration: 2800, Loss: 338.24090576171875 for layer [False]\n",
      "Iteration: 2900, Loss: 272.0756530761719 for layer [False]\n",
      "Iteration: 3000, Loss: 76.75483703613281 for layer [ True]\n",
      "Iteration: 3100, Loss: 236.7558135986328 for layer [False]\n",
      "Iteration: 3200, Loss: 243.931884765625 for layer [False]\n",
      "Iteration: 3300, Loss: 165.3414764404297 for layer [False]\n",
      "Iteration: 3400, Loss: 68.7701187133789 for layer [ True]\n",
      "Iteration: 3500, Loss: 200.54592895507812 for layer [False]\n",
      "Iteration: 3600, Loss: 55.25825500488281 for layer [ True]\n",
      "Iteration: 3700, Loss: 149.84266662597656 for layer [False]\n",
      "Iteration: 3800, Loss: 124.84369659423828 for layer [False]\n",
      "Iteration: 3900, Loss: 39.81048583984375 for layer [ True]\n",
      "Iteration: 4000, Loss: 48.72332763671875 for layer [ True]\n",
      "Iteration: 4100, Loss: 50.99411392211914 for layer [ True]\n",
      "Iteration: 4200, Loss: 74.12720489501953 for layer [ True]\n",
      "Iteration: 4300, Loss: 44.74030685424805 for layer [ True]\n",
      "Iteration: 4400, Loss: 38.71799087524414 for layer [ True]\n",
      "Iteration: 4500, Loss: 49.83980941772461 for layer [ True]\n",
      "Iteration: 4600, Loss: 36.564727783203125 for layer [ True]\n",
      "Iteration: 4700, Loss: 69.36689758300781 for layer [False]\n",
      "Iteration: 4800, Loss: 42.578269958496094 for layer [False]\n",
      "Iteration: 4900, Loss: 55.84231948852539 for layer [False]\n",
      "Iteration: 5000, Loss: 51.97993469238281 for layer [ True]\n",
      "Iteration: 5100, Loss: 21.03895378112793 for layer [ True]\n",
      "Iteration: 5200, Loss: 38.927696228027344 for layer [False]\n",
      "Iteration: 5300, Loss: 20.91880989074707 for layer [ True]\n",
      "Iteration: 5400, Loss: 27.589008331298828 for layer [False]\n",
      "Iteration: 5500, Loss: 22.485769271850586 for layer [ True]\n",
      "Iteration: 5600, Loss: 25.82533836364746 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.767513275146484 for layer [False]\n",
      "Iteration: 5800, Loss: 22.349853515625 for layer [False]\n",
      "Iteration: 5900, Loss: 14.187905311584473 for layer [False]\n",
      "Iteration: 6000, Loss: 12.841242790222168 for layer [ True]\n",
      "Iteration: 6100, Loss: 18.108543395996094 for layer [False]\n",
      "Iteration: 6200, Loss: 15.078338623046875 for layer [False]\n",
      "Iteration: 6300, Loss: 12.56963062286377 for layer [False]\n",
      "Iteration: 6400, Loss: 14.785897254943848 for layer [ True]\n",
      "Iteration: 6500, Loss: 7.70742130279541 for layer [ True]\n",
      "Iteration: 6600, Loss: 8.55797004699707 for layer [ True]\n",
      "Iteration: 6700, Loss: 13.817912101745605 for layer [False]\n",
      "Iteration: 6800, Loss: 4.439289569854736 for layer [ True]\n",
      "Iteration: 6900, Loss: 27.778696060180664 for layer [False]\n",
      "Iteration: 7000, Loss: 8.22762393951416 for layer [False]\n",
      "Iteration: 7100, Loss: 10.692144393920898 for layer [False]\n",
      "Iteration: 7200, Loss: 11.55319881439209 for layer [False]\n",
      "Iteration: 7300, Loss: 4.335299491882324 for layer [ True]\n",
      "Iteration: 7400, Loss: 10.146062850952148 for layer [False]\n",
      "Iteration: 7500, Loss: 4.31477689743042 for layer [ True]\n",
      "Iteration: 7600, Loss: 4.9770073890686035 for layer [ True]\n",
      "Iteration: 7700, Loss: 9.634899139404297 for layer [False]\n",
      "Iteration: 7800, Loss: 7.857038974761963 for layer [False]\n",
      "Iteration: 7900, Loss: 5.055891990661621 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.5506463050842285 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.1696879863739014 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.4330782890319824 for layer [ True]\n",
      "Iteration: 8300, Loss: 9.445076942443848 for layer [False]\n",
      "Iteration: 8400, Loss: 1.7988702058792114 for layer [ True]\n",
      "Iteration: 8500, Loss: 6.9524359703063965 for layer [False]\n",
      "Iteration: 8600, Loss: 4.621580600738525 for layer [False]\n",
      "Iteration: 8700, Loss: 2.5287606716156006 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.523451328277588 for layer [False]\n",
      "Iteration: 8900, Loss: 7.485498905181885 for layer [False]\n",
      "Iteration: 9000, Loss: 1.3934078216552734 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.4486085176467896 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.7836613655090332 for layer [ True]\n",
      "Iteration: 9300, Loss: 4.67396879196167 for layer [False]\n",
      "Iteration: 9400, Loss: 4.706789016723633 for layer [False]\n",
      "Iteration: 9500, Loss: 1.421019196510315 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.285688638687134 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0052348375320435 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8069813251495361 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.720006465911865 for layer [False]\n",
      "Iteration: 10000, Loss: 2.594194173812866 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1060292720794678 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7041164040565491 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.148613452911377 for layer [False]\n",
      "Iteration: 10400, Loss: 0.49818387627601624 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5088847875595093 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.8775954246520996 for layer [False]\n",
      "Iteration: 10700, Loss: 0.5812314748764038 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.2010700702667236 for layer [False]\n",
      "Iteration: 10900, Loss: 0.4633558988571167 for layer [ True]\n",
      "Iteration: 11000, Loss: 4.914287567138672 for layer [False]\n",
      "Iteration: 11100, Loss: 2.2745473384857178 for layer [False]\n",
      "Iteration: 11200, Loss: 0.31177929043769836 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.36647364497184753 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.32055097818374634 for layer [ True]\n",
      "Iteration: 11500, Loss: 1.9783921241760254 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3190816640853882 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.17464579641819 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2313585728406906 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2319282442331314 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.10736384987831116 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.16206218302249908 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.14329589903354645 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.09091398119926453 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.12067750841379166 for layer [ True]\n",
      "Iteration: 12500, Loss: 3.7936389446258545 for layer [False]\n",
      "Iteration: 12600, Loss: 4.102814674377441 for layer [False]\n",
      "Iteration: 12700, Loss: 3.4001057147979736 for layer [False]\n",
      "Iteration: 12800, Loss: 0.08343993127346039 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.07531893253326416 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.05899327993392944 for layer [ True]\n",
      "Iteration: 13100, Loss: 3.261965036392212 for layer [False]\n",
      "Iteration: 13200, Loss: 0.05497635900974274 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.703939199447632 for layer [False]\n",
      "Iteration: 13400, Loss: 0.038228727877140045 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.048991609364748 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.03870309144258499 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.013146739453077316 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03165838494896889 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.025934861972928047 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.731515884399414 for layer [False]\n",
      "Iteration: 14100, Loss: 0.023508314043283463 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.3518452644348145 for layer [False]\n",
      "Iteration: 14300, Loss: 0.00942088384181261 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.558704137802124 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2060546875 for layer [False]\n",
      "Iteration: 14600, Loss: 2.0731146335601807 for layer [False]\n",
      "Iteration: 14700, Loss: 2.436955451965332 for layer [False]\n",
      "Iteration: 14800, Loss: 2.749250650405884 for layer [False]\n",
      "Iteration: 14900, Loss: 3.4020016193389893 for layer [False]\n",
      "Iteration: 15000, Loss: 0.005430325865745544 for layer [ True]\n",
      "Iteration: 15100, Loss: 3.243302822113037 for layer [False]\n",
      "Iteration: 15200, Loss: 6.100543022155762 for layer [False]\n",
      "Iteration: 15300, Loss: 3.238781452178955 for layer [False]\n",
      "Iteration: 15400, Loss: 1.527537226676941 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0027936596889048815 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0034394599497318268 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.003948227968066931 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.9353686571121216 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0018270554719492793 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0028849116060882807 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.410625696182251 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0010415270226076245 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0019363423343747854 for layer [ True]\n",
      "Iteration: 16400, Loss: 2.2070045471191406 for layer [False]\n",
      "Iteration: 16500, Loss: 3.7960264682769775 for layer [False]\n",
      "Iteration: 16600, Loss: 2.4981579780578613 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015188343822956085 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007368844817392528 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009770174510776997 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008457303047180176 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.5524953603744507 for layer [False]\n",
      "Iteration: 17200, Loss: 2.306028127670288 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006802309071645141 for layer [ True]\n",
      "Iteration: 17400, Loss: 3.3650472164154053 for layer [False]\n",
      "Iteration: 17500, Loss: 6.279532432556152 for layer [False]\n",
      "Iteration: 17600, Loss: 2.667783260345459 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0006866553449071944 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.7502739429473877 for layer [False]\n",
      "Iteration: 17900, Loss: 2.4583210945129395 for layer [False]\n",
      "Iteration: 18000, Loss: 1.7411789894104004 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0005538069526664913 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0004740289878100157 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.5405378341674805 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00037307836464606225 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0006966571090742946 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.6048802137374878 for layer [False]\n",
      "Iteration: 18700, Loss: 2.7727367877960205 for layer [False]\n",
      "Iteration: 18800, Loss: 2.91273832321167 for layer [False]\n",
      "Iteration: 18900, Loss: 1.197134017944336 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00021754195040557534 for layer [ True]\n",
      "Iteration: 19100, Loss: 1.9981545209884644 for layer [False]\n",
      "Iteration: 19200, Loss: 0.001286679646000266 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.8395745754241943 for layer [False]\n",
      "Iteration: 19400, Loss: 3.9730727672576904 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005886309663765132 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003840188728645444 for layer [ True]\n",
      "Iteration: 19700, Loss: 2.9219589233398438 for layer [False]\n",
      "Iteration: 19800, Loss: 2.726205348968506 for layer [False]\n",
      "Iteration: 19900, Loss: 2.149768590927124 for layer [False]\n",
      "Iteration: 20000, Loss: 1.609862208366394 for layer [False]\n",
      "Iteration: 20100, Loss: 3.4889912605285645 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0015546894865110517 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.0467605590820312 for layer [False]\n",
      "Iteration: 20400, Loss: 6.652482261415571e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00026696582790464163 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.000758400303311646 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.574479341506958 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00021983409533277154 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.002971406327560544 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.2603211402893066 for layer [False]\n",
      "Iteration: 21100, Loss: 5.262874603271484 for layer [False]\n",
      "Iteration: 21200, Loss: 0.000397422380046919 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.5776278972625732 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0005736437742598355 for layer [ True]\n",
      "Iteration: 21500, Loss: 2.197420835494995 for layer [False]\n",
      "Iteration: 21600, Loss: 2.649406671524048 for layer [False]\n",
      "Iteration: 21700, Loss: 0.000691109977196902 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0006372905918397009 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.31476092338562 for layer [False]\n",
      "Iteration: 22000, Loss: 2.9571220874786377 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0002906846348196268 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00019916465680580586 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.8391904830932617 for layer [False]\n",
      "Iteration: 22400, Loss: 3.1343705654144287 for layer [False]\n",
      "Iteration: 22500, Loss: 1.2009680271148682 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00044766865903511643 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.0001182412015623413 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.000451936008175835 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0010632313787937164 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00025746619212441146 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00018251576693728566 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0004111932357773185 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0004342834872659296 for layer [ True]\n",
      "Iteration: 23400, Loss: 8.691554830875248e-05 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0002611533855088055 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00031144655076786876 for layer [ True]\n",
      "Iteration: 23700, Loss: 1.9875054359436035 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0005698846071027219 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00013659484102390707 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0002672915579751134 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.0001240804704139009 for layer [ True]\n",
      "Iteration: 24200, Loss: 2.986351251602173 for layer [False]\n",
      "Iteration: 24300, Loss: 2.0464775562286377 for layer [False]\n",
      "Iteration: 24400, Loss: 2.821836233139038 for layer [False]\n",
      "Iteration: 24500, Loss: 3.2171301427297294e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 6.758373638149351e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 3.1947293281555176 for layer [False]\n",
      "Iteration: 24800, Loss: 2.8508801460266113 for layer [False]\n",
      "Iteration: 24900, Loss: 5.790641444036737e-05 for layer [ True]\n",
      "Step 5000 | Loss: 0.000505\n",
      "Step 5100 | Loss: 0.000501\n",
      "Step 5200 | Loss: 0.000498\n",
      "Step 5300 | Loss: 0.000495\n",
      "Step 5400 | Loss: 0.000493\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1985.821533203125 for layer [False]\n",
      "Iteration: 100, Loss: 310.81109619140625 for layer [ True]\n",
      "Iteration: 200, Loss: 1982.66259765625 for layer [False]\n",
      "Iteration: 300, Loss: 1610.50341796875 for layer [False]\n",
      "Iteration: 400, Loss: 237.7749481201172 for layer [ True]\n",
      "Iteration: 500, Loss: 310.98223876953125 for layer [ True]\n",
      "Iteration: 600, Loss: 1485.2327880859375 for layer [False]\n",
      "Iteration: 700, Loss: 225.0569610595703 for layer [ True]\n",
      "Iteration: 800, Loss: 1211.2899169921875 for layer [False]\n",
      "Iteration: 900, Loss: 1044.340576171875 for layer [False]\n",
      "Iteration: 1000, Loss: 143.9930877685547 for layer [ True]\n",
      "Iteration: 1100, Loss: 927.6483154296875 for layer [False]\n",
      "Iteration: 1200, Loss: 1095.4012451171875 for layer [False]\n",
      "Iteration: 1300, Loss: 1019.5059814453125 for layer [False]\n",
      "Iteration: 1400, Loss: 226.2230682373047 for layer [ True]\n",
      "Iteration: 1500, Loss: 305.1551208496094 for layer [ True]\n",
      "Iteration: 1600, Loss: 128.2357940673828 for layer [ True]\n",
      "Iteration: 1700, Loss: 116.03949737548828 for layer [ True]\n",
      "Iteration: 1800, Loss: 635.3770751953125 for layer [False]\n",
      "Iteration: 1900, Loss: 113.01470184326172 for layer [ True]\n",
      "Iteration: 2000, Loss: 98.41751098632812 for layer [ True]\n",
      "Iteration: 2100, Loss: 156.94374084472656 for layer [ True]\n",
      "Iteration: 2200, Loss: 106.3127212524414 for layer [ True]\n",
      "Iteration: 2300, Loss: 472.837646484375 for layer [False]\n",
      "Iteration: 2400, Loss: 123.14175415039062 for layer [ True]\n",
      "Iteration: 2500, Loss: 181.77919006347656 for layer [ True]\n",
      "Iteration: 2600, Loss: 118.46098327636719 for layer [ True]\n",
      "Iteration: 2700, Loss: 84.64875793457031 for layer [ True]\n",
      "Iteration: 2800, Loss: 336.1696472167969 for layer [False]\n",
      "Iteration: 2900, Loss: 269.4522705078125 for layer [False]\n",
      "Iteration: 3000, Loss: 78.54147338867188 for layer [ True]\n",
      "Iteration: 3100, Loss: 234.05702209472656 for layer [False]\n",
      "Iteration: 3200, Loss: 242.27883911132812 for layer [False]\n",
      "Iteration: 3300, Loss: 163.99346923828125 for layer [False]\n",
      "Iteration: 3400, Loss: 71.1722183227539 for layer [ True]\n",
      "Iteration: 3500, Loss: 198.7029266357422 for layer [False]\n",
      "Iteration: 3600, Loss: 56.81431198120117 for layer [ True]\n",
      "Iteration: 3700, Loss: 147.5952911376953 for layer [False]\n",
      "Iteration: 3800, Loss: 123.29615020751953 for layer [False]\n",
      "Iteration: 3900, Loss: 40.92030334472656 for layer [ True]\n",
      "Iteration: 4000, Loss: 49.96710968017578 for layer [ True]\n",
      "Iteration: 4100, Loss: 52.61457824707031 for layer [ True]\n",
      "Iteration: 4200, Loss: 76.13822937011719 for layer [ True]\n",
      "Iteration: 4300, Loss: 45.55345916748047 for layer [ True]\n",
      "Iteration: 4400, Loss: 39.69393539428711 for layer [ True]\n",
      "Iteration: 4500, Loss: 51.58485412597656 for layer [ True]\n",
      "Iteration: 4600, Loss: 37.93009567260742 for layer [ True]\n",
      "Iteration: 4700, Loss: 68.20597839355469 for layer [False]\n",
      "Iteration: 4800, Loss: 41.99793243408203 for layer [False]\n",
      "Iteration: 4900, Loss: 55.19221878051758 for layer [False]\n",
      "Iteration: 5000, Loss: 53.36923599243164 for layer [ True]\n",
      "Iteration: 5100, Loss: 21.71453857421875 for layer [ True]\n",
      "Iteration: 5200, Loss: 38.37949752807617 for layer [False]\n",
      "Iteration: 5300, Loss: 21.500141143798828 for layer [ True]\n",
      "Iteration: 5400, Loss: 27.285762786865234 for layer [False]\n",
      "Iteration: 5500, Loss: 23.122188568115234 for layer [ True]\n",
      "Iteration: 5600, Loss: 26.637800216674805 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.79693603515625 for layer [False]\n",
      "Iteration: 5800, Loss: 22.169721603393555 for layer [False]\n",
      "Iteration: 5900, Loss: 14.04953384399414 for layer [False]\n",
      "Iteration: 6000, Loss: 13.264202117919922 for layer [ True]\n",
      "Iteration: 6100, Loss: 18.66945457458496 for layer [False]\n",
      "Iteration: 6200, Loss: 15.123784065246582 for layer [False]\n",
      "Iteration: 6300, Loss: 12.681086540222168 for layer [False]\n",
      "Iteration: 6400, Loss: 15.16041374206543 for layer [ True]\n",
      "Iteration: 6500, Loss: 7.991883277893066 for layer [ True]\n",
      "Iteration: 6600, Loss: 8.783015251159668 for layer [ True]\n",
      "Iteration: 6700, Loss: 14.331557273864746 for layer [False]\n",
      "Iteration: 6800, Loss: 4.601987361907959 for layer [ True]\n",
      "Iteration: 6900, Loss: 29.360612869262695 for layer [False]\n",
      "Iteration: 7000, Loss: 8.355372428894043 for layer [False]\n",
      "Iteration: 7100, Loss: 10.732645988464355 for layer [False]\n",
      "Iteration: 7200, Loss: 12.14334774017334 for layer [False]\n",
      "Iteration: 7300, Loss: 4.40208101272583 for layer [ True]\n",
      "Iteration: 7400, Loss: 10.434122085571289 for layer [False]\n",
      "Iteration: 7500, Loss: 4.416388988494873 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.113108158111572 for layer [ True]\n",
      "Iteration: 7700, Loss: 9.945575714111328 for layer [False]\n",
      "Iteration: 7800, Loss: 8.02389907836914 for layer [False]\n",
      "Iteration: 7900, Loss: 5.205111980438232 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.6810460090637207 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2451090812683105 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.523538827896118 for layer [ True]\n",
      "Iteration: 8300, Loss: 9.939489364624023 for layer [False]\n",
      "Iteration: 8400, Loss: 1.8345168828964233 for layer [ True]\n",
      "Iteration: 8500, Loss: 7.426787376403809 for layer [False]\n",
      "Iteration: 8600, Loss: 4.810633182525635 for layer [False]\n",
      "Iteration: 8700, Loss: 2.5927422046661377 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.590157985687256 for layer [False]\n",
      "Iteration: 8900, Loss: 7.7601447105407715 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4385465383529663 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.4902153015136719 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.7981213331222534 for layer [ True]\n",
      "Iteration: 9300, Loss: 4.859490394592285 for layer [False]\n",
      "Iteration: 9400, Loss: 4.898451328277588 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4587888717651367 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.4173836708068848 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0275620222091675 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8365052342414856 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.783158302307129 for layer [False]\n",
      "Iteration: 10000, Loss: 2.6446990966796875 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1420649290084839 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7292641997337341 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.370547294616699 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5129410028457642 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5249980688095093 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.917955160140991 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6067012548446655 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.31890606880188 for layer [False]\n",
      "Iteration: 10900, Loss: 0.47978636622428894 for layer [ True]\n",
      "Iteration: 11000, Loss: 5.380641937255859 for layer [False]\n",
      "Iteration: 11100, Loss: 2.330200672149658 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3246484696865082 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.38662776350975037 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.33016714453697205 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.0643723011016846 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3309706151485443 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.18182700872421265 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.24030491709709167 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2423585206270218 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11064819991588593 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.16960269212722778 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.14974625408649445 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.09541656076908112 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.12593922019004822 for layer [ True]\n",
      "Iteration: 12500, Loss: 3.96665096282959 for layer [False]\n",
      "Iteration: 12600, Loss: 4.39632511138916 for layer [False]\n",
      "Iteration: 12700, Loss: 3.494180202484131 for layer [False]\n",
      "Iteration: 12800, Loss: 0.08738254010677338 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.08021745830774307 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06082211807370186 for layer [ True]\n",
      "Iteration: 13100, Loss: 3.524766206741333 for layer [False]\n",
      "Iteration: 13200, Loss: 0.05743277072906494 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.7507922649383545 for layer [False]\n",
      "Iteration: 13400, Loss: 0.040177784860134125 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05112219601869583 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.040945857763290405 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.013851121068000793 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03399629890918732 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.027524573728442192 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.8021771907806396 for layer [False]\n",
      "Iteration: 14100, Loss: 0.024585912004113197 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.392966270446777 for layer [False]\n",
      "Iteration: 14300, Loss: 0.009552684612572193 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.6051015853881836 for layer [False]\n",
      "Iteration: 14500, Loss: 2.245656728744507 for layer [False]\n",
      "Iteration: 14600, Loss: 2.230868101119995 for layer [False]\n",
      "Iteration: 14700, Loss: 2.474363088607788 for layer [False]\n",
      "Iteration: 14800, Loss: 2.967670440673828 for layer [False]\n",
      "Iteration: 14900, Loss: 3.7516210079193115 for layer [False]\n",
      "Iteration: 15000, Loss: 0.005771487019956112 for layer [ True]\n",
      "Iteration: 15100, Loss: 3.6744511127471924 for layer [False]\n",
      "Iteration: 15200, Loss: 6.498745441436768 for layer [False]\n",
      "Iteration: 15300, Loss: 3.3734025955200195 for layer [False]\n",
      "Iteration: 15400, Loss: 1.5760489702224731 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0029515523929148912 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0036348362918943167 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004175770562142134 for layer [ True]\n",
      "Iteration: 15800, Loss: 1.9952698945999146 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0018972338875755668 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0030220274347811937 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.5187511444091797 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011048420565202832 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020008645951747894 for layer [ True]\n",
      "Iteration: 16400, Loss: 2.4268875122070312 for layer [False]\n",
      "Iteration: 16500, Loss: 3.814990758895874 for layer [False]\n",
      "Iteration: 16600, Loss: 2.6075758934020996 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015259271021932364 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007877309690229595 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010109668364748359 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008531352505087852 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.6027870178222656 for layer [False]\n",
      "Iteration: 17200, Loss: 2.3929216861724854 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006970259128138423 for layer [ True]\n",
      "Iteration: 17400, Loss: 3.723994255065918 for layer [False]\n",
      "Iteration: 17500, Loss: 6.7297186851501465 for layer [False]\n",
      "Iteration: 17600, Loss: 2.8102965354919434 for layer [False]\n",
      "Iteration: 17700, Loss: 0.00072611024370417 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.810760498046875 for layer [False]\n",
      "Iteration: 17900, Loss: 2.549065589904785 for layer [False]\n",
      "Iteration: 18000, Loss: 1.7960537672042847 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006081947358325124 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005104891606606543 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.66403341293335 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0003948575467802584 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0006830261554569006 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.6372830867767334 for layer [False]\n",
      "Iteration: 18700, Loss: 2.9123170375823975 for layer [False]\n",
      "Iteration: 18800, Loss: 3.0675718784332275 for layer [False]\n",
      "Iteration: 18900, Loss: 1.275983452796936 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00022731984790880233 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.1543850898742676 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0011806255206465721 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.9194117784500122 for layer [False]\n",
      "Iteration: 19400, Loss: 4.199450492858887 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006042568711563945 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00037658176734112203 for layer [ True]\n",
      "Iteration: 19700, Loss: 3.181934118270874 for layer [False]\n",
      "Iteration: 19800, Loss: 2.943713903427124 for layer [False]\n",
      "Iteration: 19900, Loss: 2.437678813934326 for layer [False]\n",
      "Iteration: 20000, Loss: 1.7349697351455688 for layer [False]\n",
      "Iteration: 20100, Loss: 3.776158332824707 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0017741775372996926 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.13421368598938 for layer [False]\n",
      "Iteration: 20400, Loss: 6.662098894594237e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0002751139982137829 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0008535028318874538 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.7314051389694214 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0001935601030709222 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0010023389477282763 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.329311490058899 for layer [False]\n",
      "Iteration: 21100, Loss: 5.5681304931640625 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0004693274386227131 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.6487197875976562 for layer [False]\n",
      "Iteration: 21400, Loss: 0.000774346524849534 for layer [ True]\n",
      "Iteration: 21500, Loss: 2.4093518257141113 for layer [False]\n",
      "Iteration: 21600, Loss: 2.7612712383270264 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0004644911678042263 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0003450809745118022 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.483532428741455 for layer [False]\n",
      "Iteration: 22000, Loss: 3.271749496459961 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00017119338735938072 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0002529214252717793 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.859816551208496 for layer [False]\n",
      "Iteration: 22400, Loss: 3.365300178527832 for layer [False]\n",
      "Iteration: 22500, Loss: 1.284233808517456 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00020058600057382137 for layer [ True]\n",
      "Iteration: 22700, Loss: 3.218030178686604e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0006799905677326024 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.001472204108722508 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00046334374928846955 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0007427575183100998 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00027047088951803744 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00021628758986480534 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0001442758657503873 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0003398568951524794 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00017428251157980412 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.1408371925354004 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006825929740443826 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00014844654651824385 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0002841549285221845 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.00012408531620167196 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.01214599609375 for layer [False]\n",
      "Iteration: 24300, Loss: 2.0404529571533203 for layer [False]\n",
      "Iteration: 24400, Loss: 2.9696788787841797 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0001584737910889089 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00015140125469770283 for layer [ True]\n",
      "Iteration: 24700, Loss: 3.4010848999023438 for layer [False]\n",
      "Iteration: 24800, Loss: 2.944370985031128 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00022707869356963784 for layer [ True]\n",
      "Step 5500 | Loss: 0.000490\n",
      "Step 5600 | Loss: 0.000488\n",
      "Step 5700 | Loss: 0.000486\n",
      "Step 5800 | Loss: 0.000485\n",
      "Step 5900 | Loss: 0.000483\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1982.732177734375 for layer [False]\n",
      "Iteration: 100, Loss: 312.3143005371094 for layer [ True]\n",
      "Iteration: 200, Loss: 1979.090087890625 for layer [False]\n",
      "Iteration: 300, Loss: 1604.619140625 for layer [False]\n",
      "Iteration: 400, Loss: 238.22792053222656 for layer [ True]\n",
      "Iteration: 500, Loss: 309.5210876464844 for layer [ True]\n",
      "Iteration: 600, Loss: 1480.94140625 for layer [False]\n",
      "Iteration: 700, Loss: 225.99664306640625 for layer [ True]\n",
      "Iteration: 800, Loss: 1207.679443359375 for layer [False]\n",
      "Iteration: 900, Loss: 1043.1070556640625 for layer [False]\n",
      "Iteration: 1000, Loss: 143.72499084472656 for layer [ True]\n",
      "Iteration: 1100, Loss: 923.6801147460938 for layer [False]\n",
      "Iteration: 1200, Loss: 1092.9296875 for layer [False]\n",
      "Iteration: 1300, Loss: 1017.2658081054688 for layer [False]\n",
      "Iteration: 1400, Loss: 227.2908172607422 for layer [ True]\n",
      "Iteration: 1500, Loss: 305.12921142578125 for layer [ True]\n",
      "Iteration: 1600, Loss: 129.30735778808594 for layer [ True]\n",
      "Iteration: 1700, Loss: 115.94573211669922 for layer [ True]\n",
      "Iteration: 1800, Loss: 634.4479370117188 for layer [False]\n",
      "Iteration: 1900, Loss: 113.88031768798828 for layer [ True]\n",
      "Iteration: 2000, Loss: 99.06814575195312 for layer [ True]\n",
      "Iteration: 2100, Loss: 157.13934326171875 for layer [ True]\n",
      "Iteration: 2200, Loss: 107.07987213134766 for layer [ True]\n",
      "Iteration: 2300, Loss: 471.0618896484375 for layer [False]\n",
      "Iteration: 2400, Loss: 123.24472045898438 for layer [ True]\n",
      "Iteration: 2500, Loss: 183.1229705810547 for layer [ True]\n",
      "Iteration: 2600, Loss: 118.38893127441406 for layer [ True]\n",
      "Iteration: 2700, Loss: 85.17596435546875 for layer [ True]\n",
      "Iteration: 2800, Loss: 336.2498779296875 for layer [False]\n",
      "Iteration: 2900, Loss: 268.2666931152344 for layer [False]\n",
      "Iteration: 3000, Loss: 79.18099975585938 for layer [ True]\n",
      "Iteration: 3100, Loss: 232.45130920410156 for layer [False]\n",
      "Iteration: 3200, Loss: 241.1923828125 for layer [False]\n",
      "Iteration: 3300, Loss: 163.50161743164062 for layer [False]\n",
      "Iteration: 3400, Loss: 72.39832305908203 for layer [ True]\n",
      "Iteration: 3500, Loss: 197.8909912109375 for layer [False]\n",
      "Iteration: 3600, Loss: 57.45661163330078 for layer [ True]\n",
      "Iteration: 3700, Loss: 146.2848358154297 for layer [False]\n",
      "Iteration: 3800, Loss: 122.3511962890625 for layer [False]\n",
      "Iteration: 3900, Loss: 41.3734130859375 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.28546142578125 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.23227310180664 for layer [ True]\n",
      "Iteration: 4200, Loss: 76.83684539794922 for layer [ True]\n",
      "Iteration: 4300, Loss: 45.6695556640625 for layer [ True]\n",
      "Iteration: 4400, Loss: 40.13479232788086 for layer [ True]\n",
      "Iteration: 4500, Loss: 52.35539245605469 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.484642028808594 for layer [ True]\n",
      "Iteration: 4700, Loss: 67.58037567138672 for layer [False]\n",
      "Iteration: 4800, Loss: 41.748661041259766 for layer [False]\n",
      "Iteration: 4900, Loss: 54.927703857421875 for layer [False]\n",
      "Iteration: 5000, Loss: 53.77481460571289 for layer [ True]\n",
      "Iteration: 5100, Loss: 22.03852081298828 for layer [ True]\n",
      "Iteration: 5200, Loss: 38.22451400756836 for layer [False]\n",
      "Iteration: 5300, Loss: 21.745647430419922 for layer [ True]\n",
      "Iteration: 5400, Loss: 27.137718200683594 for layer [False]\n",
      "Iteration: 5500, Loss: 23.415016174316406 for layer [ True]\n",
      "Iteration: 5600, Loss: 26.89637565612793 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.81536293029785 for layer [False]\n",
      "Iteration: 5800, Loss: 22.13547134399414 for layer [False]\n",
      "Iteration: 5900, Loss: 14.00993537902832 for layer [False]\n",
      "Iteration: 6000, Loss: 13.444639205932617 for layer [ True]\n",
      "Iteration: 6100, Loss: 18.70121955871582 for layer [False]\n",
      "Iteration: 6200, Loss: 15.214449882507324 for layer [False]\n",
      "Iteration: 6300, Loss: 12.613096237182617 for layer [False]\n",
      "Iteration: 6400, Loss: 15.251887321472168 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.121736526489258 for layer [ True]\n",
      "Iteration: 6600, Loss: 8.90235424041748 for layer [ True]\n",
      "Iteration: 6700, Loss: 14.675115585327148 for layer [False]\n",
      "Iteration: 6800, Loss: 4.675304412841797 for layer [ True]\n",
      "Iteration: 6900, Loss: 30.14959716796875 for layer [False]\n",
      "Iteration: 7000, Loss: 8.476191520690918 for layer [False]\n",
      "Iteration: 7100, Loss: 10.588824272155762 for layer [False]\n",
      "Iteration: 7200, Loss: 12.464276313781738 for layer [False]\n",
      "Iteration: 7300, Loss: 4.417588233947754 for layer [ True]\n",
      "Iteration: 7400, Loss: 10.60993480682373 for layer [False]\n",
      "Iteration: 7500, Loss: 4.448519229888916 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.168241024017334 for layer [ True]\n",
      "Iteration: 7700, Loss: 10.169589042663574 for layer [False]\n",
      "Iteration: 7800, Loss: 8.041397094726562 for layer [False]\n",
      "Iteration: 7900, Loss: 5.281257152557373 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.7495999336242676 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2665622234344482 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.564931631088257 for layer [ True]\n",
      "Iteration: 8300, Loss: 10.14787769317627 for layer [False]\n",
      "Iteration: 8400, Loss: 1.8540443181991577 for layer [ True]\n",
      "Iteration: 8500, Loss: 7.828446865081787 for layer [False]\n",
      "Iteration: 8600, Loss: 4.914644241333008 for layer [False]\n",
      "Iteration: 8700, Loss: 2.615586757659912 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.63051700592041 for layer [False]\n",
      "Iteration: 8900, Loss: 7.846651554107666 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4538410902023315 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5090972185134888 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.798868179321289 for layer [ True]\n",
      "Iteration: 9300, Loss: 4.9573798179626465 for layer [False]\n",
      "Iteration: 9400, Loss: 5.0519843101501465 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4814475774765015 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.4542925357818604 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0369952917099 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.851235568523407 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.736064910888672 for layer [False]\n",
      "Iteration: 10000, Loss: 2.7476818561553955 for layer [False]\n",
      "Iteration: 10100, Loss: 1.160224437713623 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7414727807044983 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.5186190605163574 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5256431102752686 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5318053960800171 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.9299569129943848 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6214662194252014 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.387852191925049 for layer [False]\n",
      "Iteration: 10900, Loss: 0.487678200006485 for layer [ True]\n",
      "Iteration: 11000, Loss: 5.601260185241699 for layer [False]\n",
      "Iteration: 11100, Loss: 2.3390021324157715 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3300957679748535 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.3974090814590454 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3326170742511749 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.1358039379119873 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3353477120399475 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.18439574539661407 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.24383634328842163 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.24621422588825226 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11142316460609436 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.17252430319786072 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1526126116514206 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.09751510620117188 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.12796129286289215 for layer [ True]\n",
      "Iteration: 12500, Loss: 4.109890460968018 for layer [False]\n",
      "Iteration: 12600, Loss: 4.515455722808838 for layer [False]\n",
      "Iteration: 12700, Loss: 3.599743604660034 for layer [False]\n",
      "Iteration: 12800, Loss: 0.08899974822998047 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.08290348947048187 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06147969514131546 for layer [ True]\n",
      "Iteration: 13100, Loss: 3.6480462551116943 for layer [False]\n",
      "Iteration: 13200, Loss: 0.058438509702682495 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.777745246887207 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04103894531726837 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.0521782822906971 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04203632101416588 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.014110825024545193 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03514258563518524 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.02844001166522503 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.87631356716156 for layer [False]\n",
      "Iteration: 14100, Loss: 0.02542703039944172 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.416588306427002 for layer [False]\n",
      "Iteration: 14300, Loss: 0.009610039182007313 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.612018585205078 for layer [False]\n",
      "Iteration: 14500, Loss: 2.1967687606811523 for layer [False]\n",
      "Iteration: 14600, Loss: 2.3201992511749268 for layer [False]\n",
      "Iteration: 14700, Loss: 2.5049233436584473 for layer [False]\n",
      "Iteration: 14800, Loss: 3.043076753616333 for layer [False]\n",
      "Iteration: 14900, Loss: 4.032902240753174 for layer [False]\n",
      "Iteration: 15000, Loss: 0.005934500601142645 for layer [ True]\n",
      "Iteration: 15100, Loss: 3.9606947898864746 for layer [False]\n",
      "Iteration: 15200, Loss: 6.755471229553223 for layer [False]\n",
      "Iteration: 15300, Loss: 3.4588394165039062 for layer [False]\n",
      "Iteration: 15400, Loss: 1.6370413303375244 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0030783554539084435 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0037214558105915785 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004192120861262083 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.055000066757202 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0019477453315630555 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0031021395698189735 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.5817556381225586 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011238412698730826 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020310780964791775 for layer [ True]\n",
      "Iteration: 16400, Loss: 2.5659735202789307 for layer [False]\n",
      "Iteration: 16500, Loss: 3.804457426071167 for layer [False]\n",
      "Iteration: 16600, Loss: 2.6180953979492188 for layer [False]\n",
      "Iteration: 16700, Loss: 0.001548622502014041 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007950126891955733 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010261287679895759 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008518290123902261 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.6641472578048706 for layer [False]\n",
      "Iteration: 17200, Loss: 2.412813186645508 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0007016708259470761 for layer [ True]\n",
      "Iteration: 17400, Loss: 3.9435994625091553 for layer [False]\n",
      "Iteration: 17500, Loss: 6.982474327087402 for layer [False]\n",
      "Iteration: 17600, Loss: 2.9097282886505127 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0007476400933228433 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.8408915996551514 for layer [False]\n",
      "Iteration: 17900, Loss: 2.6381912231445312 for layer [False]\n",
      "Iteration: 18000, Loss: 1.8178561925888062 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006228467100299895 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005204391200095415 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.69791316986084 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0004055835597682744 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0006947370711714029 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.6324007511138916 for layer [False]\n",
      "Iteration: 18700, Loss: 3.020124673843384 for layer [False]\n",
      "Iteration: 18800, Loss: 3.180135726928711 for layer [False]\n",
      "Iteration: 18900, Loss: 1.3184576034545898 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00022753870871383697 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.2624282836914062 for layer [False]\n",
      "Iteration: 19200, Loss: 0.001202049432322383 for layer [ True]\n",
      "Iteration: 19300, Loss: 1.9731286764144897 for layer [False]\n",
      "Iteration: 19400, Loss: 4.32532262802124 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006123219500295818 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003783476713579148 for layer [ True]\n",
      "Iteration: 19700, Loss: 3.324944019317627 for layer [False]\n",
      "Iteration: 19800, Loss: 2.9782748222351074 for layer [False]\n",
      "Iteration: 19900, Loss: 2.606694221496582 for layer [False]\n",
      "Iteration: 20000, Loss: 1.8151607513427734 for layer [False]\n",
      "Iteration: 20100, Loss: 3.971140146255493 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0020584051962941885 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.1996963024139404 for layer [False]\n",
      "Iteration: 20400, Loss: 6.58243297948502e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00028118139016442 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009273179457522929 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.840200424194336 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00020793000294361264 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0006940991152077913 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.4145383834838867 for layer [False]\n",
      "Iteration: 21100, Loss: 5.811367511749268 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0004471430729608983 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.677109956741333 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00037068207166157663 for layer [ True]\n",
      "Iteration: 21500, Loss: 2.5079519748687744 for layer [False]\n",
      "Iteration: 21600, Loss: 2.8663859367370605 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00012145438813604414 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0006392766954377294 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.583678722381592 for layer [False]\n",
      "Iteration: 22000, Loss: 3.3740670680999756 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0005370755097828805 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00010536356421653181 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.817756175994873 for layer [False]\n",
      "Iteration: 22400, Loss: 3.55704927444458 for layer [False]\n",
      "Iteration: 22500, Loss: 1.3140348196029663 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00015567804803140461 for layer [ True]\n",
      "Iteration: 22700, Loss: 6.205803219927475e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0007021988858468831 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0002887447190005332 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0005636600544676185 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0011055641807615757 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00023422151571139693 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00041978232911787927 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00019163919205311686 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00013409754319582134 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0003101104812230915 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.2117714881896973 for layer [False]\n",
      "Iteration: 23800, Loss: 0.000752458639908582 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00012584413343574852 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00011619203723967075 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.00012716473429463804 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.0570874214172363 for layer [False]\n",
      "Iteration: 24300, Loss: 2.035417079925537 for layer [False]\n",
      "Iteration: 24400, Loss: 3.034776210784912 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0002771087165456265 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00017564783047419041 for layer [ True]\n",
      "Iteration: 24700, Loss: 3.5000100135803223 for layer [False]\n",
      "Iteration: 24800, Loss: 3.0255355834960938 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00020674466213677078 for layer [ True]\n",
      "Step 6000 | Loss: 0.000482\n",
      "Step 6100 | Loss: 0.000481\n",
      "Step 6200 | Loss: 0.000481\n",
      "Step 6300 | Loss: 0.000480\n",
      "Step 6400 | Loss: 0.000479\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1978.060791015625 for layer [False]\n",
      "Iteration: 100, Loss: 313.09375 for layer [ True]\n",
      "Iteration: 200, Loss: 1974.0633544921875 for layer [False]\n",
      "Iteration: 300, Loss: 1597.919189453125 for layer [False]\n",
      "Iteration: 400, Loss: 237.94680786132812 for layer [ True]\n",
      "Iteration: 500, Loss: 308.74530029296875 for layer [ True]\n",
      "Iteration: 600, Loss: 1476.761474609375 for layer [False]\n",
      "Iteration: 700, Loss: 226.08697509765625 for layer [ True]\n",
      "Iteration: 800, Loss: 1202.621826171875 for layer [False]\n",
      "Iteration: 900, Loss: 1040.610107421875 for layer [False]\n",
      "Iteration: 1000, Loss: 143.91412353515625 for layer [ True]\n",
      "Iteration: 1100, Loss: 920.3258666992188 for layer [False]\n",
      "Iteration: 1200, Loss: 1089.09423828125 for layer [False]\n",
      "Iteration: 1300, Loss: 1014.78466796875 for layer [False]\n",
      "Iteration: 1400, Loss: 227.37136840820312 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.4117736816406 for layer [ True]\n",
      "Iteration: 1600, Loss: 130.03399658203125 for layer [ True]\n",
      "Iteration: 1700, Loss: 116.00106048583984 for layer [ True]\n",
      "Iteration: 1800, Loss: 632.4584350585938 for layer [False]\n",
      "Iteration: 1900, Loss: 113.77586364746094 for layer [ True]\n",
      "Iteration: 2000, Loss: 99.1715316772461 for layer [ True]\n",
      "Iteration: 2100, Loss: 156.66099548339844 for layer [ True]\n",
      "Iteration: 2200, Loss: 106.88446807861328 for layer [ True]\n",
      "Iteration: 2300, Loss: 468.90582275390625 for layer [False]\n",
      "Iteration: 2400, Loss: 122.83492279052734 for layer [ True]\n",
      "Iteration: 2500, Loss: 183.38589477539062 for layer [ True]\n",
      "Iteration: 2600, Loss: 118.16739654541016 for layer [ True]\n",
      "Iteration: 2700, Loss: 85.43433380126953 for layer [ True]\n",
      "Iteration: 2800, Loss: 335.41748046875 for layer [False]\n",
      "Iteration: 2900, Loss: 266.74627685546875 for layer [False]\n",
      "Iteration: 3000, Loss: 79.09882354736328 for layer [ True]\n",
      "Iteration: 3100, Loss: 230.73709106445312 for layer [False]\n",
      "Iteration: 3200, Loss: 239.78700256347656 for layer [False]\n",
      "Iteration: 3300, Loss: 162.50811767578125 for layer [False]\n",
      "Iteration: 3400, Loss: 72.5929183959961 for layer [ True]\n",
      "Iteration: 3500, Loss: 196.8118133544922 for layer [False]\n",
      "Iteration: 3600, Loss: 57.61642074584961 for layer [ True]\n",
      "Iteration: 3700, Loss: 145.14901733398438 for layer [False]\n",
      "Iteration: 3800, Loss: 121.44539642333984 for layer [False]\n",
      "Iteration: 3900, Loss: 41.55718994140625 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.209983825683594 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.27886199951172 for layer [ True]\n",
      "Iteration: 4200, Loss: 76.77407836914062 for layer [ True]\n",
      "Iteration: 4300, Loss: 45.63283920288086 for layer [ True]\n",
      "Iteration: 4400, Loss: 40.260677337646484 for layer [ True]\n",
      "Iteration: 4500, Loss: 52.56509017944336 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.39736557006836 for layer [ True]\n",
      "Iteration: 4700, Loss: 66.93730163574219 for layer [False]\n",
      "Iteration: 4800, Loss: 41.620452880859375 for layer [False]\n",
      "Iteration: 4900, Loss: 54.48761749267578 for layer [False]\n",
      "Iteration: 5000, Loss: 53.72957992553711 for layer [ True]\n",
      "Iteration: 5100, Loss: 22.14510154724121 for layer [ True]\n",
      "Iteration: 5200, Loss: 37.88920974731445 for layer [False]\n",
      "Iteration: 5300, Loss: 21.737064361572266 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.97189712524414 for layer [False]\n",
      "Iteration: 5500, Loss: 23.473180770874023 for layer [ True]\n",
      "Iteration: 5600, Loss: 26.94397735595703 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.76891326904297 for layer [False]\n",
      "Iteration: 5800, Loss: 22.14860725402832 for layer [False]\n",
      "Iteration: 5900, Loss: 14.031153678894043 for layer [False]\n",
      "Iteration: 6000, Loss: 13.427860260009766 for layer [ True]\n",
      "Iteration: 6100, Loss: 19.142908096313477 for layer [False]\n",
      "Iteration: 6200, Loss: 15.530534744262695 for layer [False]\n",
      "Iteration: 6300, Loss: 12.630928993225098 for layer [False]\n",
      "Iteration: 6400, Loss: 15.226466178894043 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.14169692993164 for layer [ True]\n",
      "Iteration: 6600, Loss: 8.93994140625 for layer [ True]\n",
      "Iteration: 6700, Loss: 15.268054962158203 for layer [False]\n",
      "Iteration: 6800, Loss: 4.673292636871338 for layer [ True]\n",
      "Iteration: 6900, Loss: 31.459640502929688 for layer [False]\n",
      "Iteration: 7000, Loss: 8.637420654296875 for layer [False]\n",
      "Iteration: 7100, Loss: 10.781133651733398 for layer [False]\n",
      "Iteration: 7200, Loss: 12.911230087280273 for layer [False]\n",
      "Iteration: 7300, Loss: 4.4211039543151855 for layer [ True]\n",
      "Iteration: 7400, Loss: 10.978067398071289 for layer [False]\n",
      "Iteration: 7500, Loss: 4.43488883972168 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.14902925491333 for layer [ True]\n",
      "Iteration: 7700, Loss: 10.287797927856445 for layer [False]\n",
      "Iteration: 7800, Loss: 8.213489532470703 for layer [False]\n",
      "Iteration: 7900, Loss: 5.28371000289917 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.767289400100708 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.255023956298828 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.580207109451294 for layer [ True]\n",
      "Iteration: 8300, Loss: 10.475650787353516 for layer [False]\n",
      "Iteration: 8400, Loss: 1.8555829524993896 for layer [ True]\n",
      "Iteration: 8500, Loss: 8.178767204284668 for layer [False]\n",
      "Iteration: 8600, Loss: 5.0343403816223145 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6140005588531494 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.705254554748535 for layer [False]\n",
      "Iteration: 8900, Loss: 8.029032707214355 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4550374746322632 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5114985704421997 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.793703556060791 for layer [ True]\n",
      "Iteration: 9300, Loss: 5.051779747009277 for layer [False]\n",
      "Iteration: 9400, Loss: 5.290567398071289 for layer [False]\n",
      "Iteration: 9500, Loss: 1.489771842956543 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.489792823791504 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0353248119354248 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8541384935379028 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.616124629974365 for layer [False]\n",
      "Iteration: 10000, Loss: 2.7785937786102295 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1641929149627686 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7439542412757874 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.6491055488586426 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5287819504737854 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.53436279296875 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.952317237854004 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6297768354415894 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.4315876960754395 for layer [False]\n",
      "Iteration: 10900, Loss: 0.4910896122455597 for layer [ True]\n",
      "Iteration: 11000, Loss: 5.868143558502197 for layer [False]\n",
      "Iteration: 11100, Loss: 2.4176013469696045 for layer [False]\n",
      "Iteration: 11200, Loss: 0.33192896842956543 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4013231098651886 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.33104610443115234 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.2400221824645996 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3365744650363922 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.18407168984413147 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.24464920163154602 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.24589456617832184 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11071940511465073 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.17334124445915222 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1534188687801361 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.09778335690498352 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.12844927608966827 for layer [ True]\n",
      "Iteration: 12500, Loss: 4.202361583709717 for layer [False]\n",
      "Iteration: 12600, Loss: 4.6433186531066895 for layer [False]\n",
      "Iteration: 12700, Loss: 3.6675801277160645 for layer [False]\n",
      "Iteration: 12800, Loss: 0.08940871804952621 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.0837220624089241 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.061297859996557236 for layer [ True]\n",
      "Iteration: 13100, Loss: 3.7491254806518555 for layer [False]\n",
      "Iteration: 13200, Loss: 0.058522291481494904 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.7791285514831543 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04108627140522003 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05250997096300125 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.042337749153375626 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.014163740910589695 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.035365425050258636 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.028699275106191635 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.9477916955947876 for layer [False]\n",
      "Iteration: 14100, Loss: 0.025800563395023346 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.431674480438232 for layer [False]\n",
      "Iteration: 14300, Loss: 0.00960675347596407 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.6769466400146484 for layer [False]\n",
      "Iteration: 14500, Loss: 2.1819186210632324 for layer [False]\n",
      "Iteration: 14600, Loss: 2.406920909881592 for layer [False]\n",
      "Iteration: 14700, Loss: 2.5733015537261963 for layer [False]\n",
      "Iteration: 14800, Loss: 3.095510482788086 for layer [False]\n",
      "Iteration: 14900, Loss: 4.22780704498291 for layer [False]\n",
      "Iteration: 15000, Loss: 0.005980044603347778 for layer [ True]\n",
      "Iteration: 15100, Loss: 4.1150054931640625 for layer [False]\n",
      "Iteration: 15200, Loss: 6.902009010314941 for layer [False]\n",
      "Iteration: 15300, Loss: 3.559576988220215 for layer [False]\n",
      "Iteration: 15400, Loss: 1.6713281869888306 for layer [False]\n",
      "Iteration: 15500, Loss: 0.003153982339426875 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.003775102784857154 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004159788601100445 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.1226091384887695 for layer [False]\n",
      "Iteration: 15900, Loss: 0.001969315344467759 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0031545343808829784 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.6394970417022705 for layer [False]\n",
      "Iteration: 16200, Loss: 0.00113100151065737 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.002055258024483919 for layer [ True]\n",
      "Iteration: 16400, Loss: 2.683197259902954 for layer [False]\n",
      "Iteration: 16500, Loss: 3.8431975841522217 for layer [False]\n",
      "Iteration: 16600, Loss: 2.692136287689209 for layer [False]\n",
      "Iteration: 16700, Loss: 0.00156968436203897 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007800957537256181 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.001024453667923808 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008633514516986907 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.7228355407714844 for layer [False]\n",
      "Iteration: 17200, Loss: 2.4467272758483887 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0007040932541713119 for layer [ True]\n",
      "Iteration: 17400, Loss: 4.170029640197754 for layer [False]\n",
      "Iteration: 17500, Loss: 7.310883522033691 for layer [False]\n",
      "Iteration: 17600, Loss: 2.969437837600708 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0007673727814108133 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.9032223224639893 for layer [False]\n",
      "Iteration: 17900, Loss: 2.733564615249634 for layer [False]\n",
      "Iteration: 18000, Loss: 1.8274526596069336 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006437838892452419 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005291589768603444 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.766060829162598 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00040880381129682064 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007017875905148685 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.640782117843628 for layer [False]\n",
      "Iteration: 18700, Loss: 3.087514638900757 for layer [False]\n",
      "Iteration: 18800, Loss: 3.2554006576538086 for layer [False]\n",
      "Iteration: 18900, Loss: 1.3721702098846436 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00022748298943042755 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.358881711959839 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0012472720118239522 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.035928726196289 for layer [False]\n",
      "Iteration: 19400, Loss: 4.487945556640625 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006150392000563443 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003856681287288666 for layer [ True]\n",
      "Iteration: 19700, Loss: 3.433642864227295 for layer [False]\n",
      "Iteration: 19800, Loss: 3.0669209957122803 for layer [False]\n",
      "Iteration: 19900, Loss: 2.715513229370117 for layer [False]\n",
      "Iteration: 20000, Loss: 1.8773406744003296 for layer [False]\n",
      "Iteration: 20100, Loss: 4.176350116729736 for layer [False]\n",
      "Iteration: 20200, Loss: 0.002181577030569315 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.241166591644287 for layer [False]\n",
      "Iteration: 20400, Loss: 6.615186430281028e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00028832032694481313 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009726302814669907 for layer [ True]\n",
      "Iteration: 20700, Loss: 1.9521948099136353 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00021348896552808583 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0006714651826769114 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.4147520065307617 for layer [False]\n",
      "Iteration: 21100, Loss: 5.9930243492126465 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0002166576887248084 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.713332414627075 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00016685629088897258 for layer [ True]\n",
      "Iteration: 21500, Loss: 2.596247911453247 for layer [False]\n",
      "Iteration: 21600, Loss: 2.9480297565460205 for layer [False]\n",
      "Iteration: 21700, Loss: 9.759816020959988e-05 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0006656195037066936 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.6357431411743164 for layer [False]\n",
      "Iteration: 22000, Loss: 3.4982430934906006 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00033072655787691474 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00011395046021789312 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.881150484085083 for layer [False]\n",
      "Iteration: 22400, Loss: 3.702237367630005 for layer [False]\n",
      "Iteration: 22500, Loss: 1.3580116033554077 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00014248648949433118 for layer [ True]\n",
      "Iteration: 22700, Loss: 7.2229522629641e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00034392322413623333 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.00046131349517963827 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0009181622299365699 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00037472270196303725 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.000260118511505425 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0004241751739755273 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00010139762161998078 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00015555956633761525 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0006296688225120306 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.294496774673462 for layer [False]\n",
      "Iteration: 23800, Loss: 0.00073231291025877 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0001395385479554534 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.000132876040879637 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.0002100541751133278 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.141474962234497 for layer [False]\n",
      "Iteration: 24300, Loss: 2.063495635986328 for layer [False]\n",
      "Iteration: 24400, Loss: 3.1628575325012207 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00027085814508609474 for layer [ True]\n",
      "Iteration: 24600, Loss: 8.371369767701253e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 3.622377634048462 for layer [False]\n",
      "Iteration: 24800, Loss: 3.1083853244781494 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00010150684101972729 for layer [ True]\n",
      "Step 6500 | Loss: 0.000478\n",
      "Step 6600 | Loss: 0.000478\n",
      "Step 6700 | Loss: 0.000477\n",
      "Step 6800 | Loss: 0.000476\n",
      "Step 6900 | Loss: 0.000476\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1973.7171630859375 for layer [False]\n",
      "Iteration: 100, Loss: 313.7031555175781 for layer [ True]\n",
      "Iteration: 200, Loss: 1969.450927734375 for layer [False]\n",
      "Iteration: 300, Loss: 1591.2408447265625 for layer [False]\n",
      "Iteration: 400, Loss: 237.69142150878906 for layer [ True]\n",
      "Iteration: 500, Loss: 308.708984375 for layer [ True]\n",
      "Iteration: 600, Loss: 1473.2235107421875 for layer [False]\n",
      "Iteration: 700, Loss: 225.82310485839844 for layer [ True]\n",
      "Iteration: 800, Loss: 1197.6207275390625 for layer [False]\n",
      "Iteration: 900, Loss: 1038.6746826171875 for layer [False]\n",
      "Iteration: 1000, Loss: 144.5070037841797 for layer [ True]\n",
      "Iteration: 1100, Loss: 917.1627197265625 for layer [False]\n",
      "Iteration: 1200, Loss: 1086.2139892578125 for layer [False]\n",
      "Iteration: 1300, Loss: 1012.0639038085938 for layer [False]\n",
      "Iteration: 1400, Loss: 228.17962646484375 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.00750732421875 for layer [ True]\n",
      "Iteration: 1600, Loss: 130.51785278320312 for layer [ True]\n",
      "Iteration: 1700, Loss: 116.23616027832031 for layer [ True]\n",
      "Iteration: 1800, Loss: 630.8045043945312 for layer [False]\n",
      "Iteration: 1900, Loss: 114.01070404052734 for layer [ True]\n",
      "Iteration: 2000, Loss: 99.39017486572266 for layer [ True]\n",
      "Iteration: 2100, Loss: 156.66842651367188 for layer [ True]\n",
      "Iteration: 2200, Loss: 106.77713775634766 for layer [ True]\n",
      "Iteration: 2300, Loss: 467.3008117675781 for layer [False]\n",
      "Iteration: 2400, Loss: 122.91273498535156 for layer [ True]\n",
      "Iteration: 2500, Loss: 183.3756561279297 for layer [ True]\n",
      "Iteration: 2600, Loss: 118.13481903076172 for layer [ True]\n",
      "Iteration: 2700, Loss: 85.6413345336914 for layer [ True]\n",
      "Iteration: 2800, Loss: 334.8056945800781 for layer [False]\n",
      "Iteration: 2900, Loss: 265.3294372558594 for layer [False]\n",
      "Iteration: 3000, Loss: 79.14481353759766 for layer [ True]\n",
      "Iteration: 3100, Loss: 229.38136291503906 for layer [False]\n",
      "Iteration: 3200, Loss: 238.54803466796875 for layer [False]\n",
      "Iteration: 3300, Loss: 161.7352752685547 for layer [False]\n",
      "Iteration: 3400, Loss: 72.59720611572266 for layer [ True]\n",
      "Iteration: 3500, Loss: 195.6972198486328 for layer [False]\n",
      "Iteration: 3600, Loss: 57.68790817260742 for layer [ True]\n",
      "Iteration: 3700, Loss: 143.7714080810547 for layer [False]\n",
      "Iteration: 3800, Loss: 120.63410186767578 for layer [False]\n",
      "Iteration: 3900, Loss: 41.62193298339844 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.09649658203125 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.26264572143555 for layer [ True]\n",
      "Iteration: 4200, Loss: 76.78423309326172 for layer [ True]\n",
      "Iteration: 4300, Loss: 45.57197189331055 for layer [ True]\n",
      "Iteration: 4400, Loss: 40.36244201660156 for layer [ True]\n",
      "Iteration: 4500, Loss: 52.70246505737305 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.33053207397461 for layer [ True]\n",
      "Iteration: 4700, Loss: 66.32592010498047 for layer [False]\n",
      "Iteration: 4800, Loss: 41.41273498535156 for layer [False]\n",
      "Iteration: 4900, Loss: 54.03826141357422 for layer [False]\n",
      "Iteration: 5000, Loss: 53.72649383544922 for layer [ True]\n",
      "Iteration: 5100, Loss: 22.215972900390625 for layer [ True]\n",
      "Iteration: 5200, Loss: 37.56768798828125 for layer [False]\n",
      "Iteration: 5300, Loss: 21.730636596679688 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.860076904296875 for layer [False]\n",
      "Iteration: 5500, Loss: 23.487674713134766 for layer [ True]\n",
      "Iteration: 5600, Loss: 27.056884765625 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.748371124267578 for layer [False]\n",
      "Iteration: 5800, Loss: 22.12555694580078 for layer [False]\n",
      "Iteration: 5900, Loss: 13.999220848083496 for layer [False]\n",
      "Iteration: 6000, Loss: 13.393733978271484 for layer [ True]\n",
      "Iteration: 6100, Loss: 19.378854751586914 for layer [False]\n",
      "Iteration: 6200, Loss: 15.740751266479492 for layer [False]\n",
      "Iteration: 6300, Loss: 12.746838569641113 for layer [False]\n",
      "Iteration: 6400, Loss: 15.213184356689453 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.153767585754395 for layer [ True]\n",
      "Iteration: 6600, Loss: 8.969795227050781 for layer [ True]\n",
      "Iteration: 6700, Loss: 15.69214916229248 for layer [False]\n",
      "Iteration: 6800, Loss: 4.67674446105957 for layer [ True]\n",
      "Iteration: 6900, Loss: 32.61967849731445 for layer [False]\n",
      "Iteration: 7000, Loss: 8.802730560302734 for layer [False]\n",
      "Iteration: 7100, Loss: 10.874735832214355 for layer [False]\n",
      "Iteration: 7200, Loss: 13.340105056762695 for layer [False]\n",
      "Iteration: 7300, Loss: 4.412784099578857 for layer [ True]\n",
      "Iteration: 7400, Loss: 11.396193504333496 for layer [False]\n",
      "Iteration: 7500, Loss: 4.417115211486816 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.145026206970215 for layer [ True]\n",
      "Iteration: 7700, Loss: 10.450506210327148 for layer [False]\n",
      "Iteration: 7800, Loss: 8.327431678771973 for layer [False]\n",
      "Iteration: 7900, Loss: 5.281117916107178 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.7731306552886963 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2539737224578857 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.5930774211883545 for layer [ True]\n",
      "Iteration: 8300, Loss: 10.719351768493652 for layer [False]\n",
      "Iteration: 8400, Loss: 1.8580700159072876 for layer [ True]\n",
      "Iteration: 8500, Loss: 8.533645629882812 for layer [False]\n",
      "Iteration: 8600, Loss: 5.23060941696167 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6114144325256348 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.774381637573242 for layer [False]\n",
      "Iteration: 8900, Loss: 8.156484603881836 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4535737037658691 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.513764500617981 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.7931803464889526 for layer [ True]\n",
      "Iteration: 9300, Loss: 5.141566276550293 for layer [False]\n",
      "Iteration: 9400, Loss: 5.50035285949707 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4904539585113525 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.516878604888916 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0350474119186401 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8569148778915405 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.623631954193115 for layer [False]\n",
      "Iteration: 10000, Loss: 2.856785535812378 for layer [False]\n",
      "Iteration: 10100, Loss: 1.16595458984375 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7426302433013916 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.780881881713867 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5297869443893433 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5357863306999207 for layer [ True]\n",
      "Iteration: 10600, Loss: 2.978740692138672 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6385427117347717 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.4765419960021973 for layer [False]\n",
      "Iteration: 10900, Loss: 0.49310001730918884 for layer [ True]\n",
      "Iteration: 11000, Loss: 6.12294864654541 for layer [False]\n",
      "Iteration: 11100, Loss: 2.460998058319092 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3331286907196045 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4048077166080475 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3307856023311615 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.306337833404541 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3376156985759735 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.18432684242725372 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.24532972276210785 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.24633316695690155 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11031801253557205 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.1744236946105957 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.15432673692703247 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.09814701974391937 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.12928561866283417 for layer [ True]\n",
      "Iteration: 12500, Loss: 4.310051441192627 for layer [False]\n",
      "Iteration: 12600, Loss: 4.765929698944092 for layer [False]\n",
      "Iteration: 12700, Loss: 3.7375123500823975 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09023330360651016 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.08458986133337021 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06120147928595543 for layer [ True]\n",
      "Iteration: 13100, Loss: 3.8751299381256104 for layer [False]\n",
      "Iteration: 13200, Loss: 0.05870131775736809 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.820056915283203 for layer [False]\n",
      "Iteration: 13400, Loss: 0.041400473564863205 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05298154801130295 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04282156750559807 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.014234291389584541 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.035711634904146194 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.028869060799479485 for layer [ True]\n",
      "Iteration: 14000, Loss: 1.9879398345947266 for layer [False]\n",
      "Iteration: 14100, Loss: 0.02605636976659298 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.48892068862915 for layer [False]\n",
      "Iteration: 14300, Loss: 0.00964298751205206 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.720139741897583 for layer [False]\n",
      "Iteration: 14500, Loss: 2.160768747329712 for layer [False]\n",
      "Iteration: 14600, Loss: 2.495056390762329 for layer [False]\n",
      "Iteration: 14700, Loss: 2.604257106781006 for layer [False]\n",
      "Iteration: 14800, Loss: 3.146888256072998 for layer [False]\n",
      "Iteration: 14900, Loss: 4.487515926361084 for layer [False]\n",
      "Iteration: 15000, Loss: 0.00603646133095026 for layer [ True]\n",
      "Iteration: 15100, Loss: 4.296239376068115 for layer [False]\n",
      "Iteration: 15200, Loss: 7.073029041290283 for layer [False]\n",
      "Iteration: 15300, Loss: 3.656594753265381 for layer [False]\n",
      "Iteration: 15400, Loss: 1.7264540195465088 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0032066917046904564 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.003824424697086215 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004134056158363819 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.184338092803955 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0019876838196069 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0031953840516507626 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.678833484649658 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011373129673302174 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020718537271022797 for layer [ True]\n",
      "Iteration: 16400, Loss: 2.835707902908325 for layer [False]\n",
      "Iteration: 16500, Loss: 3.8867974281311035 for layer [False]\n",
      "Iteration: 16600, Loss: 2.71850848197937 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015748124569654465 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007700543501414359 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010200380347669125 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008756071329116821 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.7643117904663086 for layer [False]\n",
      "Iteration: 17200, Loss: 2.467402458190918 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0007043542573228478 for layer [ True]\n",
      "Iteration: 17400, Loss: 4.389407157897949 for layer [False]\n",
      "Iteration: 17500, Loss: 7.574849605560303 for layer [False]\n",
      "Iteration: 17600, Loss: 3.0556838512420654 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0007818141602911055 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.9523773193359375 for layer [False]\n",
      "Iteration: 17900, Loss: 2.845914125442505 for layer [False]\n",
      "Iteration: 18000, Loss: 1.8744839429855347 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006648845737800002 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005339302588254213 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.800224304199219 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0004112754249945283 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007137189386412501 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.6501835584640503 for layer [False]\n",
      "Iteration: 18700, Loss: 3.160759210586548 for layer [False]\n",
      "Iteration: 18800, Loss: 3.3451836109161377 for layer [False]\n",
      "Iteration: 18900, Loss: 1.419932246208191 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00022966723190620542 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.45748233795166 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0013005129294469953 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.0818371772766113 for layer [False]\n",
      "Iteration: 19400, Loss: 4.631519794464111 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006239241338334978 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003869611828122288 for layer [ True]\n",
      "Iteration: 19700, Loss: 3.5889229774475098 for layer [False]\n",
      "Iteration: 19800, Loss: 3.1128904819488525 for layer [False]\n",
      "Iteration: 19900, Loss: 2.8378710746765137 for layer [False]\n",
      "Iteration: 20000, Loss: 1.9479804039001465 for layer [False]\n",
      "Iteration: 20100, Loss: 4.355618000030518 for layer [False]\n",
      "Iteration: 20200, Loss: 0.002263229340314865 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.2922770977020264 for layer [False]\n",
      "Iteration: 20400, Loss: 6.782959826523438e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00029359315522015095 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009823398431763053 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.069082260131836 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00023259478621184826 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.000638157653156668 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.4727489948272705 for layer [False]\n",
      "Iteration: 21100, Loss: 6.25381326675415 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00021462771110236645 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.8015847206115723 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0001599854149390012 for layer [ True]\n",
      "Iteration: 21500, Loss: 2.6818149089813232 for layer [False]\n",
      "Iteration: 21600, Loss: 3.068152666091919 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0001817172160372138 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00037373328814283013 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.6937925815582275 for layer [False]\n",
      "Iteration: 22000, Loss: 3.5599160194396973 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0001412320270901546 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0001302834862144664 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.938427448272705 for layer [False]\n",
      "Iteration: 22400, Loss: 3.8440840244293213 for layer [False]\n",
      "Iteration: 22500, Loss: 1.3870415687561035 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00041737756691873074 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.0001334551052423194 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00014563977310899645 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0008653437835164368 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.000815100793261081 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0006612709839828312 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00027367816073819995 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0002585685870144516 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002028677990892902 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0003422051086090505 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00017428872524760664 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.355767250061035 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007494561141356826 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0002154500543838367 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0002992801018990576 for layer [ True]\n",
      "Iteration: 24100, Loss: 6.375707744155079e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.226300001144409 for layer [False]\n",
      "Iteration: 24300, Loss: 2.0776612758636475 for layer [False]\n",
      "Iteration: 24400, Loss: 3.289360284805298 for layer [False]\n",
      "Iteration: 24500, Loss: 5.364025491871871e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 6.715470226481557e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 3.797090768814087 for layer [False]\n",
      "Iteration: 24800, Loss: 3.2201952934265137 for layer [False]\n",
      "Iteration: 24900, Loss: 7.502823427785188e-05 for layer [ True]\n",
      "Step 7000 | Loss: 0.000475\n",
      "Step 7100 | Loss: 0.000475\n",
      "Step 7200 | Loss: 0.000474\n",
      "Step 7300 | Loss: 0.000474\n",
      "Step 7400 | Loss: 0.000473\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1970.587646484375 for layer [False]\n",
      "Iteration: 100, Loss: 314.33599853515625 for layer [ True]\n",
      "Iteration: 200, Loss: 1964.616943359375 for layer [False]\n",
      "Iteration: 300, Loss: 1585.28564453125 for layer [False]\n",
      "Iteration: 400, Loss: 237.35325622558594 for layer [ True]\n",
      "Iteration: 500, Loss: 308.2481994628906 for layer [ True]\n",
      "Iteration: 600, Loss: 1469.4293212890625 for layer [False]\n",
      "Iteration: 700, Loss: 225.4628143310547 for layer [ True]\n",
      "Iteration: 800, Loss: 1194.32666015625 for layer [False]\n",
      "Iteration: 900, Loss: 1036.5184326171875 for layer [False]\n",
      "Iteration: 1000, Loss: 144.6728057861328 for layer [ True]\n",
      "Iteration: 1100, Loss: 914.1832885742188 for layer [False]\n",
      "Iteration: 1200, Loss: 1082.6163330078125 for layer [False]\n",
      "Iteration: 1300, Loss: 1009.9366455078125 for layer [False]\n",
      "Iteration: 1400, Loss: 228.09165954589844 for layer [ True]\n",
      "Iteration: 1500, Loss: 302.0198669433594 for layer [ True]\n",
      "Iteration: 1600, Loss: 130.7216339111328 for layer [ True]\n",
      "Iteration: 1700, Loss: 116.59013366699219 for layer [ True]\n",
      "Iteration: 1800, Loss: 628.9713745117188 for layer [False]\n",
      "Iteration: 1900, Loss: 113.75123596191406 for layer [ True]\n",
      "Iteration: 2000, Loss: 99.4676742553711 for layer [ True]\n",
      "Iteration: 2100, Loss: 156.32159423828125 for layer [ True]\n",
      "Iteration: 2200, Loss: 106.37118530273438 for layer [ True]\n",
      "Iteration: 2300, Loss: 466.1231384277344 for layer [False]\n",
      "Iteration: 2400, Loss: 122.69638061523438 for layer [ True]\n",
      "Iteration: 2500, Loss: 183.25057983398438 for layer [ True]\n",
      "Iteration: 2600, Loss: 117.79559326171875 for layer [ True]\n",
      "Iteration: 2700, Loss: 85.87622833251953 for layer [ True]\n",
      "Iteration: 2800, Loss: 333.8608093261719 for layer [False]\n",
      "Iteration: 2900, Loss: 264.14764404296875 for layer [False]\n",
      "Iteration: 3000, Loss: 79.01811981201172 for layer [ True]\n",
      "Iteration: 3100, Loss: 228.64260864257812 for layer [False]\n",
      "Iteration: 3200, Loss: 237.64588928222656 for layer [False]\n",
      "Iteration: 3300, Loss: 161.0778045654297 for layer [False]\n",
      "Iteration: 3400, Loss: 72.53186798095703 for layer [ True]\n",
      "Iteration: 3500, Loss: 194.7115936279297 for layer [False]\n",
      "Iteration: 3600, Loss: 57.74265670776367 for layer [ True]\n",
      "Iteration: 3700, Loss: 142.770263671875 for layer [False]\n",
      "Iteration: 3800, Loss: 120.0712661743164 for layer [False]\n",
      "Iteration: 3900, Loss: 41.610233306884766 for layer [ True]\n",
      "Iteration: 4000, Loss: 49.82923889160156 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.10133361816406 for layer [ True]\n",
      "Iteration: 4200, Loss: 76.53569793701172 for layer [ True]\n",
      "Iteration: 4300, Loss: 45.37202835083008 for layer [ True]\n",
      "Iteration: 4400, Loss: 40.37152099609375 for layer [ True]\n",
      "Iteration: 4500, Loss: 52.64366912841797 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.15847396850586 for layer [ True]\n",
      "Iteration: 4700, Loss: 66.01602172851562 for layer [False]\n",
      "Iteration: 4800, Loss: 41.21963882446289 for layer [False]\n",
      "Iteration: 4900, Loss: 53.69704818725586 for layer [False]\n",
      "Iteration: 5000, Loss: 53.60077667236328 for layer [ True]\n",
      "Iteration: 5100, Loss: 22.25372314453125 for layer [ True]\n",
      "Iteration: 5200, Loss: 37.36294937133789 for layer [False]\n",
      "Iteration: 5300, Loss: 21.699182510375977 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.721092224121094 for layer [False]\n",
      "Iteration: 5500, Loss: 23.487220764160156 for layer [ True]\n",
      "Iteration: 5600, Loss: 27.134124755859375 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.777572631835938 for layer [False]\n",
      "Iteration: 5800, Loss: 22.106863021850586 for layer [False]\n",
      "Iteration: 5900, Loss: 14.007835388183594 for layer [False]\n",
      "Iteration: 6000, Loss: 13.32946491241455 for layer [ True]\n",
      "Iteration: 6100, Loss: 19.503677368164062 for layer [False]\n",
      "Iteration: 6200, Loss: 15.747655868530273 for layer [False]\n",
      "Iteration: 6300, Loss: 12.924335479736328 for layer [False]\n",
      "Iteration: 6400, Loss: 15.206047058105469 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.160553932189941 for layer [ True]\n",
      "Iteration: 6600, Loss: 8.98891544342041 for layer [ True]\n",
      "Iteration: 6700, Loss: 15.841569900512695 for layer [False]\n",
      "Iteration: 6800, Loss: 4.659258842468262 for layer [ True]\n",
      "Iteration: 6900, Loss: 33.55009841918945 for layer [False]\n",
      "Iteration: 7000, Loss: 8.961565017700195 for layer [False]\n",
      "Iteration: 7100, Loss: 10.974990844726562 for layer [False]\n",
      "Iteration: 7200, Loss: 13.608695983886719 for layer [False]\n",
      "Iteration: 7300, Loss: 4.419109344482422 for layer [ True]\n",
      "Iteration: 7400, Loss: 11.691725730895996 for layer [False]\n",
      "Iteration: 7500, Loss: 4.387325763702393 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.105639457702637 for layer [ True]\n",
      "Iteration: 7700, Loss: 10.631044387817383 for layer [False]\n",
      "Iteration: 7800, Loss: 8.352408409118652 for layer [False]\n",
      "Iteration: 7900, Loss: 5.268530368804932 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.7620208263397217 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.230762243270874 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.5997884273529053 for layer [ True]\n",
      "Iteration: 8300, Loss: 10.964605331420898 for layer [False]\n",
      "Iteration: 8400, Loss: 1.8559648990631104 for layer [ True]\n",
      "Iteration: 8500, Loss: 8.787219047546387 for layer [False]\n",
      "Iteration: 8600, Loss: 5.396031856536865 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6032402515411377 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.827971935272217 for layer [False]\n",
      "Iteration: 8900, Loss: 8.230619430541992 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4464728832244873 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5118329524993896 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.7910860776901245 for layer [ True]\n",
      "Iteration: 9300, Loss: 5.229495048522949 for layer [False]\n",
      "Iteration: 9400, Loss: 5.592102527618408 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4889222383499146 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.5774405002593994 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0271093845367432 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8540878891944885 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.792755603790283 for layer [False]\n",
      "Iteration: 10000, Loss: 2.9645891189575195 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1645371913909912 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7401756644248962 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.8520524501800537 for layer [False]\n",
      "Iteration: 10400, Loss: 0.52525794506073 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5357106328010559 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.0082476139068604 for layer [False]\n",
      "Iteration: 10700, Loss: 0.643316388130188 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.5538594722747803 for layer [False]\n",
      "Iteration: 10900, Loss: 0.4941888749599457 for layer [ True]\n",
      "Iteration: 11000, Loss: 6.311425685882568 for layer [False]\n",
      "Iteration: 11100, Loss: 2.5147533416748047 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3329184949398041 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.40487465262413025 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.32984983921051025 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.3606626987457275 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3386325240135193 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.18286050856113434 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.24494262039661407 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.24556414783000946 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.10981568694114685 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.17436838150024414 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.15440548956394196 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.09785572439432144 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.12997327744960785 for layer [ True]\n",
      "Iteration: 12500, Loss: 4.447413921356201 for layer [False]\n",
      "Iteration: 12600, Loss: 4.853874206542969 for layer [False]\n",
      "Iteration: 12700, Loss: 3.8209831714630127 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09057067334651947 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.08442563563585281 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06088326871395111 for layer [ True]\n",
      "Iteration: 13100, Loss: 3.9701082706451416 for layer [False]\n",
      "Iteration: 13200, Loss: 0.05871180444955826 for layer [ True]\n",
      "Iteration: 13300, Loss: 3.89029860496521 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04143764078617096 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05324037745594978 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.042846839874982834 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.014226396568119526 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.0355098731815815 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.028704771772027016 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.0436928272247314 for layer [False]\n",
      "Iteration: 14100, Loss: 0.026049157604575157 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.528844356536865 for layer [False]\n",
      "Iteration: 14300, Loss: 0.009662678465247154 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.736147403717041 for layer [False]\n",
      "Iteration: 14500, Loss: 2.164029836654663 for layer [False]\n",
      "Iteration: 14600, Loss: 2.541740894317627 for layer [False]\n",
      "Iteration: 14700, Loss: 2.6332881450653076 for layer [False]\n",
      "Iteration: 14800, Loss: 3.1871602535247803 for layer [False]\n",
      "Iteration: 14900, Loss: 4.641129016876221 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006015201564878225 for layer [ True]\n",
      "Iteration: 15100, Loss: 4.415099143981934 for layer [False]\n",
      "Iteration: 15200, Loss: 7.1993255615234375 for layer [False]\n",
      "Iteration: 15300, Loss: 3.7342782020568848 for layer [False]\n",
      "Iteration: 15400, Loss: 1.7595971822738647 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0031999191269278526 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.003830605885013938 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004103428218513727 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.220776319503784 for layer [False]\n",
      "Iteration: 15900, Loss: 0.001986175077036023 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0032093559857457876 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.7313296794891357 for layer [False]\n",
      "Iteration: 16200, Loss: 0.001131863216869533 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.002065602457150817 for layer [ True]\n",
      "Iteration: 16400, Loss: 2.92999529838562 for layer [False]\n",
      "Iteration: 16500, Loss: 3.9512641429901123 for layer [False]\n",
      "Iteration: 16600, Loss: 2.746311664581299 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015713893808424473 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007604194106534123 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010038820328190923 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008802063530310988 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.7772818803787231 for layer [False]\n",
      "Iteration: 17200, Loss: 2.504253625869751 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006999453762546182 for layer [ True]\n",
      "Iteration: 17400, Loss: 4.503871440887451 for layer [False]\n",
      "Iteration: 17500, Loss: 7.814095497131348 for layer [False]\n",
      "Iteration: 17600, Loss: 3.153637409210205 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0007740833680145442 for layer [ True]\n",
      "Iteration: 17800, Loss: 2.9951841831207275 for layer [False]\n",
      "Iteration: 17900, Loss: 2.92012619972229 for layer [False]\n",
      "Iteration: 18000, Loss: 1.9150333404541016 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006826656172052026 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005368342390283942 for layer [ True]\n",
      "Iteration: 18300, Loss: 4.895819664001465 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0004067233530804515 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007276097312569618 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.690987229347229 for layer [False]\n",
      "Iteration: 18700, Loss: 3.2589852809906006 for layer [False]\n",
      "Iteration: 18800, Loss: 3.420388698577881 for layer [False]\n",
      "Iteration: 18900, Loss: 1.4314874410629272 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00023133549257181585 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.5250258445739746 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0013160724192857742 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.11086106300354 for layer [False]\n",
      "Iteration: 19400, Loss: 4.76955509185791 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006357560632750392 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003849682107102126 for layer [ True]\n",
      "Iteration: 19700, Loss: 3.731461763381958 for layer [False]\n",
      "Iteration: 19800, Loss: 3.150796413421631 for layer [False]\n",
      "Iteration: 19900, Loss: 2.948476791381836 for layer [False]\n",
      "Iteration: 20000, Loss: 1.9770878553390503 for layer [False]\n",
      "Iteration: 20100, Loss: 4.45733642578125 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0022746308241039515 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.343400716781616 for layer [False]\n",
      "Iteration: 20400, Loss: 6.762018892914057e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00029937902581878006 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009715688065625727 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.1345479488372803 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00024007643514778465 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0006448036874644458 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.5264276266098022 for layer [False]\n",
      "Iteration: 21100, Loss: 6.400606155395508 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0002927298191934824 for layer [ True]\n",
      "Iteration: 21300, Loss: 3.8485970497131348 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0004587302973959595 for layer [ True]\n",
      "Iteration: 21500, Loss: 2.7628085613250732 for layer [False]\n",
      "Iteration: 21600, Loss: 3.173245429992676 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003430994402151555 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0002731779823079705 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.72104549407959 for layer [False]\n",
      "Iteration: 22000, Loss: 3.5757577419281006 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0003344188444316387 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0002056203520623967 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.903836488723755 for layer [False]\n",
      "Iteration: 22400, Loss: 3.9267101287841797 for layer [False]\n",
      "Iteration: 22500, Loss: 1.4137147665023804 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00023794690787326545 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00015920154692139477 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0001446553651476279 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0016608057776466012 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0003922517644241452 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0008461709367111325 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00018733616161625832 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.000245721050305292 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00019014814461115748 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0005951489438302815 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00020350608974695206 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.402268409729004 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006227587582543492 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00019408103253226727 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0007543333922512829 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.00010552998719504103 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.2946627140045166 for layer [False]\n",
      "Iteration: 24300, Loss: 2.0840423107147217 for layer [False]\n",
      "Iteration: 24400, Loss: 3.2515676021575928 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00015596605953760445 for layer [ True]\n",
      "Iteration: 24600, Loss: 9.419926209375262e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 3.908633232116699 for layer [False]\n",
      "Iteration: 24800, Loss: 3.260073661804199 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00010159691737499088 for layer [ True]\n",
      "Step 7500 | Loss: 0.000473\n",
      "Step 7600 | Loss: 0.000473\n",
      "Step 7700 | Loss: 0.000472\n",
      "Step 7800 | Loss: 0.000471\n",
      "Step 7900 | Loss: 0.000469\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1965.0716552734375 for layer [False]\n",
      "Iteration: 100, Loss: 314.8636474609375 for layer [ True]\n",
      "Iteration: 200, Loss: 1959.636474609375 for layer [False]\n",
      "Iteration: 300, Loss: 1577.9307861328125 for layer [False]\n",
      "Iteration: 400, Loss: 238.31822204589844 for layer [ True]\n",
      "Iteration: 500, Loss: 309.1764221191406 for layer [ True]\n",
      "Iteration: 600, Loss: 1465.4571533203125 for layer [False]\n",
      "Iteration: 700, Loss: 225.98141479492188 for layer [ True]\n",
      "Iteration: 800, Loss: 1187.8426513671875 for layer [False]\n",
      "Iteration: 900, Loss: 1033.2054443359375 for layer [False]\n",
      "Iteration: 1000, Loss: 145.13719177246094 for layer [ True]\n",
      "Iteration: 1100, Loss: 910.1360473632812 for layer [False]\n",
      "Iteration: 1200, Loss: 1079.38037109375 for layer [False]\n",
      "Iteration: 1300, Loss: 1006.4794921875 for layer [False]\n",
      "Iteration: 1400, Loss: 229.6310272216797 for layer [ True]\n",
      "Iteration: 1500, Loss: 302.6483154296875 for layer [ True]\n",
      "Iteration: 1600, Loss: 131.51231384277344 for layer [ True]\n",
      "Iteration: 1700, Loss: 116.87399291992188 for layer [ True]\n",
      "Iteration: 1800, Loss: 626.6510620117188 for layer [False]\n",
      "Iteration: 1900, Loss: 114.00872039794922 for layer [ True]\n",
      "Iteration: 2000, Loss: 99.73214721679688 for layer [ True]\n",
      "Iteration: 2100, Loss: 156.29220581054688 for layer [ True]\n",
      "Iteration: 2200, Loss: 106.5846939086914 for layer [ True]\n",
      "Iteration: 2300, Loss: 463.4954528808594 for layer [False]\n",
      "Iteration: 2400, Loss: 122.94856262207031 for layer [ True]\n",
      "Iteration: 2500, Loss: 184.1034698486328 for layer [ True]\n",
      "Iteration: 2600, Loss: 118.17791748046875 for layer [ True]\n",
      "Iteration: 2700, Loss: 86.43830108642578 for layer [ True]\n",
      "Iteration: 2800, Loss: 332.5635986328125 for layer [False]\n",
      "Iteration: 2900, Loss: 262.1346740722656 for layer [False]\n",
      "Iteration: 3000, Loss: 79.06382751464844 for layer [ True]\n",
      "Iteration: 3100, Loss: 226.84710693359375 for layer [False]\n",
      "Iteration: 3200, Loss: 235.89971923828125 for layer [False]\n",
      "Iteration: 3300, Loss: 160.03721618652344 for layer [False]\n",
      "Iteration: 3400, Loss: 72.83138275146484 for layer [ True]\n",
      "Iteration: 3500, Loss: 193.60975646972656 for layer [False]\n",
      "Iteration: 3600, Loss: 57.95405960083008 for layer [ True]\n",
      "Iteration: 3700, Loss: 140.92996215820312 for layer [False]\n",
      "Iteration: 3800, Loss: 119.11678314208984 for layer [False]\n",
      "Iteration: 3900, Loss: 41.86247634887695 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.01519775390625 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.51264572143555 for layer [ True]\n",
      "Iteration: 4200, Loss: 77.07296752929688 for layer [ True]\n",
      "Iteration: 4300, Loss: 45.46224594116211 for layer [ True]\n",
      "Iteration: 4400, Loss: 40.69084548950195 for layer [ True]\n",
      "Iteration: 4500, Loss: 53.049949645996094 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.22960662841797 for layer [ True]\n",
      "Iteration: 4700, Loss: 65.32337951660156 for layer [False]\n",
      "Iteration: 4800, Loss: 41.06425857543945 for layer [False]\n",
      "Iteration: 4900, Loss: 53.18305587768555 for layer [False]\n",
      "Iteration: 5000, Loss: 54.03847885131836 for layer [ True]\n",
      "Iteration: 5100, Loss: 22.459352493286133 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.940792083740234 for layer [False]\n",
      "Iteration: 5300, Loss: 21.86958122253418 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.604875564575195 for layer [False]\n",
      "Iteration: 5500, Loss: 23.70393180847168 for layer [ True]\n",
      "Iteration: 5600, Loss: 27.29895782470703 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.85837173461914 for layer [False]\n",
      "Iteration: 5800, Loss: 22.04143524169922 for layer [False]\n",
      "Iteration: 5900, Loss: 13.901881217956543 for layer [False]\n",
      "Iteration: 6000, Loss: 13.427372932434082 for layer [ True]\n",
      "Iteration: 6100, Loss: 19.8656005859375 for layer [False]\n",
      "Iteration: 6200, Loss: 16.079572677612305 for layer [False]\n",
      "Iteration: 6300, Loss: 13.004591941833496 for layer [False]\n",
      "Iteration: 6400, Loss: 15.340951919555664 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.220625877380371 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.069908142089844 for layer [ True]\n",
      "Iteration: 6700, Loss: 16.5222110748291 for layer [False]\n",
      "Iteration: 6800, Loss: 4.692168235778809 for layer [ True]\n",
      "Iteration: 6900, Loss: 34.98505401611328 for layer [False]\n",
      "Iteration: 7000, Loss: 9.309983253479004 for layer [False]\n",
      "Iteration: 7100, Loss: 11.049439430236816 for layer [False]\n",
      "Iteration: 7200, Loss: 14.175148963928223 for layer [False]\n",
      "Iteration: 7300, Loss: 4.460984230041504 for layer [ True]\n",
      "Iteration: 7400, Loss: 12.356904983520508 for layer [False]\n",
      "Iteration: 7500, Loss: 4.388343334197998 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.127020835876465 for layer [ True]\n",
      "Iteration: 7700, Loss: 10.782950401306152 for layer [False]\n",
      "Iteration: 7800, Loss: 8.535052299499512 for layer [False]\n",
      "Iteration: 7900, Loss: 5.304596424102783 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.802384853363037 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2387399673461914 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.6356537342071533 for layer [ True]\n",
      "Iteration: 8300, Loss: 11.437844276428223 for layer [False]\n",
      "Iteration: 8400, Loss: 1.8776662349700928 for layer [ True]\n",
      "Iteration: 8500, Loss: 9.290321350097656 for layer [False]\n",
      "Iteration: 8600, Loss: 5.756173610687256 for layer [False]\n",
      "Iteration: 8700, Loss: 2.614109754562378 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.9019622802734375 for layer [False]\n",
      "Iteration: 8900, Loss: 8.462424278259277 for layer [False]\n",
      "Iteration: 9000, Loss: 1.450213074684143 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.519753336906433 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.7967747449874878 for layer [ True]\n",
      "Iteration: 9300, Loss: 5.4463348388671875 for layer [False]\n",
      "Iteration: 9400, Loss: 5.9301629066467285 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4827985763549805 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.640336751937866 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0321276187896729 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8521409630775452 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.826101303100586 for layer [False]\n",
      "Iteration: 10000, Loss: 3.0560007095336914 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1757973432540894 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7416148781776428 for layer [ True]\n",
      "Iteration: 10300, Loss: 3.9614994525909424 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5317611694335938 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5373908281326294 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.052696466445923 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6507673263549805 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.6552581787109375 for layer [False]\n",
      "Iteration: 10900, Loss: 0.49674075841903687 for layer [ True]\n",
      "Iteration: 11000, Loss: 6.686654090881348 for layer [False]\n",
      "Iteration: 11100, Loss: 2.6018810272216797 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3353271782398224 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4085717797279358 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.33203452825546265 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.4236059188842773 for layer [False]\n",
      "Iteration: 11600, Loss: 0.34078699350357056 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.18581590056419373 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.24687732756137848 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.248158797621727 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11000242829322815 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.17517970502376556 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.15606679022312164 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.09845989942550659 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.13083316385746002 for layer [ True]\n",
      "Iteration: 12500, Loss: 4.632189750671387 for layer [False]\n",
      "Iteration: 12600, Loss: 5.045626640319824 for layer [False]\n",
      "Iteration: 12700, Loss: 3.935866117477417 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09123537689447403 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.08521698415279388 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.0608704648911953 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.125585079193115 for layer [False]\n",
      "Iteration: 13200, Loss: 0.059517405927181244 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.005193710327148 for layer [False]\n",
      "Iteration: 13400, Loss: 0.0420650839805603 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.053556326776742935 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04340866953134537 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.014375044964253902 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.035775478929281235 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.02923489920794964 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.0818893909454346 for layer [False]\n",
      "Iteration: 14100, Loss: 0.026408206671476364 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.640497207641602 for layer [False]\n",
      "Iteration: 14300, Loss: 0.009640965610742569 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.8365554809570312 for layer [False]\n",
      "Iteration: 14500, Loss: 2.1658775806427 for layer [False]\n",
      "Iteration: 14600, Loss: 2.6727163791656494 for layer [False]\n",
      "Iteration: 14700, Loss: 2.7354705333709717 for layer [False]\n",
      "Iteration: 14800, Loss: 3.3317689895629883 for layer [False]\n",
      "Iteration: 14900, Loss: 5.084695816040039 for layer [False]\n",
      "Iteration: 15000, Loss: 0.00600156094878912 for layer [ True]\n",
      "Iteration: 15100, Loss: 4.649069309234619 for layer [False]\n",
      "Iteration: 15200, Loss: 7.560575008392334 for layer [False]\n",
      "Iteration: 15300, Loss: 3.8758156299591064 for layer [False]\n",
      "Iteration: 15400, Loss: 1.8272361755371094 for layer [False]\n",
      "Iteration: 15500, Loss: 0.003282560035586357 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0038495364133268595 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004133620299398899 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.315758466720581 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002013580407947302 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003254318842664361 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.7938170433044434 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011288884561508894 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020697724539786577 for layer [ True]\n",
      "Iteration: 16400, Loss: 3.1456472873687744 for layer [False]\n",
      "Iteration: 16500, Loss: 4.016314506530762 for layer [False]\n",
      "Iteration: 16600, Loss: 2.8130452632904053 for layer [False]\n",
      "Iteration: 16700, Loss: 0.001572107314132154 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007360150339081883 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.00100696028675884 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008985985768958926 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.8941682577133179 for layer [False]\n",
      "Iteration: 17200, Loss: 2.5479516983032227 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0007155377534218132 for layer [ True]\n",
      "Iteration: 17400, Loss: 4.802533149719238 for layer [False]\n",
      "Iteration: 17500, Loss: 8.281342506408691 for layer [False]\n",
      "Iteration: 17600, Loss: 3.2372124195098877 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0007840674952603877 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.0474236011505127 for layer [False]\n",
      "Iteration: 17900, Loss: 3.10963773727417 for layer [False]\n",
      "Iteration: 18000, Loss: 1.9862127304077148 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006964398198761046 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005326592945493758 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.018385887145996 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0004104546969756484 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007576028583571315 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.7556451559066772 for layer [False]\n",
      "Iteration: 18700, Loss: 3.3640270233154297 for layer [False]\n",
      "Iteration: 18800, Loss: 3.564629554748535 for layer [False]\n",
      "Iteration: 18900, Loss: 1.4921866655349731 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00020573149959091097 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.628324031829834 for layer [False]\n",
      "Iteration: 19200, Loss: 0.001354582724161446 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.1750662326812744 for layer [False]\n",
      "Iteration: 19400, Loss: 4.985413074493408 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006357879610732198 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003895321278832853 for layer [ True]\n",
      "Iteration: 19700, Loss: 3.9563026428222656 for layer [False]\n",
      "Iteration: 19800, Loss: 3.211928129196167 for layer [False]\n",
      "Iteration: 19900, Loss: 3.0604422092437744 for layer [False]\n",
      "Iteration: 20000, Loss: 2.0746939182281494 for layer [False]\n",
      "Iteration: 20100, Loss: 4.660701274871826 for layer [False]\n",
      "Iteration: 20200, Loss: 0.002274479251354933 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.4407408237457275 for layer [False]\n",
      "Iteration: 20400, Loss: 6.698619108647108e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00030575256096199155 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.000977294985204935 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.252032995223999 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0002176334528485313 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.000755732471588999 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.5755590200424194 for layer [False]\n",
      "Iteration: 21100, Loss: 6.85742712020874 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0001855516602518037 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.00560998916626 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00119543366599828 for layer [ True]\n",
      "Iteration: 21500, Loss: 2.89005446434021 for layer [False]\n",
      "Iteration: 21600, Loss: 3.39254093170166 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00033152944524772465 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0006419572164304554 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.8155975341796875 for layer [False]\n",
      "Iteration: 22000, Loss: 3.6816670894622803 for layer [False]\n",
      "Iteration: 22100, Loss: 7.763897156110033e-05 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00018873298540711403 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.9568252563476562 for layer [False]\n",
      "Iteration: 22400, Loss: 4.254333019256592 for layer [False]\n",
      "Iteration: 22500, Loss: 1.4482860565185547 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00012736314965877682 for layer [ True]\n",
      "Iteration: 22700, Loss: 5.549456182052381e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00019944089581258595 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0007727202028036118 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.002142096171155572 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0020659691654145718 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00012835314555559307 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0001511935261078179 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00028070888947695494 for layer [ True]\n",
      "Iteration: 23500, Loss: 6.4498555730097e-05 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00015672785229980946 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.478724956512451 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0005134553648531437 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0002047856687568128 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00041771747055463493 for layer [ True]\n",
      "Iteration: 24100, Loss: 7.519140490330756e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.417342185974121 for layer [False]\n",
      "Iteration: 24300, Loss: 2.146075963973999 for layer [False]\n",
      "Iteration: 24400, Loss: 3.4257822036743164 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00022361335868481547 for layer [ True]\n",
      "Iteration: 24600, Loss: 9.344642603537068e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 4.107934474945068 for layer [False]\n",
      "Iteration: 24800, Loss: 3.4220738410949707 for layer [False]\n",
      "Iteration: 24900, Loss: 0.0001486952241975814 for layer [ True]\n",
      "Step 8000 | Loss: 0.000468\n",
      "Step 8100 | Loss: 0.000468\n",
      "Step 8200 | Loss: 0.000467\n",
      "Step 8300 | Loss: 0.000466\n",
      "Step 8400 | Loss: 0.000466\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1959.4801025390625 for layer [False]\n",
      "Iteration: 100, Loss: 316.06341552734375 for layer [ True]\n",
      "Iteration: 200, Loss: 1954.125 for layer [False]\n",
      "Iteration: 300, Loss: 1570.7391357421875 for layer [False]\n",
      "Iteration: 400, Loss: 238.99281311035156 for layer [ True]\n",
      "Iteration: 500, Loss: 310.41156005859375 for layer [ True]\n",
      "Iteration: 600, Loss: 1460.7015380859375 for layer [False]\n",
      "Iteration: 700, Loss: 226.7565155029297 for layer [ True]\n",
      "Iteration: 800, Loss: 1182.0521240234375 for layer [False]\n",
      "Iteration: 900, Loss: 1030.296875 for layer [False]\n",
      "Iteration: 1000, Loss: 145.13890075683594 for layer [ True]\n",
      "Iteration: 1100, Loss: 906.336181640625 for layer [False]\n",
      "Iteration: 1200, Loss: 1075.5545654296875 for layer [False]\n",
      "Iteration: 1300, Loss: 1002.8655395507812 for layer [False]\n",
      "Iteration: 1400, Loss: 231.50738525390625 for layer [ True]\n",
      "Iteration: 1500, Loss: 302.20684814453125 for layer [ True]\n",
      "Iteration: 1600, Loss: 131.90553283691406 for layer [ True]\n",
      "Iteration: 1700, Loss: 117.92452239990234 for layer [ True]\n",
      "Iteration: 1800, Loss: 624.2380981445312 for layer [False]\n",
      "Iteration: 1900, Loss: 114.81546020507812 for layer [ True]\n",
      "Iteration: 2000, Loss: 100.42092895507812 for layer [ True]\n",
      "Iteration: 2100, Loss: 157.07891845703125 for layer [ True]\n",
      "Iteration: 2200, Loss: 107.3700180053711 for layer [ True]\n",
      "Iteration: 2300, Loss: 460.9682312011719 for layer [False]\n",
      "Iteration: 2400, Loss: 123.55116271972656 for layer [ True]\n",
      "Iteration: 2500, Loss: 185.80081176757812 for layer [ True]\n",
      "Iteration: 2600, Loss: 118.91658020019531 for layer [ True]\n",
      "Iteration: 2700, Loss: 87.17179870605469 for layer [ True]\n",
      "Iteration: 2800, Loss: 331.2898254394531 for layer [False]\n",
      "Iteration: 2900, Loss: 260.1897888183594 for layer [False]\n",
      "Iteration: 3000, Loss: 79.48512268066406 for layer [ True]\n",
      "Iteration: 3100, Loss: 225.24229431152344 for layer [False]\n",
      "Iteration: 3200, Loss: 234.15557861328125 for layer [False]\n",
      "Iteration: 3300, Loss: 158.9453582763672 for layer [False]\n",
      "Iteration: 3400, Loss: 73.54784393310547 for layer [ True]\n",
      "Iteration: 3500, Loss: 192.45993041992188 for layer [False]\n",
      "Iteration: 3600, Loss: 58.389427185058594 for layer [ True]\n",
      "Iteration: 3700, Loss: 139.35874938964844 for layer [False]\n",
      "Iteration: 3800, Loss: 118.15084838867188 for layer [False]\n",
      "Iteration: 3900, Loss: 42.23766326904297 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.39046096801758 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.221927642822266 for layer [ True]\n",
      "Iteration: 4200, Loss: 77.84378051757812 for layer [ True]\n",
      "Iteration: 4300, Loss: 45.70803451538086 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.25483703613281 for layer [ True]\n",
      "Iteration: 4500, Loss: 53.72340774536133 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.71488571166992 for layer [ True]\n",
      "Iteration: 4700, Loss: 64.58963775634766 for layer [False]\n",
      "Iteration: 4800, Loss: 40.7926139831543 for layer [False]\n",
      "Iteration: 4900, Loss: 52.547000885009766 for layer [False]\n",
      "Iteration: 5000, Loss: 54.79826354980469 for layer [ True]\n",
      "Iteration: 5100, Loss: 22.823888778686523 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.6972541809082 for layer [False]\n",
      "Iteration: 5300, Loss: 22.159257888793945 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.483104705810547 for layer [False]\n",
      "Iteration: 5500, Loss: 24.096582412719727 for layer [ True]\n",
      "Iteration: 5600, Loss: 27.718461990356445 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.934438705444336 for layer [False]\n",
      "Iteration: 5800, Loss: 22.011770248413086 for layer [False]\n",
      "Iteration: 5900, Loss: 13.854426383972168 for layer [False]\n",
      "Iteration: 6000, Loss: 13.591838836669922 for layer [ True]\n",
      "Iteration: 6100, Loss: 20.216167449951172 for layer [False]\n",
      "Iteration: 6200, Loss: 16.4274845123291 for layer [False]\n",
      "Iteration: 6300, Loss: 13.231388092041016 for layer [False]\n",
      "Iteration: 6400, Loss: 15.551925659179688 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.338828086853027 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.204556465148926 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.030141830444336 for layer [False]\n",
      "Iteration: 6800, Loss: 4.777299404144287 for layer [ True]\n",
      "Iteration: 6900, Loss: 36.45672607421875 for layer [False]\n",
      "Iteration: 7000, Loss: 9.599797248840332 for layer [False]\n",
      "Iteration: 7100, Loss: 11.239594459533691 for layer [False]\n",
      "Iteration: 7200, Loss: 14.777697563171387 for layer [False]\n",
      "Iteration: 7300, Loss: 4.527159690856934 for layer [ True]\n",
      "Iteration: 7400, Loss: 12.971446990966797 for layer [False]\n",
      "Iteration: 7500, Loss: 4.414798259735107 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.163810729980469 for layer [ True]\n",
      "Iteration: 7700, Loss: 10.975297927856445 for layer [False]\n",
      "Iteration: 7800, Loss: 8.632615089416504 for layer [False]\n",
      "Iteration: 7900, Loss: 5.384910583496094 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8546929359436035 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.258915901184082 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.678745746612549 for layer [ True]\n",
      "Iteration: 8300, Loss: 11.852478981018066 for layer [False]\n",
      "Iteration: 8400, Loss: 1.9010183811187744 for layer [ True]\n",
      "Iteration: 8500, Loss: 9.709845542907715 for layer [False]\n",
      "Iteration: 8600, Loss: 6.008105278015137 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6384646892547607 for layer [ True]\n",
      "Iteration: 8800, Loss: 4.983069896697998 for layer [False]\n",
      "Iteration: 8900, Loss: 8.676813125610352 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4603468179702759 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5358834266662598 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8142272233963013 for layer [ True]\n",
      "Iteration: 9300, Loss: 5.642652988433838 for layer [False]\n",
      "Iteration: 9400, Loss: 6.198024272918701 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4960111379623413 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.668020486831665 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0371475219726562 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8588113784790039 for layer [ True]\n",
      "Iteration: 9900, Loss: 6.936103820800781 for layer [False]\n",
      "Iteration: 10000, Loss: 3.133958101272583 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1922060251235962 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7480419278144836 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.065276622772217 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5308615565299988 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5451453924179077 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.0837762355804443 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6625938415527344 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.7312328815460205 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5041307806968689 for layer [ True]\n",
      "Iteration: 11000, Loss: 7.0464606285095215 for layer [False]\n",
      "Iteration: 11100, Loss: 2.676274299621582 for layer [False]\n",
      "Iteration: 11200, Loss: 0.34077227115631104 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.41463837027549744 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.33736491203308105 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.4998300075531006 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3456209599971771 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.18820500373840332 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2504618763923645 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.25077226758003235 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11114552617073059 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.17778418958187103 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1588061898946762 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.09973414987325668 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.13381104171276093 for layer [ True]\n",
      "Iteration: 12500, Loss: 4.802428245544434 for layer [False]\n",
      "Iteration: 12600, Loss: 5.110297679901123 for layer [False]\n",
      "Iteration: 12700, Loss: 4.014411449432373 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09287995100021362 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.08675383031368256 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06136593222618103 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.252574443817139 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06049549579620361 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.103687286376953 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04287625104188919 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.054624371230602264 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.044331684708595276 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.014648946933448315 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03633219376206398 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.02979961596429348 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.139841079711914 for layer [False]\n",
      "Iteration: 14100, Loss: 0.027053970843553543 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.750504970550537 for layer [False]\n",
      "Iteration: 14300, Loss: 0.00986179057508707 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.9016051292419434 for layer [False]\n",
      "Iteration: 14500, Loss: 2.159175157546997 for layer [False]\n",
      "Iteration: 14600, Loss: 2.7888503074645996 for layer [False]\n",
      "Iteration: 14700, Loss: 2.8217036724090576 for layer [False]\n",
      "Iteration: 14800, Loss: 3.4006495475769043 for layer [False]\n",
      "Iteration: 14900, Loss: 5.412940979003906 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006114514544606209 for layer [ True]\n",
      "Iteration: 15100, Loss: 4.858154296875 for layer [False]\n",
      "Iteration: 15200, Loss: 7.892613410949707 for layer [False]\n",
      "Iteration: 15300, Loss: 3.97342848777771 for layer [False]\n",
      "Iteration: 15400, Loss: 1.8999253511428833 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033166196662932634 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0039636823348701 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004168832674622536 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.3793752193450928 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002039602492004633 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0033073024824261665 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.847365140914917 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011357975890859962 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020841392688453197 for layer [ True]\n",
      "Iteration: 16400, Loss: 3.341386079788208 for layer [False]\n",
      "Iteration: 16500, Loss: 4.083016872406006 for layer [False]\n",
      "Iteration: 16600, Loss: 2.8733391761779785 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015885017346590757 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007240906124934554 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010117851197719574 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009260093793272972 for layer [ True]\n",
      "Iteration: 17100, Loss: 1.9801462888717651 for layer [False]\n",
      "Iteration: 17200, Loss: 2.5988693237304688 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0007237569661810994 for layer [ True]\n",
      "Iteration: 17400, Loss: 5.036283016204834 for layer [False]\n",
      "Iteration: 17500, Loss: 8.633414268493652 for layer [False]\n",
      "Iteration: 17600, Loss: 3.280078649520874 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008208249928429723 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.110600471496582 for layer [False]\n",
      "Iteration: 17900, Loss: 3.2551279067993164 for layer [False]\n",
      "Iteration: 18000, Loss: 2.046119213104248 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007333109388127923 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005545593448914587 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.142375469207764 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0004228964971844107 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007862210040912032 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.785729169845581 for layer [False]\n",
      "Iteration: 18700, Loss: 3.489053249359131 for layer [False]\n",
      "Iteration: 18800, Loss: 3.7073192596435547 for layer [False]\n",
      "Iteration: 18900, Loss: 1.5554581880569458 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00021991021640133113 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.724337100982666 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0013878054451197386 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.2219862937927246 for layer [False]\n",
      "Iteration: 19400, Loss: 5.159617900848389 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006374155054800212 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003911990497726947 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.084043502807617 for layer [False]\n",
      "Iteration: 19800, Loss: 3.2787721157073975 for layer [False]\n",
      "Iteration: 19900, Loss: 3.179755449295044 for layer [False]\n",
      "Iteration: 20000, Loss: 2.1490025520324707 for layer [False]\n",
      "Iteration: 20100, Loss: 4.840824127197266 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0021351363975554705 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.5475480556488037 for layer [False]\n",
      "Iteration: 20400, Loss: 7.07106664776802e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00030651871929876506 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009606284438632429 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.343324899673462 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00026062698452733457 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0006367381429299712 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.612618088722229 for layer [False]\n",
      "Iteration: 21100, Loss: 7.168498992919922 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0004370766691863537 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.136266708374023 for layer [False]\n",
      "Iteration: 21400, Loss: 0.000418626208556816 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.01212477684021 for layer [False]\n",
      "Iteration: 21600, Loss: 3.543191432952881 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0004392147238831967 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0004464396624825895 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.9073216915130615 for layer [False]\n",
      "Iteration: 22000, Loss: 3.7333462238311768 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0005294298753142357 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0004221011186018586 for layer [ True]\n",
      "Iteration: 22300, Loss: 3.9774622917175293 for layer [False]\n",
      "Iteration: 22400, Loss: 4.464554786682129 for layer [False]\n",
      "Iteration: 22500, Loss: 1.4865834712982178 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0004143382248003036 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00011682691547321156 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0007974883774295449 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.002090380061417818 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.000493848929181695 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0008689553942531347 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00012866465840488672 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0006019422435201705 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00010741157893789932 for layer [ True]\n",
      "Iteration: 23500, Loss: 8.824952965369448e-05 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00037779967533424497 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.581712245941162 for layer [False]\n",
      "Iteration: 23800, Loss: 0.000643627776298672 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00014000268129166216 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00026441572117619216 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.00024701273650862277 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.518268585205078 for layer [False]\n",
      "Iteration: 24300, Loss: 2.182900905609131 for layer [False]\n",
      "Iteration: 24400, Loss: 3.5524682998657227 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0001307409256696701 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.0001643952855374664 for layer [ True]\n",
      "Iteration: 24700, Loss: 4.304351806640625 for layer [False]\n",
      "Iteration: 24800, Loss: 3.4843151569366455 for layer [False]\n",
      "Iteration: 24900, Loss: 6.447510531870648e-05 for layer [ True]\n",
      "Step 8500 | Loss: 0.000465\n",
      "Step 8600 | Loss: 0.000464\n",
      "Step 8700 | Loss: 0.000464\n",
      "Step 8800 | Loss: 0.000463\n",
      "Step 8900 | Loss: 0.000462\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1955.5706787109375 for layer [False]\n",
      "Iteration: 100, Loss: 316.7143249511719 for layer [ True]\n",
      "Iteration: 200, Loss: 1950.954833984375 for layer [False]\n",
      "Iteration: 300, Loss: 1566.037353515625 for layer [False]\n",
      "Iteration: 400, Loss: 238.6599884033203 for layer [ True]\n",
      "Iteration: 500, Loss: 311.5354309082031 for layer [ True]\n",
      "Iteration: 600, Loss: 1458.4056396484375 for layer [False]\n",
      "Iteration: 700, Loss: 226.5189666748047 for layer [ True]\n",
      "Iteration: 800, Loss: 1178.805908203125 for layer [False]\n",
      "Iteration: 900, Loss: 1028.578857421875 for layer [False]\n",
      "Iteration: 1000, Loss: 146.12481689453125 for layer [ True]\n",
      "Iteration: 1100, Loss: 903.4227905273438 for layer [False]\n",
      "Iteration: 1200, Loss: 1072.8958740234375 for layer [False]\n",
      "Iteration: 1300, Loss: 1000.292724609375 for layer [False]\n",
      "Iteration: 1400, Loss: 233.29737854003906 for layer [ True]\n",
      "Iteration: 1500, Loss: 302.7206726074219 for layer [ True]\n",
      "Iteration: 1600, Loss: 132.40975952148438 for layer [ True]\n",
      "Iteration: 1700, Loss: 118.6617431640625 for layer [ True]\n",
      "Iteration: 1800, Loss: 622.0083618164062 for layer [False]\n",
      "Iteration: 1900, Loss: 115.32901000976562 for layer [ True]\n",
      "Iteration: 2000, Loss: 100.89458465576172 for layer [ True]\n",
      "Iteration: 2100, Loss: 157.22914123535156 for layer [ True]\n",
      "Iteration: 2200, Loss: 107.62482452392578 for layer [ True]\n",
      "Iteration: 2300, Loss: 459.2269287109375 for layer [False]\n",
      "Iteration: 2400, Loss: 124.13595581054688 for layer [ True]\n",
      "Iteration: 2500, Loss: 186.0087890625 for layer [ True]\n",
      "Iteration: 2600, Loss: 119.169189453125 for layer [ True]\n",
      "Iteration: 2700, Loss: 87.67512512207031 for layer [ True]\n",
      "Iteration: 2800, Loss: 330.3453063964844 for layer [False]\n",
      "Iteration: 2900, Loss: 258.6770324707031 for layer [False]\n",
      "Iteration: 3000, Loss: 79.67633056640625 for layer [ True]\n",
      "Iteration: 3100, Loss: 224.6050567626953 for layer [False]\n",
      "Iteration: 3200, Loss: 233.0994110107422 for layer [False]\n",
      "Iteration: 3300, Loss: 158.4955596923828 for layer [False]\n",
      "Iteration: 3400, Loss: 73.53271484375 for layer [ True]\n",
      "Iteration: 3500, Loss: 191.71832275390625 for layer [False]\n",
      "Iteration: 3600, Loss: 58.66127395629883 for layer [ True]\n",
      "Iteration: 3700, Loss: 138.39732360839844 for layer [False]\n",
      "Iteration: 3800, Loss: 117.77115631103516 for layer [False]\n",
      "Iteration: 3900, Loss: 42.46432113647461 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.49813461303711 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.23566436767578 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.08024597167969 for layer [ True]\n",
      "Iteration: 4300, Loss: 45.91539764404297 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.37677764892578 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.05713653564453 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.77438735961914 for layer [ True]\n",
      "Iteration: 4700, Loss: 64.05270385742188 for layer [False]\n",
      "Iteration: 4800, Loss: 40.65376281738281 for layer [False]\n",
      "Iteration: 4900, Loss: 52.20480728149414 for layer [False]\n",
      "Iteration: 5000, Loss: 55.10517120361328 for layer [ True]\n",
      "Iteration: 5100, Loss: 22.93787956237793 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.526485443115234 for layer [False]\n",
      "Iteration: 5300, Loss: 22.235626220703125 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.404342651367188 for layer [False]\n",
      "Iteration: 5500, Loss: 24.195390701293945 for layer [ True]\n",
      "Iteration: 5600, Loss: 28.002403259277344 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.988554000854492 for layer [False]\n",
      "Iteration: 5800, Loss: 21.93198013305664 for layer [False]\n",
      "Iteration: 5900, Loss: 13.716261863708496 for layer [False]\n",
      "Iteration: 6000, Loss: 13.59395694732666 for layer [ True]\n",
      "Iteration: 6100, Loss: 20.782377243041992 for layer [False]\n",
      "Iteration: 6200, Loss: 16.43722152709961 for layer [False]\n",
      "Iteration: 6300, Loss: 13.436246871948242 for layer [False]\n",
      "Iteration: 6400, Loss: 15.636890411376953 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.359210968017578 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.298254013061523 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.384618759155273 for layer [False]\n",
      "Iteration: 6800, Loss: 4.804474830627441 for layer [ True]\n",
      "Iteration: 6900, Loss: 37.70991516113281 for layer [False]\n",
      "Iteration: 7000, Loss: 9.758392333984375 for layer [False]\n",
      "Iteration: 7100, Loss: 11.347918510437012 for layer [False]\n",
      "Iteration: 7200, Loss: 15.051644325256348 for layer [False]\n",
      "Iteration: 7300, Loss: 4.559432506561279 for layer [ True]\n",
      "Iteration: 7400, Loss: 13.339070320129395 for layer [False]\n",
      "Iteration: 7500, Loss: 4.4175004959106445 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.169096946716309 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.169300079345703 for layer [False]\n",
      "Iteration: 7800, Loss: 8.840310096740723 for layer [False]\n",
      "Iteration: 7900, Loss: 5.386499404907227 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8490426540374756 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.271907329559326 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.7005581855773926 for layer [ True]\n",
      "Iteration: 8300, Loss: 12.245540618896484 for layer [False]\n",
      "Iteration: 8400, Loss: 1.9165416955947876 for layer [ True]\n",
      "Iteration: 8500, Loss: 10.106925010681152 for layer [False]\n",
      "Iteration: 8600, Loss: 6.269111633300781 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6487748622894287 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.051553726196289 for layer [False]\n",
      "Iteration: 8900, Loss: 8.801018714904785 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4615931510925293 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5456653833389282 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8313307762145996 for layer [ True]\n",
      "Iteration: 9300, Loss: 5.7967424392700195 for layer [False]\n",
      "Iteration: 9400, Loss: 6.461581707000732 for layer [False]\n",
      "Iteration: 9500, Loss: 1.501732587814331 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.7357139587402344 for layer [False]\n",
      "Iteration: 9700, Loss: 1.03676176071167 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8646411895751953 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.0503668785095215 for layer [False]\n",
      "Iteration: 10000, Loss: 3.201035976409912 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1967211961746216 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7478700280189514 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.1568603515625 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5314214825630188 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5460591316223145 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.119476556777954 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6719534397125244 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.851762294769287 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5072637796401978 for layer [ True]\n",
      "Iteration: 11000, Loss: 7.29718542098999 for layer [False]\n",
      "Iteration: 11100, Loss: 2.7659687995910645 for layer [False]\n",
      "Iteration: 11200, Loss: 0.34303903579711914 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.41855111718177795 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3396679162979126 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.556082010269165 for layer [False]\n",
      "Iteration: 11600, Loss: 0.35049131512641907 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.18925030529499054 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.252188116312027 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.25353357195854187 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11207333207130432 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.1790226697921753 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1605612337589264 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10087808221578598 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.13663066923618317 for layer [ True]\n",
      "Iteration: 12500, Loss: 4.907870292663574 for layer [False]\n",
      "Iteration: 12600, Loss: 5.202545166015625 for layer [False]\n",
      "Iteration: 12700, Loss: 4.14122200012207 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09431815892457962 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.08812163025140762 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06159772351384163 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.374153137207031 for layer [False]\n",
      "Iteration: 13200, Loss: 0.061061885207891464 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.247705936431885 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04345560073852539 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05552314966917038 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.045102979987859726 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.014801866374909878 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.036552198231220245 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.030126741155982018 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.1752257347106934 for layer [False]\n",
      "Iteration: 14100, Loss: 0.027487678453326225 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.7680206298828125 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010002008639276028 for layer [ True]\n",
      "Iteration: 14400, Loss: 2.98775315284729 for layer [False]\n",
      "Iteration: 14500, Loss: 2.1530232429504395 for layer [False]\n",
      "Iteration: 14600, Loss: 2.839959144592285 for layer [False]\n",
      "Iteration: 14700, Loss: 2.9024434089660645 for layer [False]\n",
      "Iteration: 14800, Loss: 3.4802322387695312 for layer [False]\n",
      "Iteration: 14900, Loss: 5.693074703216553 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0061463057063519955 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.04811954498291 for layer [False]\n",
      "Iteration: 15200, Loss: 8.19330883026123 for layer [False]\n",
      "Iteration: 15300, Loss: 4.085964679718018 for layer [False]\n",
      "Iteration: 15400, Loss: 1.9920082092285156 for layer [False]\n",
      "Iteration: 15500, Loss: 0.003331468440592289 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004015946760773659 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004109828267246485 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.4296116828918457 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0020702604670077562 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0033343194518238306 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.909428834915161 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011442576069384813 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020891830790787935 for layer [ True]\n",
      "Iteration: 16400, Loss: 3.530064105987549 for layer [False]\n",
      "Iteration: 16500, Loss: 4.153112888336182 for layer [False]\n",
      "Iteration: 16600, Loss: 2.9623351097106934 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015874410746619105 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007179526146501303 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.001007768209092319 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009170795092359185 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.0669615268707275 for layer [False]\n",
      "Iteration: 17200, Loss: 2.6754720211029053 for layer [False]\n",
      "Iteration: 17300, Loss: 0.000711425906047225 for layer [ True]\n",
      "Iteration: 17400, Loss: 5.269723892211914 for layer [False]\n",
      "Iteration: 17500, Loss: 8.983960151672363 for layer [False]\n",
      "Iteration: 17600, Loss: 3.3588411808013916 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008328487747348845 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.171208381652832 for layer [False]\n",
      "Iteration: 17900, Loss: 3.427035331726074 for layer [False]\n",
      "Iteration: 18000, Loss: 2.113661527633667 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007299978169612586 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005649857921525836 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.278739929199219 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0004242701979819685 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.000778460584115237 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.8518295288085938 for layer [False]\n",
      "Iteration: 18700, Loss: 3.5782999992370605 for layer [False]\n",
      "Iteration: 18800, Loss: 3.8940229415893555 for layer [False]\n",
      "Iteration: 18900, Loss: 1.6095763444900513 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0002204503252869472 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.832641124725342 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0014299830654636025 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.287515878677368 for layer [False]\n",
      "Iteration: 19400, Loss: 5.306290149688721 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006340513355098665 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00039074942469596863 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.225317478179932 for layer [False]\n",
      "Iteration: 19800, Loss: 3.350552797317505 for layer [False]\n",
      "Iteration: 19900, Loss: 3.284000873565674 for layer [False]\n",
      "Iteration: 20000, Loss: 2.1871652603149414 for layer [False]\n",
      "Iteration: 20100, Loss: 4.951462268829346 for layer [False]\n",
      "Iteration: 20200, Loss: 0.002066063927486539 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.6416194438934326 for layer [False]\n",
      "Iteration: 20400, Loss: 7.096942863427103e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003136338200420141 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009836198296397924 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.437683582305908 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00025457169977016747 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0007317341514863074 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.6452603340148926 for layer [False]\n",
      "Iteration: 21100, Loss: 7.4175028800964355 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0002838827786035836 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.240756511688232 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0002166822669096291 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.1026713848114014 for layer [False]\n",
      "Iteration: 21600, Loss: 3.720212936401367 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003104315255768597 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.000252709724009037 for layer [ True]\n",
      "Iteration: 21900, Loss: 2.957885265350342 for layer [False]\n",
      "Iteration: 22000, Loss: 3.807725667953491 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0004312879464123398 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00011051918409066275 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.010097980499268 for layer [False]\n",
      "Iteration: 22400, Loss: 4.651443004608154 for layer [False]\n",
      "Iteration: 22500, Loss: 1.5440528392791748 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00013524616952054203 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00017748271056916565 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00013412283442448825 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0010047095129266381 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0007495435420423746 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0006357908714562654 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00027226703241467476 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0002928815665654838 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002216160937678069 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0003904859768226743 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00019715835514944047 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.694779872894287 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007783393957652152 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00023230239457916468 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00029887177515774965 for layer [ True]\n",
      "Iteration: 24100, Loss: 5.411360689322464e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.607560396194458 for layer [False]\n",
      "Iteration: 24300, Loss: 2.2056477069854736 for layer [False]\n",
      "Iteration: 24400, Loss: 3.6530263423919678 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00012811589112970978 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00010823948832694441 for layer [ True]\n",
      "Iteration: 24700, Loss: 4.419783115386963 for layer [False]\n",
      "Iteration: 24800, Loss: 3.61828351020813 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00015682856610510498 for layer [ True]\n",
      "Step 9000 | Loss: 0.000461\n",
      "Step 9100 | Loss: 0.000460\n",
      "Step 9200 | Loss: 0.000460\n",
      "Step 9300 | Loss: 0.000459\n",
      "Step 9400 | Loss: 0.000459\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1950.3795166015625 for layer [False]\n",
      "Iteration: 100, Loss: 317.27508544921875 for layer [ True]\n",
      "Iteration: 200, Loss: 1945.8741455078125 for layer [False]\n",
      "Iteration: 300, Loss: 1559.7947998046875 for layer [False]\n",
      "Iteration: 400, Loss: 239.51443481445312 for layer [ True]\n",
      "Iteration: 500, Loss: 312.24761962890625 for layer [ True]\n",
      "Iteration: 600, Loss: 1453.8812255859375 for layer [False]\n",
      "Iteration: 700, Loss: 227.0447235107422 for layer [ True]\n",
      "Iteration: 800, Loss: 1173.568603515625 for layer [False]\n",
      "Iteration: 900, Loss: 1024.4140625 for layer [False]\n",
      "Iteration: 1000, Loss: 147.5077362060547 for layer [ True]\n",
      "Iteration: 1100, Loss: 900.42578125 for layer [False]\n",
      "Iteration: 1200, Loss: 1068.2459716796875 for layer [False]\n",
      "Iteration: 1300, Loss: 997.42529296875 for layer [False]\n",
      "Iteration: 1400, Loss: 235.75265502929688 for layer [ True]\n",
      "Iteration: 1500, Loss: 305.18408203125 for layer [ True]\n",
      "Iteration: 1600, Loss: 133.04058837890625 for layer [ True]\n",
      "Iteration: 1700, Loss: 118.75495147705078 for layer [ True]\n",
      "Iteration: 1800, Loss: 619.1978149414062 for layer [False]\n",
      "Iteration: 1900, Loss: 116.32199096679688 for layer [ True]\n",
      "Iteration: 2000, Loss: 101.30329132080078 for layer [ True]\n",
      "Iteration: 2100, Loss: 157.92222595214844 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.1891098022461 for layer [ True]\n",
      "Iteration: 2300, Loss: 457.259521484375 for layer [False]\n",
      "Iteration: 2400, Loss: 125.4924087524414 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.55213928222656 for layer [ True]\n",
      "Iteration: 2600, Loss: 119.54732513427734 for layer [ True]\n",
      "Iteration: 2700, Loss: 88.35575866699219 for layer [ True]\n",
      "Iteration: 2800, Loss: 329.0802917480469 for layer [False]\n",
      "Iteration: 2900, Loss: 257.2082824707031 for layer [False]\n",
      "Iteration: 3000, Loss: 80.12679290771484 for layer [ True]\n",
      "Iteration: 3100, Loss: 223.15493774414062 for layer [False]\n",
      "Iteration: 3200, Loss: 231.82928466796875 for layer [False]\n",
      "Iteration: 3300, Loss: 157.43348693847656 for layer [False]\n",
      "Iteration: 3400, Loss: 74.0630111694336 for layer [ True]\n",
      "Iteration: 3500, Loss: 190.47755432128906 for layer [False]\n",
      "Iteration: 3600, Loss: 59.1861686706543 for layer [ True]\n",
      "Iteration: 3700, Loss: 137.22247314453125 for layer [False]\n",
      "Iteration: 3800, Loss: 116.9416275024414 for layer [False]\n",
      "Iteration: 3900, Loss: 42.8333740234375 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.751380920410156 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.65263366699219 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.73532104492188 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.38169479370117 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.782108306884766 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.56911849975586 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.22919464111328 for layer [ True]\n",
      "Iteration: 4700, Loss: 63.451072692871094 for layer [False]\n",
      "Iteration: 4800, Loss: 40.47921371459961 for layer [False]\n",
      "Iteration: 4900, Loss: 51.855438232421875 for layer [False]\n",
      "Iteration: 5000, Loss: 55.62035369873047 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.119876861572266 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.38285446166992 for layer [False]\n",
      "Iteration: 5300, Loss: 22.484779357910156 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.20592498779297 for layer [False]\n",
      "Iteration: 5500, Loss: 24.450361251831055 for layer [ True]\n",
      "Iteration: 5600, Loss: 28.35532569885254 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.95485496520996 for layer [False]\n",
      "Iteration: 5800, Loss: 21.871654510498047 for layer [False]\n",
      "Iteration: 5900, Loss: 13.736059188842773 for layer [False]\n",
      "Iteration: 6000, Loss: 13.663158416748047 for layer [ True]\n",
      "Iteration: 6100, Loss: 21.386322021484375 for layer [False]\n",
      "Iteration: 6200, Loss: 16.61554718017578 for layer [False]\n",
      "Iteration: 6300, Loss: 13.63631820678711 for layer [False]\n",
      "Iteration: 6400, Loss: 15.810466766357422 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.43630313873291 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.457942008972168 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.725507736206055 for layer [False]\n",
      "Iteration: 6800, Loss: 4.862725257873535 for layer [ True]\n",
      "Iteration: 6900, Loss: 38.43240737915039 for layer [False]\n",
      "Iteration: 7000, Loss: 9.882172584533691 for layer [False]\n",
      "Iteration: 7100, Loss: 11.494999885559082 for layer [False]\n",
      "Iteration: 7200, Loss: 15.355313301086426 for layer [False]\n",
      "Iteration: 7300, Loss: 4.617652416229248 for layer [ True]\n",
      "Iteration: 7400, Loss: 13.88261890411377 for layer [False]\n",
      "Iteration: 7500, Loss: 4.443410396575928 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.227362155914307 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.438727378845215 for layer [False]\n",
      "Iteration: 7800, Loss: 9.043780326843262 for layer [False]\n",
      "Iteration: 7900, Loss: 5.40956974029541 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8856217861175537 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.3157055377960205 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.7391741275787354 for layer [ True]\n",
      "Iteration: 8300, Loss: 12.405985832214355 for layer [False]\n",
      "Iteration: 8400, Loss: 1.9424357414245605 for layer [ True]\n",
      "Iteration: 8500, Loss: 10.388422966003418 for layer [False]\n",
      "Iteration: 8600, Loss: 6.420557022094727 for layer [False]\n",
      "Iteration: 8700, Loss: 2.679244041442871 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.147986888885498 for layer [False]\n",
      "Iteration: 8900, Loss: 8.942779541015625 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4732710123062134 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5620898008346558 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8566018342971802 for layer [ True]\n",
      "Iteration: 9300, Loss: 5.879754543304443 for layer [False]\n",
      "Iteration: 9400, Loss: 6.72727108001709 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5057655572891235 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.756321907043457 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0492618083953857 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8733465671539307 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.141154766082764 for layer [False]\n",
      "Iteration: 10000, Loss: 3.208832263946533 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2093935012817383 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7526693344116211 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.2857208251953125 for layer [False]\n",
      "Iteration: 10400, Loss: 0.536696195602417 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5505165457725525 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.179013252258301 for layer [False]\n",
      "Iteration: 10700, Loss: 0.685375988483429 for layer [ True]\n",
      "Iteration: 10800, Loss: 3.939974784851074 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5109089016914368 for layer [ True]\n",
      "Iteration: 11000, Loss: 7.466532230377197 for layer [False]\n",
      "Iteration: 11100, Loss: 2.804487943649292 for layer [False]\n",
      "Iteration: 11200, Loss: 0.34551939368247986 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4233371317386627 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.34400612115859985 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.634389877319336 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3561418354511261 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19180837273597717 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2557099461555481 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.25735920667648315 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11287273466587067 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18039920926094055 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1624918282032013 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10278991609811783 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.1395120918750763 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.071225643157959 for layer [False]\n",
      "Iteration: 12600, Loss: 5.335347652435303 for layer [False]\n",
      "Iteration: 12700, Loss: 4.236437797546387 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09562209993600845 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.08975058048963547 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06166447326540947 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.435973644256592 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06189566105604172 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.334060192108154 for layer [False]\n",
      "Iteration: 13400, Loss: 0.044270943850278854 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05643561854958534 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04595261439681053 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.01502603106200695 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03692905604839325 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03053540550172329 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.240150213241577 for layer [False]\n",
      "Iteration: 14100, Loss: 0.027804046869277954 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.886346817016602 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010115849785506725 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.079944610595703 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2061150074005127 for layer [False]\n",
      "Iteration: 14600, Loss: 2.8972795009613037 for layer [False]\n",
      "Iteration: 14700, Loss: 2.968768835067749 for layer [False]\n",
      "Iteration: 14800, Loss: 3.59426212310791 for layer [False]\n",
      "Iteration: 14900, Loss: 5.767977237701416 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006174907553941011 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.099963665008545 for layer [False]\n",
      "Iteration: 15200, Loss: 8.373334884643555 for layer [False]\n",
      "Iteration: 15300, Loss: 4.255855560302734 for layer [False]\n",
      "Iteration: 15400, Loss: 2.034785032272339 for layer [False]\n",
      "Iteration: 15500, Loss: 0.00336261885240674 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004046980291604996 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004073061048984528 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.498079299926758 for layer [False]\n",
      "Iteration: 15900, Loss: 0.00208752928301692 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003352006198838353 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.9375483989715576 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011674526613205671 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021036858670413494 for layer [ True]\n",
      "Iteration: 16400, Loss: 3.6139402389526367 for layer [False]\n",
      "Iteration: 16500, Loss: 4.270805358886719 for layer [False]\n",
      "Iteration: 16600, Loss: 3.036332607269287 for layer [False]\n",
      "Iteration: 16700, Loss: 0.001560943783260882 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007190078613348305 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010014059953391552 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008972103823907673 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.139626979827881 for layer [False]\n",
      "Iteration: 17200, Loss: 2.773968458175659 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006949960952624679 for layer [ True]\n",
      "Iteration: 17400, Loss: 5.432121276855469 for layer [False]\n",
      "Iteration: 17500, Loss: 9.306915283203125 for layer [False]\n",
      "Iteration: 17600, Loss: 3.423197031021118 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008154142415151 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.2613399028778076 for layer [False]\n",
      "Iteration: 17900, Loss: 3.539384365081787 for layer [False]\n",
      "Iteration: 18000, Loss: 2.1637980937957764 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007162726251408458 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.000537746527697891 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.443455219268799 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00045225597568787634 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008417626959271729 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.8875854015350342 for layer [False]\n",
      "Iteration: 18700, Loss: 3.6454458236694336 for layer [False]\n",
      "Iteration: 18800, Loss: 3.9969236850738525 for layer [False]\n",
      "Iteration: 18900, Loss: 1.6655666828155518 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00046174778253771365 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.92625093460083 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0016038904432207346 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.3439481258392334 for layer [False]\n",
      "Iteration: 19400, Loss: 5.4669718742370605 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005832851165905595 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00039595976704731584 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.347562313079834 for layer [False]\n",
      "Iteration: 19800, Loss: 3.398327350616455 for layer [False]\n",
      "Iteration: 19900, Loss: 3.3525946140289307 for layer [False]\n",
      "Iteration: 20000, Loss: 2.2315099239349365 for layer [False]\n",
      "Iteration: 20100, Loss: 5.034862041473389 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0020919053349643946 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.702279806137085 for layer [False]\n",
      "Iteration: 20400, Loss: 6.91527093295008e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003202266525477171 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.001026494661346078 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.5129709243774414 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00023386775865219533 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0008024273556657135 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.7128221988677979 for layer [False]\n",
      "Iteration: 21100, Loss: 7.680581092834473 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0005887586739845574 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.30601167678833 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0002513528452254832 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.138766050338745 for layer [False]\n",
      "Iteration: 21600, Loss: 3.738300323486328 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0002780661452561617 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0005673232371918857 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.011861801147461 for layer [False]\n",
      "Iteration: 22000, Loss: 3.916120767593384 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00012657495972234756 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00032839266350492835 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.159343242645264 for layer [False]\n",
      "Iteration: 22400, Loss: 4.723740577697754 for layer [False]\n",
      "Iteration: 22500, Loss: 1.5775364637374878 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0001558816438773647 for layer [ True]\n",
      "Iteration: 22700, Loss: 9.790415060706437e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0005697905435226858 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0003128679527435452 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00031984251108951867 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0017740631010383368 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0002586865739431232 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0005113719962537289 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00015921863086987287 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00019081099890172482 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0002985204628203064 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.757558584213257 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007048555999062955 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00013473043509293348 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00012853740190621465 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.00010194785863859579 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.6615920066833496 for layer [False]\n",
      "Iteration: 24300, Loss: 2.312432289123535 for layer [False]\n",
      "Iteration: 24400, Loss: 3.7747392654418945 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00027132497052662075 for layer [ True]\n",
      "Iteration: 24600, Loss: 6.988138920860365e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 4.550835609436035 for layer [False]\n",
      "Iteration: 24800, Loss: 3.7514007091522217 for layer [False]\n",
      "Iteration: 24900, Loss: 9.021519508678466e-05 for layer [ True]\n",
      "Step 9500 | Loss: 0.000458\n",
      "Step 9600 | Loss: 0.000457\n",
      "Step 9700 | Loss: 0.000457\n",
      "Step 9800 | Loss: 0.000457\n",
      "Step 9900 | Loss: 0.000456\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1948.336181640625 for layer [False]\n",
      "Iteration: 100, Loss: 318.0095520019531 for layer [ True]\n",
      "Iteration: 200, Loss: 1941.8629150390625 for layer [False]\n",
      "Iteration: 300, Loss: 1554.6614990234375 for layer [False]\n",
      "Iteration: 400, Loss: 239.0767059326172 for layer [ True]\n",
      "Iteration: 500, Loss: 311.94317626953125 for layer [ True]\n",
      "Iteration: 600, Loss: 1451.310791015625 for layer [False]\n",
      "Iteration: 700, Loss: 226.5536651611328 for layer [ True]\n",
      "Iteration: 800, Loss: 1169.75390625 for layer [False]\n",
      "Iteration: 900, Loss: 1023.2752685546875 for layer [False]\n",
      "Iteration: 1000, Loss: 148.06814575195312 for layer [ True]\n",
      "Iteration: 1100, Loss: 897.8530883789062 for layer [False]\n",
      "Iteration: 1200, Loss: 1066.065185546875 for layer [False]\n",
      "Iteration: 1300, Loss: 995.63232421875 for layer [False]\n",
      "Iteration: 1400, Loss: 236.4872283935547 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.4581298828125 for layer [ True]\n",
      "Iteration: 1600, Loss: 133.2122039794922 for layer [ True]\n",
      "Iteration: 1700, Loss: 118.87059020996094 for layer [ True]\n",
      "Iteration: 1800, Loss: 618.0430908203125 for layer [False]\n",
      "Iteration: 1900, Loss: 116.37060546875 for layer [ True]\n",
      "Iteration: 2000, Loss: 101.55353546142578 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.06646728515625 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.05351257324219 for layer [ True]\n",
      "Iteration: 2300, Loss: 456.6901550292969 for layer [False]\n",
      "Iteration: 2400, Loss: 125.50628662109375 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.7040252685547 for layer [ True]\n",
      "Iteration: 2600, Loss: 119.3678207397461 for layer [ True]\n",
      "Iteration: 2700, Loss: 88.45266723632812 for layer [ True]\n",
      "Iteration: 2800, Loss: 328.37408447265625 for layer [False]\n",
      "Iteration: 2900, Loss: 256.0984802246094 for layer [False]\n",
      "Iteration: 3000, Loss: 80.3021240234375 for layer [ True]\n",
      "Iteration: 3100, Loss: 222.66033935546875 for layer [False]\n",
      "Iteration: 3200, Loss: 230.84921264648438 for layer [False]\n",
      "Iteration: 3300, Loss: 157.14988708496094 for layer [False]\n",
      "Iteration: 3400, Loss: 74.0562744140625 for layer [ True]\n",
      "Iteration: 3500, Loss: 189.593505859375 for layer [False]\n",
      "Iteration: 3600, Loss: 59.17815399169922 for layer [ True]\n",
      "Iteration: 3700, Loss: 136.28604125976562 for layer [False]\n",
      "Iteration: 3800, Loss: 116.5379867553711 for layer [False]\n",
      "Iteration: 3900, Loss: 42.905723571777344 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.54621124267578 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.52208709716797 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.62092590332031 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.33159255981445 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.89502716064453 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.60308074951172 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.20835876464844 for layer [ True]\n",
      "Iteration: 4700, Loss: 63.23887252807617 for layer [False]\n",
      "Iteration: 4800, Loss: 40.34112548828125 for layer [False]\n",
      "Iteration: 4900, Loss: 51.653202056884766 for layer [False]\n",
      "Iteration: 5000, Loss: 55.56925582885742 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.12844467163086 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.31735610961914 for layer [False]\n",
      "Iteration: 5300, Loss: 22.489992141723633 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.16781234741211 for layer [False]\n",
      "Iteration: 5500, Loss: 24.451961517333984 for layer [ True]\n",
      "Iteration: 5600, Loss: 28.499069213867188 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.12894058227539 for layer [False]\n",
      "Iteration: 5800, Loss: 21.844831466674805 for layer [False]\n",
      "Iteration: 5900, Loss: 13.796435356140137 for layer [False]\n",
      "Iteration: 6000, Loss: 13.633362770080566 for layer [ True]\n",
      "Iteration: 6100, Loss: 21.751956939697266 for layer [False]\n",
      "Iteration: 6200, Loss: 16.70187759399414 for layer [False]\n",
      "Iteration: 6300, Loss: 13.92227554321289 for layer [False]\n",
      "Iteration: 6400, Loss: 15.809423446655273 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.45103931427002 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.496716499328613 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.914194107055664 for layer [False]\n",
      "Iteration: 6800, Loss: 4.871764183044434 for layer [ True]\n",
      "Iteration: 6900, Loss: 39.25383377075195 for layer [False]\n",
      "Iteration: 7000, Loss: 10.049394607543945 for layer [False]\n",
      "Iteration: 7100, Loss: 11.51962947845459 for layer [False]\n",
      "Iteration: 7200, Loss: 15.646041870117188 for layer [False]\n",
      "Iteration: 7300, Loss: 4.622798919677734 for layer [ True]\n",
      "Iteration: 7400, Loss: 14.3434419631958 for layer [False]\n",
      "Iteration: 7500, Loss: 4.4369940757751465 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.223305702209473 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.65284252166748 for layer [False]\n",
      "Iteration: 7800, Loss: 9.17083740234375 for layer [False]\n",
      "Iteration: 7900, Loss: 5.427948474884033 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8792736530303955 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.320824146270752 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.7572662830352783 for layer [ True]\n",
      "Iteration: 8300, Loss: 12.64072036743164 for layer [False]\n",
      "Iteration: 8400, Loss: 1.9557697772979736 for layer [ True]\n",
      "Iteration: 8500, Loss: 10.600201606750488 for layer [False]\n",
      "Iteration: 8600, Loss: 6.644744873046875 for layer [False]\n",
      "Iteration: 8700, Loss: 2.684401750564575 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.191542625427246 for layer [False]\n",
      "Iteration: 8900, Loss: 9.064108848571777 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4742802381515503 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.566125750541687 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8597033023834229 for layer [ True]\n",
      "Iteration: 9300, Loss: 5.95168924331665 for layer [False]\n",
      "Iteration: 9400, Loss: 6.861489772796631 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5062264204025269 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.806899309158325 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0566891431808472 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8790008425712585 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.263555526733398 for layer [False]\n",
      "Iteration: 10000, Loss: 3.3277928829193115 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2107048034667969 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7498837113380432 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.334634780883789 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5388849377632141 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5512717962265015 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.2167675495147705 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6933891773223877 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.026206970214844 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5118763446807861 for layer [ True]\n",
      "Iteration: 11000, Loss: 7.656774520874023 for layer [False]\n",
      "Iteration: 11100, Loss: 2.8670547008514404 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3468186855316162 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4276309609413147 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3436048924922943 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.6830356121063232 for layer [False]\n",
      "Iteration: 11600, Loss: 0.35786962509155273 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19278962910175323 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.25671854615211487 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2577710747718811 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1126680076122284 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18125206232070923 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.16318242251873016 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10326862335205078 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.14120732247829437 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.1597161293029785 for layer [False]\n",
      "Iteration: 12600, Loss: 5.392033576965332 for layer [False]\n",
      "Iteration: 12700, Loss: 4.28969669342041 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09655708074569702 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09065533429384232 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06144315004348755 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.479582786560059 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06190963461995125 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.3679609298706055 for layer [False]\n",
      "Iteration: 13400, Loss: 0.0447782464325428 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05687801167368889 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04657275602221489 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.015148922801017761 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03708702325820923 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.030574999749660492 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.30604887008667 for layer [False]\n",
      "Iteration: 14100, Loss: 0.028091032058000565 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.903099060058594 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010124505497515202 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.089425563812256 for layer [False]\n",
      "Iteration: 14500, Loss: 2.191058397293091 for layer [False]\n",
      "Iteration: 14600, Loss: 2.9753496646881104 for layer [False]\n",
      "Iteration: 14700, Loss: 2.98413348197937 for layer [False]\n",
      "Iteration: 14800, Loss: 3.582974672317505 for layer [False]\n",
      "Iteration: 14900, Loss: 5.9708709716796875 for layer [False]\n",
      "Iteration: 15000, Loss: 0.00624115252867341 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.195822238922119 for layer [False]\n",
      "Iteration: 15200, Loss: 8.48015022277832 for layer [False]\n",
      "Iteration: 15300, Loss: 4.2631635665893555 for layer [False]\n",
      "Iteration: 15400, Loss: 2.072359800338745 for layer [False]\n",
      "Iteration: 15500, Loss: 0.003382408060133457 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004073134623467922 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004027518909424543 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.5425946712493896 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0021174342837184668 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0033763134852051735 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.969834327697754 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011781934881582856 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.002099947538226843 for layer [ True]\n",
      "Iteration: 16400, Loss: 3.752387285232544 for layer [False]\n",
      "Iteration: 16500, Loss: 4.385080337524414 for layer [False]\n",
      "Iteration: 16600, Loss: 3.09182071685791 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015571152325719595 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007284472230821848 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009921332821249962 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008880951208993793 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.154082775115967 for layer [False]\n",
      "Iteration: 17200, Loss: 2.7687628269195557 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006898419233039021 for layer [ True]\n",
      "Iteration: 17400, Loss: 5.563126087188721 for layer [False]\n",
      "Iteration: 17500, Loss: 9.463346481323242 for layer [False]\n",
      "Iteration: 17600, Loss: 3.521149158477783 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008193150861188769 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.2859742641448975 for layer [False]\n",
      "Iteration: 17900, Loss: 3.6612091064453125 for layer [False]\n",
      "Iteration: 18000, Loss: 2.1950602531433105 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007213194039650261 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005429174052551389 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.531006336212158 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0004421417252160609 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008127604378387332 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.9310961961746216 for layer [False]\n",
      "Iteration: 18700, Loss: 3.7078373432159424 for layer [False]\n",
      "Iteration: 18800, Loss: 4.052420616149902 for layer [False]\n",
      "Iteration: 18900, Loss: 1.6926374435424805 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00043382434523664415 for layer [ True]\n",
      "Iteration: 19100, Loss: 2.9951677322387695 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0016864534700289369 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.3826794624328613 for layer [False]\n",
      "Iteration: 19400, Loss: 5.603190898895264 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005423239199444652 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00040966764208860695 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.479103088378906 for layer [False]\n",
      "Iteration: 19800, Loss: 3.383579730987549 for layer [False]\n",
      "Iteration: 19900, Loss: 3.4124279022216797 for layer [False]\n",
      "Iteration: 20000, Loss: 2.2760443687438965 for layer [False]\n",
      "Iteration: 20100, Loss: 5.11969518661499 for layer [False]\n",
      "Iteration: 20200, Loss: 0.002123584970831871 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.769653558731079 for layer [False]\n",
      "Iteration: 20400, Loss: 6.864853639854118e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003212881274521351 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0010525673860684037 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.5833990573883057 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00024184734502341598 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0007927513215690851 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.749293327331543 for layer [False]\n",
      "Iteration: 21100, Loss: 7.862160682678223 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00018997094593942165 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.358558177947998 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0006548475939780474 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.1937224864959717 for layer [False]\n",
      "Iteration: 21600, Loss: 3.926790714263916 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00017735834990162402 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00037991779390722513 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.006152391433716 for layer [False]\n",
      "Iteration: 22000, Loss: 3.843475580215454 for layer [False]\n",
      "Iteration: 22100, Loss: 9.542987390886992e-05 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00016980967484414577 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.153899669647217 for layer [False]\n",
      "Iteration: 22400, Loss: 4.861082553863525 for layer [False]\n",
      "Iteration: 22500, Loss: 1.5943647623062134 for layer [False]\n",
      "Iteration: 22600, Loss: 8.145121682900935e-05 for layer [ True]\n",
      "Iteration: 22700, Loss: 9.342778503196314e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0004564867413137108 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0006543449708260596 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0025297473184764385 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.002102710772305727 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00013176733045838773 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00018610752886161208 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002936570963356644 for layer [ True]\n",
      "Iteration: 23500, Loss: 9.486708586337045e-05 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00012957076251041144 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.7865898609161377 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006381563725881279 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0002048846654361114 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00021152783301658928 for layer [ True]\n",
      "Iteration: 24100, Loss: 8.194548718165606e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.732783794403076 for layer [False]\n",
      "Iteration: 24300, Loss: 2.3070967197418213 for layer [False]\n",
      "Iteration: 24400, Loss: 3.7580831050872803 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00024368181766476482 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00010284034942742437 for layer [ True]\n",
      "Iteration: 24700, Loss: 4.693110942840576 for layer [False]\n",
      "Iteration: 24800, Loss: 3.7637596130371094 for layer [False]\n",
      "Iteration: 24900, Loss: 0.0001151512042270042 for layer [ True]\n",
      "Step 10000 | Loss: 0.000456\n",
      "Step 10100 | Loss: 0.000455\n",
      "Step 10200 | Loss: 0.000455\n",
      "Step 10300 | Loss: 0.000455\n",
      "Step 10400 | Loss: 0.000454\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1944.6942138671875 for layer [False]\n",
      "Iteration: 100, Loss: 318.13983154296875 for layer [ True]\n",
      "Iteration: 200, Loss: 1939.05029296875 for layer [False]\n",
      "Iteration: 300, Loss: 1549.4271240234375 for layer [False]\n",
      "Iteration: 400, Loss: 238.6796112060547 for layer [ True]\n",
      "Iteration: 500, Loss: 312.6716003417969 for layer [ True]\n",
      "Iteration: 600, Loss: 1448.314453125 for layer [False]\n",
      "Iteration: 700, Loss: 226.1868133544922 for layer [ True]\n",
      "Iteration: 800, Loss: 1165.7874755859375 for layer [False]\n",
      "Iteration: 900, Loss: 1021.1675415039062 for layer [False]\n",
      "Iteration: 1000, Loss: 149.04522705078125 for layer [ True]\n",
      "Iteration: 1100, Loss: 895.7473754882812 for layer [False]\n",
      "Iteration: 1200, Loss: 1063.029296875 for layer [False]\n",
      "Iteration: 1300, Loss: 993.4669189453125 for layer [False]\n",
      "Iteration: 1400, Loss: 237.42245483398438 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.8437805175781 for layer [ True]\n",
      "Iteration: 1600, Loss: 133.64141845703125 for layer [ True]\n",
      "Iteration: 1700, Loss: 119.29769134521484 for layer [ True]\n",
      "Iteration: 1800, Loss: 616.615234375 for layer [False]\n",
      "Iteration: 1900, Loss: 116.34945678710938 for layer [ True]\n",
      "Iteration: 2000, Loss: 101.86894226074219 for layer [ True]\n",
      "Iteration: 2100, Loss: 157.99945068359375 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.0313720703125 for layer [ True]\n",
      "Iteration: 2300, Loss: 455.33197021484375 for layer [False]\n",
      "Iteration: 2400, Loss: 125.59317779541016 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.71278381347656 for layer [ True]\n",
      "Iteration: 2600, Loss: 119.31731414794922 for layer [ True]\n",
      "Iteration: 2700, Loss: 88.66890716552734 for layer [ True]\n",
      "Iteration: 2800, Loss: 327.73406982421875 for layer [False]\n",
      "Iteration: 2900, Loss: 254.88308715820312 for layer [False]\n",
      "Iteration: 3000, Loss: 80.22425079345703 for layer [ True]\n",
      "Iteration: 3100, Loss: 221.54351806640625 for layer [False]\n",
      "Iteration: 3200, Loss: 229.94338989257812 for layer [False]\n",
      "Iteration: 3300, Loss: 156.4107666015625 for layer [False]\n",
      "Iteration: 3400, Loss: 73.83465576171875 for layer [ True]\n",
      "Iteration: 3500, Loss: 188.8236846923828 for layer [False]\n",
      "Iteration: 3600, Loss: 59.286277770996094 for layer [ True]\n",
      "Iteration: 3700, Loss: 135.26239013671875 for layer [False]\n",
      "Iteration: 3800, Loss: 115.91633605957031 for layer [False]\n",
      "Iteration: 3900, Loss: 42.97526168823242 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.49335861206055 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.34495544433594 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.58170318603516 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.358924865722656 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.77373504638672 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.59698486328125 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.1600456237793 for layer [ True]\n",
      "Iteration: 4700, Loss: 62.70325469970703 for layer [False]\n",
      "Iteration: 4800, Loss: 40.21916961669922 for layer [False]\n",
      "Iteration: 4900, Loss: 51.36052703857422 for layer [False]\n",
      "Iteration: 5000, Loss: 55.54193115234375 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.167247772216797 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.235233306884766 for layer [False]\n",
      "Iteration: 5300, Loss: 22.477758407592773 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.14164924621582 for layer [False]\n",
      "Iteration: 5500, Loss: 24.408090591430664 for layer [ True]\n",
      "Iteration: 5600, Loss: 28.56963348388672 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.19318389892578 for layer [False]\n",
      "Iteration: 5800, Loss: 21.819604873657227 for layer [False]\n",
      "Iteration: 5900, Loss: 13.873981475830078 for layer [False]\n",
      "Iteration: 6000, Loss: 13.611281394958496 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.155397415161133 for layer [False]\n",
      "Iteration: 6200, Loss: 16.945154190063477 for layer [False]\n",
      "Iteration: 6300, Loss: 14.089387893676758 for layer [False]\n",
      "Iteration: 6400, Loss: 15.846170425415039 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.45380687713623 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.509652137756348 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.18259620666504 for layer [False]\n",
      "Iteration: 6800, Loss: 4.8748650550842285 for layer [ True]\n",
      "Iteration: 6900, Loss: 39.84242630004883 for layer [False]\n",
      "Iteration: 7000, Loss: 10.098875999450684 for layer [False]\n",
      "Iteration: 7100, Loss: 11.642958641052246 for layer [False]\n",
      "Iteration: 7200, Loss: 15.959172248840332 for layer [False]\n",
      "Iteration: 7300, Loss: 4.625298500061035 for layer [ True]\n",
      "Iteration: 7400, Loss: 14.920755386352539 for layer [False]\n",
      "Iteration: 7500, Loss: 4.442157745361328 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.214327335357666 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.75210952758789 for layer [False]\n",
      "Iteration: 7800, Loss: 9.3004732131958 for layer [False]\n",
      "Iteration: 7900, Loss: 5.421228408813477 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8660666942596436 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.317815065383911 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.7668986320495605 for layer [ True]\n",
      "Iteration: 8300, Loss: 12.797277450561523 for layer [False]\n",
      "Iteration: 8400, Loss: 1.9628825187683105 for layer [ True]\n",
      "Iteration: 8500, Loss: 10.789281845092773 for layer [False]\n",
      "Iteration: 8600, Loss: 6.779419898986816 for layer [False]\n",
      "Iteration: 8700, Loss: 2.680640697479248 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.249054431915283 for layer [False]\n",
      "Iteration: 8900, Loss: 9.210941314697266 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4708815813064575 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5603694915771484 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.860532522201538 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.004397392272949 for layer [False]\n",
      "Iteration: 9400, Loss: 7.116076469421387 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5024539232254028 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.810683012008667 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0562400817871094 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.880682647228241 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.247071266174316 for layer [False]\n",
      "Iteration: 10000, Loss: 3.3417043685913086 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2090386152267456 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7453588843345642 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.441990375518799 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5403093695640564 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5539873242378235 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.256359100341797 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6970357894897461 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.060161113739014 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5119112730026245 for layer [ True]\n",
      "Iteration: 11000, Loss: 7.823873519897461 for layer [False]\n",
      "Iteration: 11100, Loss: 2.9181289672851562 for layer [False]\n",
      "Iteration: 11200, Loss: 0.34697213768959045 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.42980581521987915 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.34450966119766235 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.754396915435791 for layer [False]\n",
      "Iteration: 11600, Loss: 0.35992321372032166 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19403348863124847 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.25756725668907166 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2599588930606842 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1137658953666687 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18235480785369873 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.16411584615707397 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10375582426786423 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.14257891476154327 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.286721229553223 for layer [False]\n",
      "Iteration: 12600, Loss: 5.407059669494629 for layer [False]\n",
      "Iteration: 12700, Loss: 4.365036487579346 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09742111712694168 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09144958108663559 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.0616702176630497 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.529266834259033 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06239256635308266 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.373012065887451 for layer [False]\n",
      "Iteration: 13400, Loss: 0.045310910791158676 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05728660523891449 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.046972207725048065 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.015332137234508991 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03736443445086479 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03067556582391262 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.3585493564605713 for layer [False]\n",
      "Iteration: 14100, Loss: 0.02831968478858471 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.948992729187012 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010208708234131336 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.1524181365966797 for layer [False]\n",
      "Iteration: 14500, Loss: 2.206623077392578 for layer [False]\n",
      "Iteration: 14600, Loss: 3.0428645610809326 for layer [False]\n",
      "Iteration: 14700, Loss: 3.039149761199951 for layer [False]\n",
      "Iteration: 14800, Loss: 3.629411220550537 for layer [False]\n",
      "Iteration: 14900, Loss: 6.022067546844482 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006318665109574795 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.235025405883789 for layer [False]\n",
      "Iteration: 15200, Loss: 8.647602081298828 for layer [False]\n",
      "Iteration: 15300, Loss: 4.354060173034668 for layer [False]\n",
      "Iteration: 15400, Loss: 2.101269245147705 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0034077607560902834 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004126289859414101 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004050194751471281 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.60640811920166 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0021385636646300554 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003423921065405011 for layer [ True]\n",
      "Iteration: 16100, Loss: 2.9907262325286865 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011887081200256944 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021160361357033253 for layer [ True]\n",
      "Iteration: 16400, Loss: 3.8422982692718506 for layer [False]\n",
      "Iteration: 16500, Loss: 4.455921649932861 for layer [False]\n",
      "Iteration: 16600, Loss: 3.1504902839660645 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015590011607855558 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007318826392292976 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009950647363439202 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009082833421416581 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.22055721282959 for layer [False]\n",
      "Iteration: 17200, Loss: 2.823359727859497 for layer [False]\n",
      "Iteration: 17300, Loss: 0.000692455330863595 for layer [ True]\n",
      "Iteration: 17400, Loss: 5.706503391265869 for layer [False]\n",
      "Iteration: 17500, Loss: 9.777441024780273 for layer [False]\n",
      "Iteration: 17600, Loss: 3.525768280029297 for layer [False]\n",
      "Iteration: 17700, Loss: 0.00083731091581285 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.3568942546844482 for layer [False]\n",
      "Iteration: 17900, Loss: 3.7518081665039062 for layer [False]\n",
      "Iteration: 18000, Loss: 2.2510526180267334 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007374564884230494 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005528131150640547 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.63414192199707 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00044062541564926505 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008086638990789652 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.9314881563186646 for layer [False]\n",
      "Iteration: 18700, Loss: 3.7697036266326904 for layer [False]\n",
      "Iteration: 18800, Loss: 4.121525287628174 for layer [False]\n",
      "Iteration: 18900, Loss: 1.7479506731033325 for layer [False]\n",
      "Iteration: 19000, Loss: 0.000530015560798347 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.0954971313476562 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0014697808073833585 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.4162416458129883 for layer [False]\n",
      "Iteration: 19400, Loss: 5.7281036376953125 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0005490952171385288 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00041103234980255365 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.536230087280273 for layer [False]\n",
      "Iteration: 19800, Loss: 3.409080982208252 for layer [False]\n",
      "Iteration: 19900, Loss: 3.4997355937957764 for layer [False]\n",
      "Iteration: 20000, Loss: 2.3416337966918945 for layer [False]\n",
      "Iteration: 20100, Loss: 5.2040276527404785 for layer [False]\n",
      "Iteration: 20200, Loss: 0.002061602659523487 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.7780022621154785 for layer [False]\n",
      "Iteration: 20400, Loss: 6.853090599179268e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00033247139072045684 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0010365510825067759 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.632519483566284 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0002320589846931398 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0010600427631288767 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.792000412940979 for layer [False]\n",
      "Iteration: 21100, Loss: 8.00407886505127 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0007047925610095263 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.397594451904297 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00015344310668297112 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.2306697368621826 for layer [False]\n",
      "Iteration: 21600, Loss: 3.933195114135742 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00027645917725749314 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.000412146036978811 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.0453481674194336 for layer [False]\n",
      "Iteration: 22000, Loss: 3.8975307941436768 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0002974322414956987 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00016804045299068093 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.214530944824219 for layer [False]\n",
      "Iteration: 22400, Loss: 4.934266090393066 for layer [False]\n",
      "Iteration: 22500, Loss: 1.6451336145401 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00019112876907456666 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00019502427312545478 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00016547180712223053 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.001818025833927095 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00037196173798292875 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0007513909367844462 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00019367899221833795 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00029124770662747324 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00023794005392119288 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0003484638000372797 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0001005964440992102 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.8227083683013916 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0003343094140291214 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00027515849797055125 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00030071355286054313 for layer [ True]\n",
      "Iteration: 24100, Loss: 4.421708581503481e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.775390148162842 for layer [False]\n",
      "Iteration: 24300, Loss: 2.3525516986846924 for layer [False]\n",
      "Iteration: 24400, Loss: 3.8955235481262207 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0005564872990362346 for layer [ True]\n",
      "Iteration: 24600, Loss: 6.0822520026704296e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 4.822440147399902 for layer [False]\n",
      "Iteration: 24800, Loss: 3.8507189750671387 for layer [False]\n",
      "Iteration: 24900, Loss: 0.0002584029862191528 for layer [ True]\n",
      "Step 10500 | Loss: 0.000454\n",
      "Step 10600 | Loss: 0.000454\n",
      "Step 10700 | Loss: 0.000454\n",
      "Step 10800 | Loss: 0.000453\n",
      "Step 10900 | Loss: 0.000453\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1942.49462890625 for layer [False]\n",
      "Iteration: 100, Loss: 317.9234313964844 for layer [ True]\n",
      "Iteration: 200, Loss: 1935.9324951171875 for layer [False]\n",
      "Iteration: 300, Loss: 1545.8209228515625 for layer [False]\n",
      "Iteration: 400, Loss: 238.90216064453125 for layer [ True]\n",
      "Iteration: 500, Loss: 313.1972961425781 for layer [ True]\n",
      "Iteration: 600, Loss: 1446.6031494140625 for layer [False]\n",
      "Iteration: 700, Loss: 225.78366088867188 for layer [ True]\n",
      "Iteration: 800, Loss: 1163.215576171875 for layer [False]\n",
      "Iteration: 900, Loss: 1019.8099975585938 for layer [False]\n",
      "Iteration: 1000, Loss: 149.93069458007812 for layer [ True]\n",
      "Iteration: 1100, Loss: 894.0873413085938 for layer [False]\n",
      "Iteration: 1200, Loss: 1060.68212890625 for layer [False]\n",
      "Iteration: 1300, Loss: 991.4746704101562 for layer [False]\n",
      "Iteration: 1400, Loss: 238.72930908203125 for layer [ True]\n",
      "Iteration: 1500, Loss: 306.2735595703125 for layer [ True]\n",
      "Iteration: 1600, Loss: 134.501220703125 for layer [ True]\n",
      "Iteration: 1700, Loss: 119.93375396728516 for layer [ True]\n",
      "Iteration: 1800, Loss: 615.3786010742188 for layer [False]\n",
      "Iteration: 1900, Loss: 116.67689514160156 for layer [ True]\n",
      "Iteration: 2000, Loss: 102.33995056152344 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.07762145996094 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.13133239746094 for layer [ True]\n",
      "Iteration: 2300, Loss: 454.4772033691406 for layer [False]\n",
      "Iteration: 2400, Loss: 126.32273864746094 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.95245361328125 for layer [ True]\n",
      "Iteration: 2600, Loss: 119.79493713378906 for layer [ True]\n",
      "Iteration: 2700, Loss: 89.18917846679688 for layer [ True]\n",
      "Iteration: 2800, Loss: 327.0956115722656 for layer [False]\n",
      "Iteration: 2900, Loss: 254.0699920654297 for layer [False]\n",
      "Iteration: 3000, Loss: 80.3057632446289 for layer [ True]\n",
      "Iteration: 3100, Loss: 220.94590759277344 for layer [False]\n",
      "Iteration: 3200, Loss: 229.2332763671875 for layer [False]\n",
      "Iteration: 3300, Loss: 156.16539001464844 for layer [False]\n",
      "Iteration: 3400, Loss: 73.68043518066406 for layer [ True]\n",
      "Iteration: 3500, Loss: 188.40475463867188 for layer [False]\n",
      "Iteration: 3600, Loss: 59.57295608520508 for layer [ True]\n",
      "Iteration: 3700, Loss: 134.49131774902344 for layer [False]\n",
      "Iteration: 3800, Loss: 115.40831756591797 for layer [False]\n",
      "Iteration: 3900, Loss: 43.10171890258789 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.504451751708984 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.27337646484375 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.6387939453125 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.36589813232422 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.891258239746094 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.65719223022461 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.1216926574707 for layer [ True]\n",
      "Iteration: 4700, Loss: 62.41172790527344 for layer [False]\n",
      "Iteration: 4800, Loss: 40.1688232421875 for layer [False]\n",
      "Iteration: 4900, Loss: 51.10777282714844 for layer [False]\n",
      "Iteration: 5000, Loss: 55.71281051635742 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.21402931213379 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.14352035522461 for layer [False]\n",
      "Iteration: 5300, Loss: 22.54899024963379 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.04840850830078 for layer [False]\n",
      "Iteration: 5500, Loss: 24.457721710205078 for layer [ True]\n",
      "Iteration: 5600, Loss: 28.771284103393555 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.2327823638916 for layer [False]\n",
      "Iteration: 5800, Loss: 21.804943084716797 for layer [False]\n",
      "Iteration: 5900, Loss: 13.842007637023926 for layer [False]\n",
      "Iteration: 6000, Loss: 13.66103744506836 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.380523681640625 for layer [False]\n",
      "Iteration: 6200, Loss: 17.138530731201172 for layer [False]\n",
      "Iteration: 6300, Loss: 14.24379825592041 for layer [False]\n",
      "Iteration: 6400, Loss: 15.94272518157959 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.49490737915039 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.56494140625 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.3884334564209 for layer [False]\n",
      "Iteration: 6800, Loss: 4.910408973693848 for layer [ True]\n",
      "Iteration: 6900, Loss: 40.69843292236328 for layer [False]\n",
      "Iteration: 7000, Loss: 10.20458984375 for layer [False]\n",
      "Iteration: 7100, Loss: 11.714156150817871 for layer [False]\n",
      "Iteration: 7200, Loss: 16.211057662963867 for layer [False]\n",
      "Iteration: 7300, Loss: 4.647701740264893 for layer [ True]\n",
      "Iteration: 7400, Loss: 15.2393159866333 for layer [False]\n",
      "Iteration: 7500, Loss: 4.449991226196289 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.227094650268555 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.816828727722168 for layer [False]\n",
      "Iteration: 7800, Loss: 9.447840690612793 for layer [False]\n",
      "Iteration: 7900, Loss: 5.452744007110596 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8752753734588623 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.3399908542633057 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.790252208709717 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.017147064208984 for layer [False]\n",
      "Iteration: 8400, Loss: 1.977065086364746 for layer [ True]\n",
      "Iteration: 8500, Loss: 10.979615211486816 for layer [False]\n",
      "Iteration: 8600, Loss: 6.895413875579834 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6876628398895264 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.295015811920166 for layer [False]\n",
      "Iteration: 8900, Loss: 9.256555557250977 for layer [False]\n",
      "Iteration: 9000, Loss: 1.481081485748291 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5697911977767944 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.866915225982666 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.07457160949707 for layer [False]\n",
      "Iteration: 9400, Loss: 7.262722015380859 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5079782009124756 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.8414034843444824 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0626100301742554 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8833773136138916 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.25916862487793 for layer [False]\n",
      "Iteration: 10000, Loss: 3.3870482444763184 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2129853963851929 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7472399473190308 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.511005878448486 for layer [False]\n",
      "Iteration: 10400, Loss: 0.540618360042572 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.557152271270752 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.3170552253723145 for layer [False]\n",
      "Iteration: 10700, Loss: 0.702875018119812 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.083009719848633 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5168789625167847 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.00657844543457 for layer [False]\n",
      "Iteration: 11100, Loss: 2.956282615661621 for layer [False]\n",
      "Iteration: 11200, Loss: 0.351311594247818 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.43501949310302734 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.34569263458251953 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.7645070552825928 for layer [False]\n",
      "Iteration: 11600, Loss: 0.36314913630485535 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19439472258090973 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.25908133387565613 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2612975537776947 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11489827185869217 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18437910079956055 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1653352528810501 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10413044691085815 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.14484544098377228 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.335913181304932 for layer [False]\n",
      "Iteration: 12600, Loss: 5.481552600860596 for layer [False]\n",
      "Iteration: 12700, Loss: 4.382025241851807 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09819521754980087 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09201966971158981 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06206439062952995 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.558140277862549 for layer [False]\n",
      "Iteration: 13200, Loss: 0.0626140832901001 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.384467601776123 for layer [False]\n",
      "Iteration: 13400, Loss: 0.045540813356637955 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.058121077716350555 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04740773141384125 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.01548689417541027 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03751342371106148 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.030914511531591415 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.373645305633545 for layer [False]\n",
      "Iteration: 14100, Loss: 0.02857905812561512 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.970021724700928 for layer [False]\n",
      "Iteration: 14300, Loss: 0.01027669757604599 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.1657185554504395 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2161378860473633 for layer [False]\n",
      "Iteration: 14600, Loss: 3.0830888748168945 for layer [False]\n",
      "Iteration: 14700, Loss: 3.0784831047058105 for layer [False]\n",
      "Iteration: 14800, Loss: 3.66068959236145 for layer [False]\n",
      "Iteration: 14900, Loss: 6.113593578338623 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006371766794472933 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.29161262512207 for layer [False]\n",
      "Iteration: 15200, Loss: 8.773209571838379 for layer [False]\n",
      "Iteration: 15300, Loss: 4.391585826873779 for layer [False]\n",
      "Iteration: 15400, Loss: 2.1419754028320312 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0034121384378522635 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004158289637416601 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004056683741509914 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.6480815410614014 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0021761611569672823 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003423490561544895 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.0108799934387207 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011884488631039858 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.002110792323946953 for layer [ True]\n",
      "Iteration: 16400, Loss: 3.912663698196411 for layer [False]\n",
      "Iteration: 16500, Loss: 4.467088222503662 for layer [False]\n",
      "Iteration: 16600, Loss: 3.1889586448669434 for layer [False]\n",
      "Iteration: 16700, Loss: 0.001553997048176825 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.000731996784452349 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010026629315689206 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008967070025391877 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.2423181533813477 for layer [False]\n",
      "Iteration: 17200, Loss: 2.8659486770629883 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006879110587760806 for layer [ True]\n",
      "Iteration: 17400, Loss: 5.860304832458496 for layer [False]\n",
      "Iteration: 17500, Loss: 9.874174118041992 for layer [False]\n",
      "Iteration: 17600, Loss: 3.546710968017578 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008353154407814145 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.397242307662964 for layer [False]\n",
      "Iteration: 17900, Loss: 3.8139586448669434 for layer [False]\n",
      "Iteration: 18000, Loss: 2.2845723628997803 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007454596343450248 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0005833962932229042 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.678868293762207 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0004225432639941573 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007745432085357606 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.9569898843765259 for layer [False]\n",
      "Iteration: 18700, Loss: 3.835879325866699 for layer [False]\n",
      "Iteration: 18800, Loss: 4.196610927581787 for layer [False]\n",
      "Iteration: 18900, Loss: 1.773322582244873 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0002090581983793527 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.1581759452819824 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0015957201831042767 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.448859214782715 for layer [False]\n",
      "Iteration: 19400, Loss: 5.824745178222656 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006174353184178472 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00039578849100507796 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.586440086364746 for layer [False]\n",
      "Iteration: 19800, Loss: 3.4250338077545166 for layer [False]\n",
      "Iteration: 19900, Loss: 3.546010971069336 for layer [False]\n",
      "Iteration: 20000, Loss: 2.379282236099243 for layer [False]\n",
      "Iteration: 20100, Loss: 5.207526683807373 for layer [False]\n",
      "Iteration: 20200, Loss: 0.002070170361548662 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.801222562789917 for layer [False]\n",
      "Iteration: 20400, Loss: 7.010293484199792e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003318753733765334 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0010232208296656609 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.6481781005859375 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0002749385603237897 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.000910213275346905 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.8330402374267578 for layer [False]\n",
      "Iteration: 21100, Loss: 8.11201286315918 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0009145878721028566 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.46436882019043 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00014527620805893093 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.282597541809082 for layer [False]\n",
      "Iteration: 21600, Loss: 4.0118021965026855 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0002855527272913605 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0002308854745933786 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.0760741233825684 for layer [False]\n",
      "Iteration: 22000, Loss: 3.9414756298065186 for layer [False]\n",
      "Iteration: 22100, Loss: 7.376506982836872e-05 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.000703374738804996 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.222355365753174 for layer [False]\n",
      "Iteration: 22400, Loss: 5.00197696685791 for layer [False]\n",
      "Iteration: 22500, Loss: 1.67835533618927 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00025524551165290177 for layer [ True]\n",
      "Iteration: 22700, Loss: 7.700562127865851e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0002519185945857316 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.002276540268212557 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00023395045718643814 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00038128942833282053 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00028917539748363197 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0002443189441692084 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002363833482377231 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0003665427793748677 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00019089771376457065 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.8774986267089844 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007255984819494188 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00021694616589229554 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0003176125464960933 for layer [ True]\n",
      "Iteration: 24100, Loss: 5.781868821941316e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.7992751598358154 for layer [False]\n",
      "Iteration: 24300, Loss: 2.354619026184082 for layer [False]\n",
      "Iteration: 24400, Loss: 3.9536709785461426 for layer [False]\n",
      "Iteration: 24500, Loss: 7.959954382386059e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 7.238929538289085e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 4.956984043121338 for layer [False]\n",
      "Iteration: 24800, Loss: 3.8704793453216553 for layer [False]\n",
      "Iteration: 24900, Loss: 9.878616401692852e-05 for layer [ True]\n",
      "Step 11000 | Loss: 0.000453\n",
      "Step 11100 | Loss: 0.000453\n",
      "Step 11200 | Loss: 0.000452\n",
      "Step 11300 | Loss: 0.000452\n",
      "Step 11400 | Loss: 0.000451\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1941.829345703125 for layer [False]\n",
      "Iteration: 100, Loss: 318.88006591796875 for layer [ True]\n",
      "Iteration: 200, Loss: 1935.6798095703125 for layer [False]\n",
      "Iteration: 300, Loss: 1546.0810546875 for layer [False]\n",
      "Iteration: 400, Loss: 238.6944122314453 for layer [ True]\n",
      "Iteration: 500, Loss: 312.5274963378906 for layer [ True]\n",
      "Iteration: 600, Loss: 1445.8563232421875 for layer [False]\n",
      "Iteration: 700, Loss: 225.55259704589844 for layer [ True]\n",
      "Iteration: 800, Loss: 1162.072509765625 for layer [False]\n",
      "Iteration: 900, Loss: 1020.1904296875 for layer [False]\n",
      "Iteration: 1000, Loss: 149.70758056640625 for layer [ True]\n",
      "Iteration: 1100, Loss: 893.7606811523438 for layer [False]\n",
      "Iteration: 1200, Loss: 1060.84130859375 for layer [False]\n",
      "Iteration: 1300, Loss: 991.9182739257812 for layer [False]\n",
      "Iteration: 1400, Loss: 239.15325927734375 for layer [ True]\n",
      "Iteration: 1500, Loss: 305.1463623046875 for layer [ True]\n",
      "Iteration: 1600, Loss: 134.7360076904297 for layer [ True]\n",
      "Iteration: 1700, Loss: 120.71805572509766 for layer [ True]\n",
      "Iteration: 1800, Loss: 615.744140625 for layer [False]\n",
      "Iteration: 1900, Loss: 116.720458984375 for layer [ True]\n",
      "Iteration: 2000, Loss: 102.9753189086914 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.46270751953125 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.3671646118164 for layer [ True]\n",
      "Iteration: 2300, Loss: 454.0155029296875 for layer [False]\n",
      "Iteration: 2400, Loss: 126.67916870117188 for layer [ True]\n",
      "Iteration: 2500, Loss: 188.6535186767578 for layer [ True]\n",
      "Iteration: 2600, Loss: 120.10304260253906 for layer [ True]\n",
      "Iteration: 2700, Loss: 89.56267547607422 for layer [ True]\n",
      "Iteration: 2800, Loss: 327.36285400390625 for layer [False]\n",
      "Iteration: 2900, Loss: 253.12904357910156 for layer [False]\n",
      "Iteration: 3000, Loss: 80.67021179199219 for layer [ True]\n",
      "Iteration: 3100, Loss: 220.9359130859375 for layer [False]\n",
      "Iteration: 3200, Loss: 228.94024658203125 for layer [False]\n",
      "Iteration: 3300, Loss: 156.00074768066406 for layer [False]\n",
      "Iteration: 3400, Loss: 74.02800750732422 for layer [ True]\n",
      "Iteration: 3500, Loss: 188.3055419921875 for layer [False]\n",
      "Iteration: 3600, Loss: 59.868865966796875 for layer [ True]\n",
      "Iteration: 3700, Loss: 134.3485870361328 for layer [False]\n",
      "Iteration: 3800, Loss: 115.21810913085938 for layer [False]\n",
      "Iteration: 3900, Loss: 43.25252914428711 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.571834564208984 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.27383041381836 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.67472076416016 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.40241241455078 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.10462188720703 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.81675720214844 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.211814880371094 for layer [ True]\n",
      "Iteration: 4700, Loss: 62.21887969970703 for layer [False]\n",
      "Iteration: 4800, Loss: 40.09524917602539 for layer [False]\n",
      "Iteration: 4900, Loss: 50.96702194213867 for layer [False]\n",
      "Iteration: 5000, Loss: 55.878662109375 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.3022403717041 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.225914001464844 for layer [False]\n",
      "Iteration: 5300, Loss: 22.579580307006836 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.066301345825195 for layer [False]\n",
      "Iteration: 5500, Loss: 24.56017303466797 for layer [ True]\n",
      "Iteration: 5600, Loss: 29.014745712280273 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.335834503173828 for layer [False]\n",
      "Iteration: 5800, Loss: 21.889842987060547 for layer [False]\n",
      "Iteration: 5900, Loss: 13.904539108276367 for layer [False]\n",
      "Iteration: 6000, Loss: 13.718080520629883 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.369647979736328 for layer [False]\n",
      "Iteration: 6200, Loss: 17.293306350708008 for layer [False]\n",
      "Iteration: 6300, Loss: 14.334677696228027 for layer [False]\n",
      "Iteration: 6400, Loss: 16.012401580810547 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.548774719238281 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.617359161376953 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.40642738342285 for layer [False]\n",
      "Iteration: 6800, Loss: 4.934708595275879 for layer [ True]\n",
      "Iteration: 6900, Loss: 41.324886322021484 for layer [False]\n",
      "Iteration: 7000, Loss: 10.334054946899414 for layer [False]\n",
      "Iteration: 7100, Loss: 11.71851634979248 for layer [False]\n",
      "Iteration: 7200, Loss: 16.33728790283203 for layer [False]\n",
      "Iteration: 7300, Loss: 4.664290904998779 for layer [ True]\n",
      "Iteration: 7400, Loss: 15.53329086303711 for layer [False]\n",
      "Iteration: 7500, Loss: 4.454628944396973 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.223628520965576 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.860274314880371 for layer [False]\n",
      "Iteration: 7800, Loss: 9.583118438720703 for layer [False]\n",
      "Iteration: 7900, Loss: 5.5030317306518555 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8841936588287354 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.3421521186828613 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.8056108951568604 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.124342918395996 for layer [False]\n",
      "Iteration: 8400, Loss: 1.9885876178741455 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.130842208862305 for layer [False]\n",
      "Iteration: 8600, Loss: 7.040812969207764 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6955134868621826 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.320038795471191 for layer [False]\n",
      "Iteration: 8900, Loss: 9.208504676818848 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4851959943771362 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5792611837387085 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.871604323387146 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.136984825134277 for layer [False]\n",
      "Iteration: 9400, Loss: 7.37352991104126 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5217865705490112 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.843686819076538 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0674819946289062 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8880608677864075 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.1993231773376465 for layer [False]\n",
      "Iteration: 10000, Loss: 3.393954038619995 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2184978723526 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7506202459335327 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.485965728759766 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5411384105682373 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5601672530174255 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.292618751525879 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7094546556472778 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.10745096206665 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5218969583511353 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.142241477966309 for layer [False]\n",
      "Iteration: 11100, Loss: 2.918311595916748 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3550715446472168 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4400395452976227 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3476303517818451 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.774470806121826 for layer [False]\n",
      "Iteration: 11600, Loss: 0.36674708127975464 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19506339728832245 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2616473138332367 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2631308138370514 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11574704200029373 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.1861078292131424 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.16706030070781708 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10462915897369385 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.1477116197347641 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.421150207519531 for layer [False]\n",
      "Iteration: 12600, Loss: 5.476719856262207 for layer [False]\n",
      "Iteration: 12700, Loss: 4.387671947479248 for layer [False]\n",
      "Iteration: 12800, Loss: 0.09985136985778809 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09290663152933121 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06253179162740707 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.575326919555664 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06324609369039536 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.370816707611084 for layer [False]\n",
      "Iteration: 13400, Loss: 0.046089425683021545 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.05916621908545494 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04793890193104744 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.01570531539618969 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.037745945155620575 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.031216077506542206 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.385369300842285 for layer [False]\n",
      "Iteration: 14100, Loss: 0.02906712517142296 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.956928253173828 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010412937961518764 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.1739277839660645 for layer [False]\n",
      "Iteration: 14500, Loss: 2.196664571762085 for layer [False]\n",
      "Iteration: 14600, Loss: 3.1223409175872803 for layer [False]\n",
      "Iteration: 14700, Loss: 3.0829989910125732 for layer [False]\n",
      "Iteration: 14800, Loss: 3.652538776397705 for layer [False]\n",
      "Iteration: 14900, Loss: 6.242917537689209 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006487497594207525 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.35828971862793 for layer [False]\n",
      "Iteration: 15200, Loss: 8.942414283752441 for layer [False]\n",
      "Iteration: 15300, Loss: 4.3770599365234375 for layer [False]\n",
      "Iteration: 15400, Loss: 2.1661505699157715 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0034235073253512383 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004238307010382414 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004072766285389662 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.669543743133545 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002226288663223386 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0034543387591838837 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.0246434211730957 for layer [False]\n",
      "Iteration: 16200, Loss: 0.001193188363686204 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021045661997050047 for layer [ True]\n",
      "Iteration: 16400, Loss: 3.9701552391052246 for layer [False]\n",
      "Iteration: 16500, Loss: 4.540266513824463 for layer [False]\n",
      "Iteration: 16600, Loss: 3.2203142642974854 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015469574136659503 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007566360873170197 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009994027204811573 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.000893910531885922 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.2926571369171143 for layer [False]\n",
      "Iteration: 17200, Loss: 2.8498809337615967 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006824615993537009 for layer [ True]\n",
      "Iteration: 17400, Loss: 5.943002223968506 for layer [False]\n",
      "Iteration: 17500, Loss: 9.944626808166504 for layer [False]\n",
      "Iteration: 17600, Loss: 3.549233913421631 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008314530132338405 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.4055256843566895 for layer [False]\n",
      "Iteration: 17900, Loss: 3.8980724811553955 for layer [False]\n",
      "Iteration: 18000, Loss: 2.330909490585327 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007478808984160423 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0006112971459515393 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.710249900817871 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00041927568963728845 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007982177194207907 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.960654854774475 for layer [False]\n",
      "Iteration: 18700, Loss: 3.843949556350708 for layer [False]\n",
      "Iteration: 18800, Loss: 4.275513648986816 for layer [False]\n",
      "Iteration: 18900, Loss: 1.779123306274414 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00022041179181542248 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.1429338455200195 for layer [False]\n",
      "Iteration: 19200, Loss: 0.001551813562400639 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.447307825088501 for layer [False]\n",
      "Iteration: 19400, Loss: 5.885214328765869 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006342832348309457 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00038831550045870245 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.6674628257751465 for layer [False]\n",
      "Iteration: 19800, Loss: 3.3942196369171143 for layer [False]\n",
      "Iteration: 19900, Loss: 3.5730390548706055 for layer [False]\n",
      "Iteration: 20000, Loss: 2.3663980960845947 for layer [False]\n",
      "Iteration: 20100, Loss: 5.203906536102295 for layer [False]\n",
      "Iteration: 20200, Loss: 0.002059480408206582 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.8296594619750977 for layer [False]\n",
      "Iteration: 20400, Loss: 6.94962582201697e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003349612816236913 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009947940707206726 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.6652655601501465 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0002873108023777604 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.00109447306022048 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.844207763671875 for layer [False]\n",
      "Iteration: 21100, Loss: 8.24589729309082 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0007749533979222178 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.450492858886719 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0001514344330644235 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.301650285720825 for layer [False]\n",
      "Iteration: 21600, Loss: 4.1336798667907715 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00030575969140045345 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00030707885161973536 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.0734291076660156 for layer [False]\n",
      "Iteration: 22000, Loss: 3.933450937271118 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00035087004653178155 for layer [ True]\n",
      "Iteration: 22200, Loss: 8.268481178674847e-05 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.208906173706055 for layer [False]\n",
      "Iteration: 22400, Loss: 5.058082580566406 for layer [False]\n",
      "Iteration: 22500, Loss: 1.6741905212402344 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00017509562894701958 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00019216281361877918 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00014783516235183924 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.001419444102793932 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0004153104091528803 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0007082967204041779 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00022989572607912123 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00026545507716946304 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00022257480304688215 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0003730739699676633 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0001623580901650712 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.8994641304016113 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007945862016640604 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.000247031741309911 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0003049122169613838 for layer [ True]\n",
      "Iteration: 24100, Loss: 6.717160431435332e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.816701650619507 for layer [False]\n",
      "Iteration: 24300, Loss: 2.3831992149353027 for layer [False]\n",
      "Iteration: 24400, Loss: 3.996279716491699 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0002647507644724101 for layer [ True]\n",
      "Iteration: 24600, Loss: 7.80091795604676e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.011196613311768 for layer [False]\n",
      "Iteration: 24800, Loss: 3.869908094406128 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00020245563064236194 for layer [ True]\n",
      "Step 11500 | Loss: 0.000451\n",
      "Step 11600 | Loss: 0.000451\n",
      "Step 11700 | Loss: 0.000451\n",
      "Step 11800 | Loss: 0.000451\n",
      "Step 11900 | Loss: 0.000450\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1940.22265625 for layer [False]\n",
      "Iteration: 100, Loss: 318.5867919921875 for layer [ True]\n",
      "Iteration: 200, Loss: 1934.11181640625 for layer [False]\n",
      "Iteration: 300, Loss: 1543.7572021484375 for layer [False]\n",
      "Iteration: 400, Loss: 238.0918426513672 for layer [ True]\n",
      "Iteration: 500, Loss: 311.94451904296875 for layer [ True]\n",
      "Iteration: 600, Loss: 1444.160400390625 for layer [False]\n",
      "Iteration: 700, Loss: 224.52142333984375 for layer [ True]\n",
      "Iteration: 800, Loss: 1160.50634765625 for layer [False]\n",
      "Iteration: 900, Loss: 1019.671875 for layer [False]\n",
      "Iteration: 1000, Loss: 150.4765167236328 for layer [ True]\n",
      "Iteration: 1100, Loss: 892.3251953125 for layer [False]\n",
      "Iteration: 1200, Loss: 1059.167724609375 for layer [False]\n",
      "Iteration: 1300, Loss: 991.3580932617188 for layer [False]\n",
      "Iteration: 1400, Loss: 238.52932739257812 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.0523681640625 for layer [ True]\n",
      "Iteration: 1600, Loss: 134.7440185546875 for layer [ True]\n",
      "Iteration: 1700, Loss: 121.042236328125 for layer [ True]\n",
      "Iteration: 1800, Loss: 615.0181274414062 for layer [False]\n",
      "Iteration: 1900, Loss: 116.18977355957031 for layer [ True]\n",
      "Iteration: 2000, Loss: 102.92241668701172 for layer [ True]\n",
      "Iteration: 2100, Loss: 157.8915557861328 for layer [ True]\n",
      "Iteration: 2200, Loss: 107.86409759521484 for layer [ True]\n",
      "Iteration: 2300, Loss: 453.43701171875 for layer [False]\n",
      "Iteration: 2400, Loss: 126.61071014404297 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.2528533935547 for layer [ True]\n",
      "Iteration: 2600, Loss: 119.65873718261719 for layer [ True]\n",
      "Iteration: 2700, Loss: 89.46380615234375 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.96990966796875 for layer [False]\n",
      "Iteration: 2900, Loss: 252.729248046875 for layer [False]\n",
      "Iteration: 3000, Loss: 80.2998046875 for layer [ True]\n",
      "Iteration: 3100, Loss: 220.53671264648438 for layer [False]\n",
      "Iteration: 3200, Loss: 228.34567260742188 for layer [False]\n",
      "Iteration: 3300, Loss: 155.804931640625 for layer [False]\n",
      "Iteration: 3400, Loss: 73.2904052734375 for layer [ True]\n",
      "Iteration: 3500, Loss: 188.0144500732422 for layer [False]\n",
      "Iteration: 3600, Loss: 59.736663818359375 for layer [ True]\n",
      "Iteration: 3700, Loss: 133.83885192871094 for layer [False]\n",
      "Iteration: 3800, Loss: 115.05611419677734 for layer [False]\n",
      "Iteration: 3900, Loss: 42.960262298583984 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.217987060546875 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.728397369384766 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.05907440185547 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.07925033569336 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.80531692504883 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.26822280883789 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.82561492919922 for layer [ True]\n",
      "Iteration: 4700, Loss: 61.97117233276367 for layer [False]\n",
      "Iteration: 4800, Loss: 40.056663513183594 for layer [False]\n",
      "Iteration: 4900, Loss: 50.73976135253906 for layer [False]\n",
      "Iteration: 5000, Loss: 55.50300216674805 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.13311767578125 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.051353454589844 for layer [False]\n",
      "Iteration: 5300, Loss: 22.39616584777832 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.12162971496582 for layer [False]\n",
      "Iteration: 5500, Loss: 24.36092185974121 for layer [ True]\n",
      "Iteration: 5600, Loss: 28.991872787475586 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.351009368896484 for layer [False]\n",
      "Iteration: 5800, Loss: 21.885507583618164 for layer [False]\n",
      "Iteration: 5900, Loss: 13.950960159301758 for layer [False]\n",
      "Iteration: 6000, Loss: 13.62493896484375 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.56273651123047 for layer [False]\n",
      "Iteration: 6200, Loss: 17.429147720336914 for layer [False]\n",
      "Iteration: 6300, Loss: 14.52354621887207 for layer [False]\n",
      "Iteration: 6400, Loss: 15.977375030517578 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.508180618286133 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.565930366516113 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.526111602783203 for layer [False]\n",
      "Iteration: 6800, Loss: 4.900060176849365 for layer [ True]\n",
      "Iteration: 6900, Loss: 41.761680603027344 for layer [False]\n",
      "Iteration: 7000, Loss: 10.419291496276855 for layer [False]\n",
      "Iteration: 7100, Loss: 11.82271957397461 for layer [False]\n",
      "Iteration: 7200, Loss: 16.644840240478516 for layer [False]\n",
      "Iteration: 7300, Loss: 4.644068717956543 for layer [ True]\n",
      "Iteration: 7400, Loss: 15.780885696411133 for layer [False]\n",
      "Iteration: 7500, Loss: 4.427745342254639 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.184492111206055 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.911531448364258 for layer [False]\n",
      "Iteration: 7800, Loss: 9.647815704345703 for layer [False]\n",
      "Iteration: 7900, Loss: 5.473381042480469 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8494677543640137 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.3222062587738037 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.7950565814971924 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.284996032714844 for layer [False]\n",
      "Iteration: 8400, Loss: 1.989711880683899 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.252785682678223 for layer [False]\n",
      "Iteration: 8600, Loss: 7.199459075927734 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6769769191741943 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.340949535369873 for layer [False]\n",
      "Iteration: 8900, Loss: 9.334172248840332 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4808428287506104 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5692944526672363 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8617017269134521 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.164730548858643 for layer [False]\n",
      "Iteration: 9400, Loss: 7.41139554977417 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5112942457199097 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.8685362339019775 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0640918016433716 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8820513486862183 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.230349540710449 for layer [False]\n",
      "Iteration: 10000, Loss: 3.427546262741089 for layer [False]\n",
      "Iteration: 10100, Loss: 1.210008978843689 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7429015040397644 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.553125381469727 for layer [False]\n",
      "Iteration: 10400, Loss: 0.540163516998291 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5582748055458069 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.331754684448242 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7097622752189636 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.1236724853515625 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5186893939971924 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.32489013671875 for layer [False]\n",
      "Iteration: 11100, Loss: 2.9865939617156982 for layer [False]\n",
      "Iteration: 11200, Loss: 0.35359591245651245 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.43886691331863403 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.34598392248153687 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.819462299346924 for layer [False]\n",
      "Iteration: 11600, Loss: 0.36531588435173035 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.1945599466562271 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.26014548540115356 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2634982168674469 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11588329076766968 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18561173975467682 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.16713468730449677 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10433868318796158 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.148030623793602 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.49095344543457 for layer [False]\n",
      "Iteration: 12600, Loss: 5.518843173980713 for layer [False]\n",
      "Iteration: 12700, Loss: 4.412507057189941 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10048459470272064 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09308089315891266 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.062432095408439636 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.636512756347656 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06348711252212524 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.410563945770264 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04640254005789757 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.059369686990976334 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04785812273621559 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.015769148245453835 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03744843602180481 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.031132671982049942 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.440866708755493 for layer [False]\n",
      "Iteration: 14100, Loss: 0.028837300837039948 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.95823335647583 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010424911975860596 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.2031545639038086 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2003366947174072 for layer [False]\n",
      "Iteration: 14600, Loss: 3.181983470916748 for layer [False]\n",
      "Iteration: 14700, Loss: 3.1161062717437744 for layer [False]\n",
      "Iteration: 14800, Loss: 3.684727430343628 for layer [False]\n",
      "Iteration: 14900, Loss: 6.334506034851074 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006483597215265036 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.355395793914795 for layer [False]\n",
      "Iteration: 15200, Loss: 9.0472412109375 for layer [False]\n",
      "Iteration: 15300, Loss: 4.429541110992432 for layer [False]\n",
      "Iteration: 15400, Loss: 2.1769399642944336 for layer [False]\n",
      "Iteration: 15500, Loss: 0.003384736366569996 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.0042059835977852345 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004062758293002844 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.6913809776306152 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002217274159193039 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0034469454549252987 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.0759572982788086 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011852086754515767 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020799203775823116 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.072894096374512 for layer [False]\n",
      "Iteration: 16500, Loss: 4.548099994659424 for layer [False]\n",
      "Iteration: 16600, Loss: 3.2624425888061523 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015108478255569935 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007679125410504639 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009787975577637553 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009001846192404628 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.320870876312256 for layer [False]\n",
      "Iteration: 17200, Loss: 2.8991506099700928 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006731200264766812 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.076797962188721 for layer [False]\n",
      "Iteration: 17500, Loss: 10.107715606689453 for layer [False]\n",
      "Iteration: 17600, Loss: 3.5826964378356934 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008257340523414314 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.4497299194335938 for layer [False]\n",
      "Iteration: 17900, Loss: 3.964611053466797 for layer [False]\n",
      "Iteration: 18000, Loss: 2.3630330562591553 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007427078089676797 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.000614142685662955 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.763195037841797 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00040741509292274714 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007747579365968704 for layer [ True]\n",
      "Iteration: 18600, Loss: 1.9835096597671509 for layer [False]\n",
      "Iteration: 18700, Loss: 3.891022205352783 for layer [False]\n",
      "Iteration: 18800, Loss: 4.35093879699707 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8022276163101196 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00021043540618848056 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.188624143600464 for layer [False]\n",
      "Iteration: 19200, Loss: 0.001667450531385839 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.4780893325805664 for layer [False]\n",
      "Iteration: 19400, Loss: 5.977491855621338 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006233202293515205 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00038843858055770397 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.710086345672607 for layer [False]\n",
      "Iteration: 19800, Loss: 3.424771308898926 for layer [False]\n",
      "Iteration: 19900, Loss: 3.6307244300842285 for layer [False]\n",
      "Iteration: 20000, Loss: 2.411417007446289 for layer [False]\n",
      "Iteration: 20100, Loss: 5.248607635498047 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0020147645846009254 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.8694472312927246 for layer [False]\n",
      "Iteration: 20400, Loss: 6.9342517235782e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00033816893119364977 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009497321443632245 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.6976840496063232 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00031322348513640463 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0016627456061542034 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.8664519786834717 for layer [False]\n",
      "Iteration: 21100, Loss: 8.341574668884277 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0008024665294215083 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.486852169036865 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00015193915169220418 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.3291380405426025 for layer [False]\n",
      "Iteration: 21600, Loss: 4.209558010101318 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00032344262581318617 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00041387867531739175 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.100586414337158 for layer [False]\n",
      "Iteration: 22000, Loss: 3.937201976776123 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0001445959642296657 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0002286559174535796 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.208643913269043 for layer [False]\n",
      "Iteration: 22400, Loss: 5.1596174240112305 for layer [False]\n",
      "Iteration: 22500, Loss: 1.7166565656661987 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0001391319092363119 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00019434835121501237 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00014048931188881397 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0010509806452319026 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0007064505480229855 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.000493632978759706 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00026911654276773334 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0003174923767801374 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002328874106751755 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0004250133642926812 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00021643677609972656 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.921966075897217 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007451284327544272 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00022179883671924472 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0003300309763289988 for layer [ True]\n",
      "Iteration: 24100, Loss: 7.684786396566778e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.8641440868377686 for layer [False]\n",
      "Iteration: 24300, Loss: 2.4103052616119385 for layer [False]\n",
      "Iteration: 24400, Loss: 4.060088634490967 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0001276902185054496 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00013365411723498255 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.121573448181152 for layer [False]\n",
      "Iteration: 24800, Loss: 3.89632248878479 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00014247251965571195 for layer [ True]\n",
      "Step 12000 | Loss: 0.000450\n",
      "Step 12100 | Loss: 0.000450\n",
      "Step 12200 | Loss: 0.000450\n",
      "Step 12300 | Loss: 0.000450\n",
      "Step 12400 | Loss: 0.000450\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1938.21337890625 for layer [False]\n",
      "Iteration: 100, Loss: 318.65179443359375 for layer [ True]\n",
      "Iteration: 200, Loss: 1932.6429443359375 for layer [False]\n",
      "Iteration: 300, Loss: 1542.2730712890625 for layer [False]\n",
      "Iteration: 400, Loss: 238.44216918945312 for layer [ True]\n",
      "Iteration: 500, Loss: 313.3814697265625 for layer [ True]\n",
      "Iteration: 600, Loss: 1442.917724609375 for layer [False]\n",
      "Iteration: 700, Loss: 224.21197509765625 for layer [ True]\n",
      "Iteration: 800, Loss: 1158.6053466796875 for layer [False]\n",
      "Iteration: 900, Loss: 1018.2023315429688 for layer [False]\n",
      "Iteration: 1000, Loss: 151.99114990234375 for layer [ True]\n",
      "Iteration: 1100, Loss: 891.8204345703125 for layer [False]\n",
      "Iteration: 1200, Loss: 1057.082275390625 for layer [False]\n",
      "Iteration: 1300, Loss: 990.9108276367188 for layer [False]\n",
      "Iteration: 1400, Loss: 239.99676513671875 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.8673400878906 for layer [ True]\n",
      "Iteration: 1600, Loss: 135.49522399902344 for layer [ True]\n",
      "Iteration: 1700, Loss: 121.82809448242188 for layer [ True]\n",
      "Iteration: 1800, Loss: 614.272705078125 for layer [False]\n",
      "Iteration: 1900, Loss: 116.21697235107422 for layer [ True]\n",
      "Iteration: 2000, Loss: 103.1778564453125 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.0078582763672 for layer [ True]\n",
      "Iteration: 2200, Loss: 107.75687408447266 for layer [ True]\n",
      "Iteration: 2300, Loss: 453.0562438964844 for layer [False]\n",
      "Iteration: 2400, Loss: 127.282958984375 for layer [ True]\n",
      "Iteration: 2500, Loss: 186.76466369628906 for layer [ True]\n",
      "Iteration: 2600, Loss: 119.9609603881836 for layer [ True]\n",
      "Iteration: 2700, Loss: 89.8342056274414 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.7166442871094 for layer [False]\n",
      "Iteration: 2900, Loss: 252.3597869873047 for layer [False]\n",
      "Iteration: 3000, Loss: 80.14984893798828 for layer [ True]\n",
      "Iteration: 3100, Loss: 220.13031005859375 for layer [False]\n",
      "Iteration: 3200, Loss: 228.04638671875 for layer [False]\n",
      "Iteration: 3300, Loss: 155.57325744628906 for layer [False]\n",
      "Iteration: 3400, Loss: 72.80691528320312 for layer [ True]\n",
      "Iteration: 3500, Loss: 187.64120483398438 for layer [False]\n",
      "Iteration: 3600, Loss: 59.84773635864258 for layer [ True]\n",
      "Iteration: 3700, Loss: 133.4768524169922 for layer [False]\n",
      "Iteration: 3800, Loss: 114.66200256347656 for layer [False]\n",
      "Iteration: 3900, Loss: 42.95509719848633 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.207801818847656 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.62968063354492 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.04887390136719 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.16737365722656 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.837894439697266 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.19715881347656 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.604984283447266 for layer [ True]\n",
      "Iteration: 4700, Loss: 61.83076095581055 for layer [False]\n",
      "Iteration: 4800, Loss: 39.91826248168945 for layer [False]\n",
      "Iteration: 4900, Loss: 50.5991096496582 for layer [False]\n",
      "Iteration: 5000, Loss: 55.53406524658203 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.211835861206055 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.99947738647461 for layer [False]\n",
      "Iteration: 5300, Loss: 22.455541610717773 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.09034538269043 for layer [False]\n",
      "Iteration: 5500, Loss: 24.379608154296875 for layer [ True]\n",
      "Iteration: 5600, Loss: 29.130428314208984 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.341522216796875 for layer [False]\n",
      "Iteration: 5800, Loss: 21.950300216674805 for layer [False]\n",
      "Iteration: 5900, Loss: 14.015899658203125 for layer [False]\n",
      "Iteration: 6000, Loss: 13.603742599487305 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.41834831237793 for layer [False]\n",
      "Iteration: 6200, Loss: 17.658777236938477 for layer [False]\n",
      "Iteration: 6300, Loss: 14.543124198913574 for layer [False]\n",
      "Iteration: 6400, Loss: 16.063947677612305 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.495866775512695 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.58956527709961 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.431188583374023 for layer [False]\n",
      "Iteration: 6800, Loss: 4.913580417633057 for layer [ True]\n",
      "Iteration: 6900, Loss: 42.10517883300781 for layer [False]\n",
      "Iteration: 7000, Loss: 10.437310218811035 for layer [False]\n",
      "Iteration: 7100, Loss: 11.86130428314209 for layer [False]\n",
      "Iteration: 7200, Loss: 16.87386703491211 for layer [False]\n",
      "Iteration: 7300, Loss: 4.653804302215576 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.091535568237305 for layer [False]\n",
      "Iteration: 7500, Loss: 4.42202615737915 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.181961536407471 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.98344898223877 for layer [False]\n",
      "Iteration: 7800, Loss: 9.709012031555176 for layer [False]\n",
      "Iteration: 7900, Loss: 5.467780590057373 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.836014986038208 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.3196752071380615 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.802616834640503 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.284463882446289 for layer [False]\n",
      "Iteration: 8400, Loss: 1.9894928932189941 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.342976570129395 for layer [False]\n",
      "Iteration: 8600, Loss: 7.328343868255615 for layer [False]\n",
      "Iteration: 8700, Loss: 2.678656578063965 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.367244243621826 for layer [False]\n",
      "Iteration: 8900, Loss: 9.34119701385498 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4761407375335693 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5666074752807617 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8659956455230713 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.187488079071045 for layer [False]\n",
      "Iteration: 9400, Loss: 7.567758083343506 for layer [False]\n",
      "Iteration: 9500, Loss: 1.512196660041809 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.849095582962036 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0547289848327637 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8841402530670166 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.2552714347839355 for layer [False]\n",
      "Iteration: 10000, Loss: 3.4296810626983643 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2089157104492188 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7390749454498291 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.582678318023682 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5381503105163574 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5594483613967896 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.3868467807769775 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7107877731323242 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.134488582611084 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5192171335220337 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.378722190856934 for layer [False]\n",
      "Iteration: 11100, Loss: 2.991605043411255 for layer [False]\n",
      "Iteration: 11200, Loss: 0.35390013456344604 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4400951564311981 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3479633629322052 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.849294900894165 for layer [False]\n",
      "Iteration: 11600, Loss: 0.36680468916893005 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19492316246032715 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2618257701396942 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2661120593547821 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1168748065829277 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.1867240071296692 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.16838476061820984 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10452377796173096 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.1496323198080063 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.547900676727295 for layer [False]\n",
      "Iteration: 12600, Loss: 5.52920389175415 for layer [False]\n",
      "Iteration: 12700, Loss: 4.447844505310059 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10222592949867249 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09372058510780334 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06310442090034485 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.680957794189453 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06418491899967194 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.474238872528076 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04685525223612785 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06011643260717392 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.048290278762578964 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.015950728207826614 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.037593863904476166 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03128043934702873 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.4652817249298096 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029047321528196335 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.986515045166016 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010608579032123089 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.2420504093170166 for layer [False]\n",
      "Iteration: 14500, Loss: 2.221137762069702 for layer [False]\n",
      "Iteration: 14600, Loss: 3.165564775466919 for layer [False]\n",
      "Iteration: 14700, Loss: 3.14353084564209 for layer [False]\n",
      "Iteration: 14800, Loss: 3.676617383956909 for layer [False]\n",
      "Iteration: 14900, Loss: 6.2635393142700195 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006600980646908283 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.354766845703125 for layer [False]\n",
      "Iteration: 15200, Loss: 9.148153305053711 for layer [False]\n",
      "Iteration: 15300, Loss: 4.481084823608398 for layer [False]\n",
      "Iteration: 15400, Loss: 2.192574977874756 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033787903375923634 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004301948938518763 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004123843740671873 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.7344064712524414 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002236762549728155 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0034896808210760355 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.0546391010284424 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011948117753490806 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020944091957062483 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.078003883361816 for layer [False]\n",
      "Iteration: 16500, Loss: 4.5853753089904785 for layer [False]\n",
      "Iteration: 16600, Loss: 3.2996652126312256 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0015052437083795667 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.000779140624217689 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009846099419519305 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009256479679606855 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.3366034030914307 for layer [False]\n",
      "Iteration: 17200, Loss: 2.976972818374634 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006781236152164638 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.113309383392334 for layer [False]\n",
      "Iteration: 17500, Loss: 10.267233848571777 for layer [False]\n",
      "Iteration: 17600, Loss: 3.5495707988739014 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008660735911689699 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.4631435871124268 for layer [False]\n",
      "Iteration: 17900, Loss: 3.981346845626831 for layer [False]\n",
      "Iteration: 18000, Loss: 2.383943796157837 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007764027686789632 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0006814549560658634 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.826684474945068 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00039772092713974416 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007876923773437738 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.008298635482788 for layer [False]\n",
      "Iteration: 18700, Loss: 3.9409916400909424 for layer [False]\n",
      "Iteration: 18800, Loss: 4.401376247406006 for layer [False]\n",
      "Iteration: 18900, Loss: 1.7941718101501465 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00021742690296377987 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.202831983566284 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0016720738494768739 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.497302532196045 for layer [False]\n",
      "Iteration: 19400, Loss: 6.046590328216553 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006599733605980873 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00038139900425449014 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.737030506134033 for layer [False]\n",
      "Iteration: 19800, Loss: 3.3916943073272705 for layer [False]\n",
      "Iteration: 19900, Loss: 3.6344785690307617 for layer [False]\n",
      "Iteration: 20000, Loss: 2.409679889678955 for layer [False]\n",
      "Iteration: 20100, Loss: 5.205392360687256 for layer [False]\n",
      "Iteration: 20200, Loss: 0.001957600237801671 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.8732101917266846 for layer [False]\n",
      "Iteration: 20400, Loss: 7.076265319483355e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003436361439526081 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009051902452483773 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.7040748596191406 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0004243127186782658 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0017061117105185986 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.8945519924163818 for layer [False]\n",
      "Iteration: 21100, Loss: 8.378969192504883 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0007503185188397765 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.506974220275879 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00015280579100362957 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.33390212059021 for layer [False]\n",
      "Iteration: 21600, Loss: 4.218563556671143 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003166251117363572 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0004272832302376628 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.1093640327453613 for layer [False]\n",
      "Iteration: 22000, Loss: 3.939640760421753 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00014340846973937005 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00020003138342872262 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.2442474365234375 for layer [False]\n",
      "Iteration: 22400, Loss: 5.161209583282471 for layer [False]\n",
      "Iteration: 22500, Loss: 1.7228093147277832 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00017054553609341383 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00018519244622439146 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0001390519755659625 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0011270820396021008 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0006772694760002196 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00057505740551278 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00024567008949816227 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00031425675842911005 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00022812280803918839 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0004197980451863259 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0002129467757185921 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.957597017288208 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007549678557552397 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00023473270994145423 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0003300244570709765 for layer [ True]\n",
      "Iteration: 24100, Loss: 7.051372085697949e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.8861939907073975 for layer [False]\n",
      "Iteration: 24300, Loss: 2.4413881301879883 for layer [False]\n",
      "Iteration: 24400, Loss: 4.122614860534668 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00021808889869134873 for layer [ True]\n",
      "Iteration: 24600, Loss: 5.093755680718459e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.176164627075195 for layer [False]\n",
      "Iteration: 24800, Loss: 3.90674090385437 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00035318071604706347 for layer [ True]\n",
      "Step 12500 | Loss: 0.000450\n",
      "Step 12600 | Loss: 0.000449\n",
      "Step 12700 | Loss: 0.000449\n",
      "Step 12800 | Loss: 0.000449\n",
      "Step 12900 | Loss: 0.000449\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1938.2713623046875 for layer [False]\n",
      "Iteration: 100, Loss: 319.154296875 for layer [ True]\n",
      "Iteration: 200, Loss: 1931.521728515625 for layer [False]\n",
      "Iteration: 300, Loss: 1541.4869384765625 for layer [False]\n",
      "Iteration: 400, Loss: 238.63194274902344 for layer [ True]\n",
      "Iteration: 500, Loss: 314.04632568359375 for layer [ True]\n",
      "Iteration: 600, Loss: 1442.213134765625 for layer [False]\n",
      "Iteration: 700, Loss: 224.59976196289062 for layer [ True]\n",
      "Iteration: 800, Loss: 1158.5926513671875 for layer [False]\n",
      "Iteration: 900, Loss: 1019.0299682617188 for layer [False]\n",
      "Iteration: 1000, Loss: 152.587646484375 for layer [ True]\n",
      "Iteration: 1100, Loss: 890.8345336914062 for layer [False]\n",
      "Iteration: 1200, Loss: 1056.74072265625 for layer [False]\n",
      "Iteration: 1300, Loss: 989.7405395507812 for layer [False]\n",
      "Iteration: 1400, Loss: 242.22378540039062 for layer [ True]\n",
      "Iteration: 1500, Loss: 306.89459228515625 for layer [ True]\n",
      "Iteration: 1600, Loss: 136.35073852539062 for layer [ True]\n",
      "Iteration: 1700, Loss: 123.17891693115234 for layer [ True]\n",
      "Iteration: 1800, Loss: 613.4569091796875 for layer [False]\n",
      "Iteration: 1900, Loss: 117.31644439697266 for layer [ True]\n",
      "Iteration: 2000, Loss: 104.4619140625 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.757080078125 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.54031372070312 for layer [ True]\n",
      "Iteration: 2300, Loss: 452.79681396484375 for layer [False]\n",
      "Iteration: 2400, Loss: 128.8592529296875 for layer [ True]\n",
      "Iteration: 2500, Loss: 188.7816162109375 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.07041931152344 for layer [ True]\n",
      "Iteration: 2700, Loss: 90.6951675415039 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.5310363769531 for layer [False]\n",
      "Iteration: 2900, Loss: 252.13037109375 for layer [False]\n",
      "Iteration: 3000, Loss: 80.6697998046875 for layer [ True]\n",
      "Iteration: 3100, Loss: 220.1317901611328 for layer [False]\n",
      "Iteration: 3200, Loss: 227.840576171875 for layer [False]\n",
      "Iteration: 3300, Loss: 155.64859008789062 for layer [False]\n",
      "Iteration: 3400, Loss: 73.29447937011719 for layer [ True]\n",
      "Iteration: 3500, Loss: 187.46438598632812 for layer [False]\n",
      "Iteration: 3600, Loss: 60.65196228027344 for layer [ True]\n",
      "Iteration: 3700, Loss: 133.36257934570312 for layer [False]\n",
      "Iteration: 3800, Loss: 114.71112823486328 for layer [False]\n",
      "Iteration: 3900, Loss: 43.403812408447266 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.77152633666992 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.04789733886719 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.74591064453125 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.48659896850586 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.16518020629883 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.55738830566406 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.125675201416016 for layer [ True]\n",
      "Iteration: 4700, Loss: 61.87749481201172 for layer [False]\n",
      "Iteration: 4800, Loss: 39.84844970703125 for layer [False]\n",
      "Iteration: 4900, Loss: 50.46918869018555 for layer [False]\n",
      "Iteration: 5000, Loss: 56.26560974121094 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.487777709960938 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.98664474487305 for layer [False]\n",
      "Iteration: 5300, Loss: 22.71034812927246 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.103097915649414 for layer [False]\n",
      "Iteration: 5500, Loss: 24.679960250854492 for layer [ True]\n",
      "Iteration: 5600, Loss: 29.73219108581543 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.408615112304688 for layer [False]\n",
      "Iteration: 5800, Loss: 21.926509857177734 for layer [False]\n",
      "Iteration: 5900, Loss: 13.974165916442871 for layer [False]\n",
      "Iteration: 6000, Loss: 13.798650741577148 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.508338928222656 for layer [False]\n",
      "Iteration: 6200, Loss: 17.603822708129883 for layer [False]\n",
      "Iteration: 6300, Loss: 14.796174049377441 for layer [False]\n",
      "Iteration: 6400, Loss: 16.297771453857422 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.61677074432373 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.72647762298584 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.2357234954834 for layer [False]\n",
      "Iteration: 6800, Loss: 5.003807067871094 for layer [ True]\n",
      "Iteration: 6900, Loss: 42.56769561767578 for layer [False]\n",
      "Iteration: 7000, Loss: 10.560287475585938 for layer [False]\n",
      "Iteration: 7100, Loss: 11.928441047668457 for layer [False]\n",
      "Iteration: 7200, Loss: 16.936511993408203 for layer [False]\n",
      "Iteration: 7300, Loss: 4.7086052894592285 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.12827491760254 for layer [False]\n",
      "Iteration: 7500, Loss: 4.479763984680176 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.245171546936035 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.021472930908203 for layer [False]\n",
      "Iteration: 7800, Loss: 9.677351951599121 for layer [False]\n",
      "Iteration: 7900, Loss: 5.532352924346924 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.871955156326294 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.3485822677612305 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.846883773803711 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.481019973754883 for layer [False]\n",
      "Iteration: 8400, Loss: 2.0212271213531494 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.391292572021484 for layer [False]\n",
      "Iteration: 8600, Loss: 7.415346622467041 for layer [False]\n",
      "Iteration: 8700, Loss: 2.70750093460083 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.338019847869873 for layer [False]\n",
      "Iteration: 8900, Loss: 9.315141677856445 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4980204105377197 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5863758325576782 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8870794773101807 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.252628803253174 for layer [False]\n",
      "Iteration: 9400, Loss: 7.517019271850586 for layer [False]\n",
      "Iteration: 9500, Loss: 1.532849907875061 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.8889644145965576 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0650465488433838 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8944565653800964 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.354569911956787 for layer [False]\n",
      "Iteration: 10000, Loss: 3.49613881111145 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2236053943634033 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7491239905357361 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.541952610015869 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5440812706947327 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5720126032829285 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.398096799850464 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7212927341461182 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.1565260887146 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5284082889556885 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.489307403564453 for layer [False]\n",
      "Iteration: 11100, Loss: 2.9917004108428955 for layer [False]\n",
      "Iteration: 11200, Loss: 0.36008214950561523 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4476380944252014 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.35260429978370667 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.844633102416992 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3740446865558624 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19823916256427765 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.26540499925613403 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.27147942781448364 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11915381252765656 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.1898571401834488 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.17144803702831268 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10585033893585205 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15385644137859344 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.603304386138916 for layer [False]\n",
      "Iteration: 12600, Loss: 5.5485453605651855 for layer [False]\n",
      "Iteration: 12700, Loss: 4.495920658111572 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10415132343769073 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09523124247789383 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06411344558000565 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.768054485321045 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06545955687761307 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.480788230895996 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04764989763498306 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06152195855975151 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.048781778663396835 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016339393332600594 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.0380464568734169 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03161313384771347 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.4980571269989014 for layer [False]\n",
      "Iteration: 14100, Loss: 0.0294357817620039 for layer [ True]\n",
      "Iteration: 14200, Loss: 4.944561958312988 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010779879987239838 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.228854179382324 for layer [False]\n",
      "Iteration: 14500, Loss: 2.203172206878662 for layer [False]\n",
      "Iteration: 14600, Loss: 3.1982641220092773 for layer [False]\n",
      "Iteration: 14700, Loss: 3.126688241958618 for layer [False]\n",
      "Iteration: 14800, Loss: 3.7002415657043457 for layer [False]\n",
      "Iteration: 14900, Loss: 6.371789455413818 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0067300740629434586 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.399061679840088 for layer [False]\n",
      "Iteration: 15200, Loss: 9.251561164855957 for layer [False]\n",
      "Iteration: 15300, Loss: 4.489072322845459 for layer [False]\n",
      "Iteration: 15400, Loss: 2.1967780590057373 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033964249305427074 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004367870278656483 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004215544555336237 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.7650909423828125 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0022820117883384228 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003525451058521867 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.109628438949585 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011984508018940687 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021244564559310675 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.174032688140869 for layer [False]\n",
      "Iteration: 16500, Loss: 4.622562885284424 for layer [False]\n",
      "Iteration: 16600, Loss: 3.3136019706726074 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014927404699847102 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008171690860763192 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010031649144366384 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009359182440675795 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.3340132236480713 for layer [False]\n",
      "Iteration: 17200, Loss: 2.9441049098968506 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006801062845624983 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.136229038238525 for layer [False]\n",
      "Iteration: 17500, Loss: 10.320050239562988 for layer [False]\n",
      "Iteration: 17600, Loss: 3.5782582759857178 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0009018928976729512 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.4864048957824707 for layer [False]\n",
      "Iteration: 17900, Loss: 4.0606818199157715 for layer [False]\n",
      "Iteration: 18000, Loss: 2.39996075630188 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007577914511784911 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.000815995445009321 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.8573455810546875 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00038140270044095814 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008173896931111813 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.029480457305908 for layer [False]\n",
      "Iteration: 18700, Loss: 3.9706106185913086 for layer [False]\n",
      "Iteration: 18800, Loss: 4.450632095336914 for layer [False]\n",
      "Iteration: 18900, Loss: 1.7921706438064575 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00021126048523001373 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.200437307357788 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0015251986915245652 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.489971160888672 for layer [False]\n",
      "Iteration: 19400, Loss: 6.10750150680542 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007180566899478436 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00035856972681358457 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.802260398864746 for layer [False]\n",
      "Iteration: 19800, Loss: 3.4066851139068604 for layer [False]\n",
      "Iteration: 19900, Loss: 3.6642816066741943 for layer [False]\n",
      "Iteration: 20000, Loss: 2.422114849090576 for layer [False]\n",
      "Iteration: 20100, Loss: 5.215210437774658 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0017751676496118307 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.932025909423828 for layer [False]\n",
      "Iteration: 20400, Loss: 6.562843191204593e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003714275371748954 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0008829968282952905 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.7391104698181152 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00037493676063604653 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0017075970536097884 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.8980491161346436 for layer [False]\n",
      "Iteration: 21100, Loss: 8.46828842163086 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0006707145366817713 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.530535697937012 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00015456517576240003 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.3856499195098877 for layer [False]\n",
      "Iteration: 21600, Loss: 4.315524578094482 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003295915084891021 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0004216974775772542 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.1029226779937744 for layer [False]\n",
      "Iteration: 22000, Loss: 3.904087543487549 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00014070766337681562 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0001838181633502245 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.174090385437012 for layer [False]\n",
      "Iteration: 22400, Loss: 5.228426456451416 for layer [False]\n",
      "Iteration: 22500, Loss: 1.7317014932632446 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00024130790552590042 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00017089715402107686 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00013686773309018463 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0014139021513983607 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00048699721810407937 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0006924235494807363 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00022130203433334827 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00028998564812354743 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00024260542704723775 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.000388447631848976 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00018952945538330823 for layer [ True]\n",
      "Iteration: 23700, Loss: 2.9805514812469482 for layer [False]\n",
      "Iteration: 23800, Loss: 0.000761692353989929 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00025940220803022385 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0003204238601028919 for layer [ True]\n",
      "Iteration: 24100, Loss: 4.926833207719028e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.9127485752105713 for layer [False]\n",
      "Iteration: 24300, Loss: 2.455078601837158 for layer [False]\n",
      "Iteration: 24400, Loss: 4.0616230964660645 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0006075738929212093 for layer [ True]\n",
      "Iteration: 24600, Loss: 4.0651757444720715e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.219646453857422 for layer [False]\n",
      "Iteration: 24800, Loss: 3.9052419662475586 for layer [False]\n",
      "Iteration: 24900, Loss: 6.121467595221475e-05 for layer [ True]\n",
      "Step 13000 | Loss: 0.000449\n",
      "Step 13100 | Loss: 0.000449\n",
      "Step 13200 | Loss: 0.000449\n",
      "Step 13300 | Loss: 0.000448\n",
      "Step 13400 | Loss: 0.000449\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1936.3890380859375 for layer [False]\n",
      "Iteration: 100, Loss: 320.1609191894531 for layer [ True]\n",
      "Iteration: 200, Loss: 1931.82275390625 for layer [False]\n",
      "Iteration: 300, Loss: 1541.7672119140625 for layer [False]\n",
      "Iteration: 400, Loss: 238.6763153076172 for layer [ True]\n",
      "Iteration: 500, Loss: 314.73272705078125 for layer [ True]\n",
      "Iteration: 600, Loss: 1442.3883056640625 for layer [False]\n",
      "Iteration: 700, Loss: 224.93421936035156 for layer [ True]\n",
      "Iteration: 800, Loss: 1156.5421142578125 for layer [False]\n",
      "Iteration: 900, Loss: 1017.6170654296875 for layer [False]\n",
      "Iteration: 1000, Loss: 152.31300354003906 for layer [ True]\n",
      "Iteration: 1100, Loss: 890.57568359375 for layer [False]\n",
      "Iteration: 1200, Loss: 1057.4033203125 for layer [False]\n",
      "Iteration: 1300, Loss: 989.393798828125 for layer [False]\n",
      "Iteration: 1400, Loss: 242.50209045410156 for layer [ True]\n",
      "Iteration: 1500, Loss: 305.8526306152344 for layer [ True]\n",
      "Iteration: 1600, Loss: 136.38046264648438 for layer [ True]\n",
      "Iteration: 1700, Loss: 123.63320922851562 for layer [ True]\n",
      "Iteration: 1800, Loss: 613.44140625 for layer [False]\n",
      "Iteration: 1900, Loss: 117.3250732421875 for layer [ True]\n",
      "Iteration: 2000, Loss: 104.7034912109375 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.496337890625 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.44112396240234 for layer [ True]\n",
      "Iteration: 2300, Loss: 452.6017761230469 for layer [False]\n",
      "Iteration: 2400, Loss: 128.81398010253906 for layer [ True]\n",
      "Iteration: 2500, Loss: 188.71910095214844 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.1502914428711 for layer [ True]\n",
      "Iteration: 2700, Loss: 90.6572036743164 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.8126525878906 for layer [False]\n",
      "Iteration: 2900, Loss: 251.88694763183594 for layer [False]\n",
      "Iteration: 3000, Loss: 80.61664581298828 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.9883575439453 for layer [False]\n",
      "Iteration: 3200, Loss: 227.63775634765625 for layer [False]\n",
      "Iteration: 3300, Loss: 155.62545776367188 for layer [False]\n",
      "Iteration: 3400, Loss: 73.22252655029297 for layer [ True]\n",
      "Iteration: 3500, Loss: 187.3427734375 for layer [False]\n",
      "Iteration: 3600, Loss: 60.56010437011719 for layer [ True]\n",
      "Iteration: 3700, Loss: 132.95663452148438 for layer [False]\n",
      "Iteration: 3800, Loss: 114.47250366210938 for layer [False]\n",
      "Iteration: 3900, Loss: 43.50339889526367 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.818031311035156 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.15448760986328 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.87120056152344 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.4520149230957 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.204891204833984 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.65183639526367 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.229881286621094 for layer [ True]\n",
      "Iteration: 4700, Loss: 61.57112503051758 for layer [False]\n",
      "Iteration: 4800, Loss: 39.85779571533203 for layer [False]\n",
      "Iteration: 4900, Loss: 50.36799621582031 for layer [False]\n",
      "Iteration: 5000, Loss: 56.61334228515625 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.59035873413086 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.07126235961914 for layer [False]\n",
      "Iteration: 5300, Loss: 22.80467414855957 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.13060188293457 for layer [False]\n",
      "Iteration: 5500, Loss: 24.802490234375 for layer [ True]\n",
      "Iteration: 5600, Loss: 29.935102462768555 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.222063064575195 for layer [False]\n",
      "Iteration: 5800, Loss: 21.93743133544922 for layer [False]\n",
      "Iteration: 5900, Loss: 14.051883697509766 for layer [False]\n",
      "Iteration: 6000, Loss: 13.848825454711914 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.506603240966797 for layer [False]\n",
      "Iteration: 6200, Loss: 17.726547241210938 for layer [False]\n",
      "Iteration: 6300, Loss: 14.7661771774292 for layer [False]\n",
      "Iteration: 6400, Loss: 16.34576416015625 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.623895645141602 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.751766204833984 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.1744384765625 for layer [False]\n",
      "Iteration: 6800, Loss: 5.046041965484619 for layer [ True]\n",
      "Iteration: 6900, Loss: 42.73155975341797 for layer [False]\n",
      "Iteration: 7000, Loss: 10.50438117980957 for layer [False]\n",
      "Iteration: 7100, Loss: 11.852800369262695 for layer [False]\n",
      "Iteration: 7200, Loss: 16.95924186706543 for layer [False]\n",
      "Iteration: 7300, Loss: 4.712306022644043 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.180908203125 for layer [False]\n",
      "Iteration: 7500, Loss: 4.468417644500732 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.251378536224365 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.000445365905762 for layer [False]\n",
      "Iteration: 7800, Loss: 9.701043128967285 for layer [False]\n",
      "Iteration: 7900, Loss: 5.557413578033447 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8662657737731934 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.346876382827759 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.8517820835113525 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.539971351623535 for layer [False]\n",
      "Iteration: 8400, Loss: 2.022075653076172 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.46821117401123 for layer [False]\n",
      "Iteration: 8600, Loss: 7.515203475952148 for layer [False]\n",
      "Iteration: 8700, Loss: 2.7235336303710938 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.350349426269531 for layer [False]\n",
      "Iteration: 8900, Loss: 9.23553466796875 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4952300786972046 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5931659936904907 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8897546529769897 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.274641513824463 for layer [False]\n",
      "Iteration: 9400, Loss: 7.672275066375732 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5454055070877075 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.881467819213867 for layer [False]\n",
      "Iteration: 9700, Loss: 1.065759539604187 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8984313011169434 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.25903844833374 for layer [False]\n",
      "Iteration: 10000, Loss: 3.516307830810547 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2271361351013184 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7518147230148315 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.544489860534668 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5405487418174744 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5747895836830139 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4104321002960205 for layer [False]\n",
      "Iteration: 10700, Loss: 0.721680760383606 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.155872821807861 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5299878120422363 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.486772537231445 for layer [False]\n",
      "Iteration: 11100, Loss: 2.9856131076812744 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3619188368320465 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.44991493225097656 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.35232678055763245 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.795353412628174 for layer [False]\n",
      "Iteration: 11600, Loss: 0.37579843401908875 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19804096221923828 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2658505141735077 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2710373103618622 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1187247559428215 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.19015458226203918 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.17148950695991516 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10550020635128021 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15495043992996216 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.59076452255249 for layer [False]\n",
      "Iteration: 12600, Loss: 5.563806533813477 for layer [False]\n",
      "Iteration: 12700, Loss: 4.460270881652832 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10432541370391846 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09511128813028336 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06409398466348648 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.776972770690918 for layer [False]\n",
      "Iteration: 13200, Loss: 0.0653822124004364 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.499575614929199 for layer [False]\n",
      "Iteration: 13400, Loss: 0.047341130673885345 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06159671023488045 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.048620376735925674 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016341153532266617 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.037811338901519775 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.0315047986805439 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.4903182983398438 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029552342370152473 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.03566312789917 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010774384252727032 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.246488332748413 for layer [False]\n",
      "Iteration: 14500, Loss: 2.189013719558716 for layer [False]\n",
      "Iteration: 14600, Loss: 3.2582128047943115 for layer [False]\n",
      "Iteration: 14700, Loss: 3.184906005859375 for layer [False]\n",
      "Iteration: 14800, Loss: 3.7543749809265137 for layer [False]\n",
      "Iteration: 14900, Loss: 6.384275913238525 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0067157321609556675 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.3602118492126465 for layer [False]\n",
      "Iteration: 15200, Loss: 9.334345817565918 for layer [False]\n",
      "Iteration: 15300, Loss: 4.505822658538818 for layer [False]\n",
      "Iteration: 15400, Loss: 2.210312843322754 for layer [False]\n",
      "Iteration: 15500, Loss: 0.00338414846919477 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004366687964648008 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004222022369503975 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.7818806171417236 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002287849085405469 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0035223516169935465 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.1062278747558594 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0012022294104099274 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021219495683908463 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.225047588348389 for layer [False]\n",
      "Iteration: 16500, Loss: 4.615600109100342 for layer [False]\n",
      "Iteration: 16600, Loss: 3.306964635848999 for layer [False]\n",
      "Iteration: 16700, Loss: 0.00148688571061939 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008320671622641385 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010060110362246633 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009251037845388055 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.3192148208618164 for layer [False]\n",
      "Iteration: 17200, Loss: 3.002345323562622 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006701899692416191 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.243575096130371 for layer [False]\n",
      "Iteration: 17500, Loss: 10.452375411987305 for layer [False]\n",
      "Iteration: 17600, Loss: 3.5924785137176514 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008902697591111064 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.482999801635742 for layer [False]\n",
      "Iteration: 17900, Loss: 4.088892936706543 for layer [False]\n",
      "Iteration: 18000, Loss: 2.4048912525177 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007268548361025751 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008469507447443902 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.883942127227783 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00038076264900155365 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008219848969019949 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.0298094749450684 for layer [False]\n",
      "Iteration: 18700, Loss: 3.948307514190674 for layer [False]\n",
      "Iteration: 18800, Loss: 4.510149002075195 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8221803903579712 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0005245218053460121 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.2206082344055176 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0017271260730922222 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.4812469482421875 for layer [False]\n",
      "Iteration: 19400, Loss: 6.06559944152832 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006470142980106175 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00036596221616491675 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.83358097076416 for layer [False]\n",
      "Iteration: 19800, Loss: 3.4143526554107666 for layer [False]\n",
      "Iteration: 19900, Loss: 3.707761764526367 for layer [False]\n",
      "Iteration: 20000, Loss: 2.4663031101226807 for layer [False]\n",
      "Iteration: 20100, Loss: 5.161442756652832 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0019240310648456216 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.922311305999756 for layer [False]\n",
      "Iteration: 20400, Loss: 6.976824079174548e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00033846357837319374 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.000863265770021826 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.7104315757751465 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0004611634649336338 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0012019997229799628 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.92267906665802 for layer [False]\n",
      "Iteration: 21100, Loss: 8.592368125915527 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00073166040237993 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.527851581573486 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00014461178216151893 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.3772401809692383 for layer [False]\n",
      "Iteration: 21600, Loss: 4.305771827697754 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003612105792853981 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0003080021997448057 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.1269428730010986 for layer [False]\n",
      "Iteration: 22000, Loss: 3.971324920654297 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00014825776452198625 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00013534961908590049 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.206069469451904 for layer [False]\n",
      "Iteration: 22400, Loss: 5.2586236000061035 for layer [False]\n",
      "Iteration: 22500, Loss: 1.7285511493682861 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00018303557590115815 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00015505765622947365 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0002893358177971095 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0007010153494775295 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0008841168018989265 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00016263422730844468 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0004133718030061573 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00046259755617938936 for layer [ True]\n",
      "Iteration: 23400, Loss: 5.8697427448350936e-05 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00019013829296454787 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0006530697573907673 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.024580955505371 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006747269653715193 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00013362492609303445 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00024119560839608312 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.0002643738698679954 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.8971710205078125 for layer [False]\n",
      "Iteration: 24300, Loss: 2.463459014892578 for layer [False]\n",
      "Iteration: 24400, Loss: 4.228139400482178 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0001273274392588064 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00013390794629231095 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.233464241027832 for layer [False]\n",
      "Iteration: 24800, Loss: 3.975402355194092 for layer [False]\n",
      "Iteration: 24900, Loss: 8.462292316835374e-05 for layer [ True]\n",
      "Step 13500 | Loss: 0.000449\n",
      "Step 13600 | Loss: 0.000448\n",
      "Step 13700 | Loss: 0.000448\n",
      "Step 13800 | Loss: 0.000448\n",
      "Step 13900 | Loss: 0.000448\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1935.5804443359375 for layer [False]\n",
      "Iteration: 100, Loss: 320.2129211425781 for layer [ True]\n",
      "Iteration: 200, Loss: 1928.552734375 for layer [False]\n",
      "Iteration: 300, Loss: 1540.47314453125 for layer [False]\n",
      "Iteration: 400, Loss: 239.73556518554688 for layer [ True]\n",
      "Iteration: 500, Loss: 316.2245788574219 for layer [ True]\n",
      "Iteration: 600, Loss: 1440.771728515625 for layer [False]\n",
      "Iteration: 700, Loss: 225.408935546875 for layer [ True]\n",
      "Iteration: 800, Loss: 1155.09765625 for layer [False]\n",
      "Iteration: 900, Loss: 1017.3671875 for layer [False]\n",
      "Iteration: 1000, Loss: 153.20828247070312 for layer [ True]\n",
      "Iteration: 1100, Loss: 889.3638916015625 for layer [False]\n",
      "Iteration: 1200, Loss: 1054.9400634765625 for layer [False]\n",
      "Iteration: 1300, Loss: 988.519287109375 for layer [False]\n",
      "Iteration: 1400, Loss: 244.52574157714844 for layer [ True]\n",
      "Iteration: 1500, Loss: 307.7768859863281 for layer [ True]\n",
      "Iteration: 1600, Loss: 137.11993408203125 for layer [ True]\n",
      "Iteration: 1700, Loss: 123.91036224365234 for layer [ True]\n",
      "Iteration: 1800, Loss: 612.5206298828125 for layer [False]\n",
      "Iteration: 1900, Loss: 118.21407318115234 for layer [ True]\n",
      "Iteration: 2000, Loss: 105.17195892333984 for layer [ True]\n",
      "Iteration: 2100, Loss: 159.32240295410156 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.92906951904297 for layer [ True]\n",
      "Iteration: 2300, Loss: 452.0639343261719 for layer [False]\n",
      "Iteration: 2400, Loss: 129.6250457763672 for layer [ True]\n",
      "Iteration: 2500, Loss: 189.4372100830078 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.72186279296875 for layer [ True]\n",
      "Iteration: 2700, Loss: 91.18915557861328 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.3119201660156 for layer [False]\n",
      "Iteration: 2900, Loss: 251.14044189453125 for layer [False]\n",
      "Iteration: 3000, Loss: 80.71156311035156 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.9192352294922 for layer [False]\n",
      "Iteration: 3200, Loss: 227.07723999023438 for layer [False]\n",
      "Iteration: 3300, Loss: 155.35064697265625 for layer [False]\n",
      "Iteration: 3400, Loss: 73.69892120361328 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.87022399902344 for layer [False]\n",
      "Iteration: 3600, Loss: 60.9208869934082 for layer [ True]\n",
      "Iteration: 3700, Loss: 132.73941040039062 for layer [False]\n",
      "Iteration: 3800, Loss: 114.35111999511719 for layer [False]\n",
      "Iteration: 3900, Loss: 43.743656158447266 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.033973693847656 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.44446563720703 for layer [ True]\n",
      "Iteration: 4200, Loss: 79.33550262451172 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.733524322509766 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.56472396850586 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.94137191772461 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.59050750732422 for layer [ True]\n",
      "Iteration: 4700, Loss: 61.48527526855469 for layer [False]\n",
      "Iteration: 4800, Loss: 39.71797561645508 for layer [False]\n",
      "Iteration: 4900, Loss: 50.1163330078125 for layer [False]\n",
      "Iteration: 5000, Loss: 57.02956008911133 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.763166427612305 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.02267837524414 for layer [False]\n",
      "Iteration: 5300, Loss: 23.03898811340332 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.10002899169922 for layer [False]\n",
      "Iteration: 5500, Loss: 25.031871795654297 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.208961486816406 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.355098724365234 for layer [False]\n",
      "Iteration: 5800, Loss: 21.96974754333496 for layer [False]\n",
      "Iteration: 5900, Loss: 14.073084831237793 for layer [False]\n",
      "Iteration: 6000, Loss: 13.918397903442383 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.63762092590332 for layer [False]\n",
      "Iteration: 6200, Loss: 17.84522819519043 for layer [False]\n",
      "Iteration: 6300, Loss: 14.919034004211426 for layer [False]\n",
      "Iteration: 6400, Loss: 16.496538162231445 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.687745094299316 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.874390602111816 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.2176513671875 for layer [False]\n",
      "Iteration: 6800, Loss: 5.111278533935547 for layer [ True]\n",
      "Iteration: 6900, Loss: 43.60100555419922 for layer [False]\n",
      "Iteration: 7000, Loss: 10.74354362487793 for layer [False]\n",
      "Iteration: 7100, Loss: 12.014754295349121 for layer [False]\n",
      "Iteration: 7200, Loss: 17.220993041992188 for layer [False]\n",
      "Iteration: 7300, Loss: 4.771280288696289 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.47615623474121 for layer [False]\n",
      "Iteration: 7500, Loss: 4.494781017303467 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.305820941925049 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.050750732421875 for layer [False]\n",
      "Iteration: 7800, Loss: 9.786849975585938 for layer [False]\n",
      "Iteration: 7900, Loss: 5.586768627166748 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.901179075241089 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.377274513244629 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.8789379596710205 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.726688385009766 for layer [False]\n",
      "Iteration: 8400, Loss: 2.0380520820617676 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.646146774291992 for layer [False]\n",
      "Iteration: 8600, Loss: 7.714537143707275 for layer [False]\n",
      "Iteration: 8700, Loss: 2.738682270050049 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.37082576751709 for layer [False]\n",
      "Iteration: 8900, Loss: 9.341419219970703 for layer [False]\n",
      "Iteration: 9000, Loss: 1.5050188302993774 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.6010551452636719 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.905642032623291 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.346606254577637 for layer [False]\n",
      "Iteration: 9400, Loss: 7.7694830894470215 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5502678155899048 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.9273149967193604 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0672663450241089 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.9028564095497131 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.437633037567139 for layer [False]\n",
      "Iteration: 10000, Loss: 3.580111503601074 for layer [False]\n",
      "Iteration: 10100, Loss: 1.238403081893921 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7547177672386169 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.580218315124512 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5429049730300903 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.580838680267334 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4626643657684326 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7284241318702698 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.1862359046936035 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5347360372543335 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.602483749389648 for layer [False]\n",
      "Iteration: 11100, Loss: 3.0189945697784424 for layer [False]\n",
      "Iteration: 11200, Loss: 0.36545029282569885 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4525147080421448 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.35855966806411743 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.8434743881225586 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3795199990272522 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.20065425336360931 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2691546678543091 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2754157483577728 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1202763244509697 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.19200149178504944 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.17339259386062622 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10654818266630173 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15735401213169098 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.6900858879089355 for layer [False]\n",
      "Iteration: 12600, Loss: 5.567666053771973 for layer [False]\n",
      "Iteration: 12700, Loss: 4.522361755371094 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10549572110176086 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.0961957797408104 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.0648346096277237 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.848938465118408 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06642608344554901 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.586747646331787 for layer [False]\n",
      "Iteration: 13400, Loss: 0.048046521842479706 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06250465661287308 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04934285581111908 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016603892669081688 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.038129568099975586 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03196191415190697 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.534148693084717 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029928267002105713 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.013228416442871 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010903303511440754 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.2707762718200684 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2147209644317627 for layer [False]\n",
      "Iteration: 14600, Loss: 3.2442543506622314 for layer [False]\n",
      "Iteration: 14700, Loss: 3.20554256439209 for layer [False]\n",
      "Iteration: 14800, Loss: 3.7258870601654053 for layer [False]\n",
      "Iteration: 14900, Loss: 6.446694374084473 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0068208808079361916 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.445369243621826 for layer [False]\n",
      "Iteration: 15200, Loss: 9.441978454589844 for layer [False]\n",
      "Iteration: 15300, Loss: 4.517213821411133 for layer [False]\n",
      "Iteration: 15400, Loss: 2.2487070560455322 for layer [False]\n",
      "Iteration: 15500, Loss: 0.003418348263949156 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004461362026631832 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004255938343703747 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.8443827629089355 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0023032990284264088 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0035376395098865032 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.1394526958465576 for layer [False]\n",
      "Iteration: 16200, Loss: 0.001209119101986289 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.002152626868337393 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.30220890045166 for layer [False]\n",
      "Iteration: 16500, Loss: 4.701535701751709 for layer [False]\n",
      "Iteration: 16600, Loss: 3.371232271194458 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014793806476518512 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008357992628589272 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010139207588508725 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009344962891191244 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.313988208770752 for layer [False]\n",
      "Iteration: 17200, Loss: 3.041496753692627 for layer [False]\n",
      "Iteration: 17300, Loss: 0.000670995912514627 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.298742771148682 for layer [False]\n",
      "Iteration: 17500, Loss: 10.594322204589844 for layer [False]\n",
      "Iteration: 17600, Loss: 3.623311758041382 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008941501728259027 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.533921957015991 for layer [False]\n",
      "Iteration: 17900, Loss: 4.1852521896362305 for layer [False]\n",
      "Iteration: 18000, Loss: 2.426346778869629 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007258376572281122 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008309015538543463 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.966608047485352 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00039094185922294855 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.000819968874566257 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.0727336406707764 for layer [False]\n",
      "Iteration: 18700, Loss: 4.055257320404053 for layer [False]\n",
      "Iteration: 18800, Loss: 4.638391971588135 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8167626857757568 for layer [False]\n",
      "Iteration: 19000, Loss: 0.000521996698807925 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.262732744216919 for layer [False]\n",
      "Iteration: 19200, Loss: 0.001289188046939671 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.515272855758667 for layer [False]\n",
      "Iteration: 19400, Loss: 6.223474502563477 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006095265271142125 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003919375594705343 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.926435470581055 for layer [False]\n",
      "Iteration: 19800, Loss: 3.4538073539733887 for layer [False]\n",
      "Iteration: 19900, Loss: 3.7303478717803955 for layer [False]\n",
      "Iteration: 20000, Loss: 2.483616590499878 for layer [False]\n",
      "Iteration: 20100, Loss: 5.166476726531982 for layer [False]\n",
      "Iteration: 20200, Loss: 0.001911989296786487 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.9759886264801025 for layer [False]\n",
      "Iteration: 20400, Loss: 7.01224198564887e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003414198290556669 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0008611840312369168 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.768585205078125 for layer [False]\n",
      "Iteration: 20800, Loss: 0.000456138834124431 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0012822281569242477 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.9437849521636963 for layer [False]\n",
      "Iteration: 21100, Loss: 8.683528900146484 for layer [False]\n",
      "Iteration: 21200, Loss: 0.000641218270175159 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.611724376678467 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00014948874013498425 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.458829164505005 for layer [False]\n",
      "Iteration: 21600, Loss: 4.404764175415039 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003583356156013906 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00031117870821617544 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.1422295570373535 for layer [False]\n",
      "Iteration: 22000, Loss: 3.949136972427368 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00014990774798206985 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00013851073163095862 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.2068562507629395 for layer [False]\n",
      "Iteration: 22400, Loss: 5.309398651123047 for layer [False]\n",
      "Iteration: 22500, Loss: 1.7457789182662964 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0001781521859811619 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00014277388982009143 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00033926323521882296 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0004653430951293558 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0005674378480762243 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00172851188108325 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0002228887751698494 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0006804841104894876 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00012008115299977362 for layer [ True]\n",
      "Iteration: 23500, Loss: 3.3233489375561476e-05 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0008358901832252741 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.074652671813965 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0004313536628615111 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00023686280474066734 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00033972604433074594 for layer [ True]\n",
      "Iteration: 24100, Loss: 8.544838783564046e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.9798507690429688 for layer [False]\n",
      "Iteration: 24300, Loss: 2.485029935836792 for layer [False]\n",
      "Iteration: 24400, Loss: 4.1896891593933105 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00022779555001761764 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00010274431406287476 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.321549415588379 for layer [False]\n",
      "Iteration: 24800, Loss: 4.045116901397705 for layer [False]\n",
      "Iteration: 24900, Loss: 0.0001308510109083727 for layer [ True]\n",
      "Step 14000 | Loss: 0.000448\n",
      "Step 14100 | Loss: 0.000447\n",
      "Step 14200 | Loss: 0.000447\n",
      "Step 14300 | Loss: 0.000447\n",
      "Step 14400 | Loss: 0.000447\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1934.9046630859375 for layer [False]\n",
      "Iteration: 100, Loss: 320.7759704589844 for layer [ True]\n",
      "Iteration: 200, Loss: 1927.8306884765625 for layer [False]\n",
      "Iteration: 300, Loss: 1541.8880615234375 for layer [False]\n",
      "Iteration: 400, Loss: 239.66554260253906 for layer [ True]\n",
      "Iteration: 500, Loss: 316.23638916015625 for layer [ True]\n",
      "Iteration: 600, Loss: 1440.0657958984375 for layer [False]\n",
      "Iteration: 700, Loss: 225.25582885742188 for layer [ True]\n",
      "Iteration: 800, Loss: 1154.8355712890625 for layer [False]\n",
      "Iteration: 900, Loss: 1017.0465698242188 for layer [False]\n",
      "Iteration: 1000, Loss: 153.3623046875 for layer [ True]\n",
      "Iteration: 1100, Loss: 889.5223388671875 for layer [False]\n",
      "Iteration: 1200, Loss: 1055.9171142578125 for layer [False]\n",
      "Iteration: 1300, Loss: 988.2235717773438 for layer [False]\n",
      "Iteration: 1400, Loss: 245.5609893798828 for layer [ True]\n",
      "Iteration: 1500, Loss: 308.5234375 for layer [ True]\n",
      "Iteration: 1600, Loss: 137.18536376953125 for layer [ True]\n",
      "Iteration: 1700, Loss: 124.60034942626953 for layer [ True]\n",
      "Iteration: 1800, Loss: 613.0831298828125 for layer [False]\n",
      "Iteration: 1900, Loss: 118.73745727539062 for layer [ True]\n",
      "Iteration: 2000, Loss: 105.75125122070312 for layer [ True]\n",
      "Iteration: 2100, Loss: 159.7937469482422 for layer [ True]\n",
      "Iteration: 2200, Loss: 109.36537170410156 for layer [ True]\n",
      "Iteration: 2300, Loss: 451.8735046386719 for layer [False]\n",
      "Iteration: 2400, Loss: 130.6024627685547 for layer [ True]\n",
      "Iteration: 2500, Loss: 189.99192810058594 for layer [ True]\n",
      "Iteration: 2600, Loss: 122.07271575927734 for layer [ True]\n",
      "Iteration: 2700, Loss: 91.64946746826172 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.61322021484375 for layer [False]\n",
      "Iteration: 2900, Loss: 250.9109344482422 for layer [False]\n",
      "Iteration: 3000, Loss: 81.00640106201172 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.53504943847656 for layer [False]\n",
      "Iteration: 3200, Loss: 227.06398010253906 for layer [False]\n",
      "Iteration: 3300, Loss: 155.14199829101562 for layer [False]\n",
      "Iteration: 3400, Loss: 73.90129089355469 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.765625 for layer [False]\n",
      "Iteration: 3600, Loss: 61.218318939208984 for layer [ True]\n",
      "Iteration: 3700, Loss: 132.48121643066406 for layer [False]\n",
      "Iteration: 3800, Loss: 114.16950988769531 for layer [False]\n",
      "Iteration: 3900, Loss: 43.93618392944336 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.30154800415039 for layer [ True]\n",
      "Iteration: 4100, Loss: 54.53807830810547 for layer [ True]\n",
      "Iteration: 4200, Loss: 79.55164337158203 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.82014846801758 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.58441925048828 for layer [ True]\n",
      "Iteration: 4500, Loss: 55.09801483154297 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.70915985107422 for layer [ True]\n",
      "Iteration: 4700, Loss: 61.41209030151367 for layer [False]\n",
      "Iteration: 4800, Loss: 39.78288650512695 for layer [False]\n",
      "Iteration: 4900, Loss: 49.98789596557617 for layer [False]\n",
      "Iteration: 5000, Loss: 57.23670196533203 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.88531494140625 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.02388381958008 for layer [False]\n",
      "Iteration: 5300, Loss: 23.10195541381836 for layer [ True]\n",
      "Iteration: 5400, Loss: 26.109472274780273 for layer [False]\n",
      "Iteration: 5500, Loss: 25.04115867614746 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.402549743652344 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.40278434753418 for layer [False]\n",
      "Iteration: 5800, Loss: 22.086795806884766 for layer [False]\n",
      "Iteration: 5900, Loss: 14.120361328125 for layer [False]\n",
      "Iteration: 6000, Loss: 13.991483688354492 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.156591415405273 for layer [False]\n",
      "Iteration: 6200, Loss: 18.09929084777832 for layer [False]\n",
      "Iteration: 6300, Loss: 14.82275104522705 for layer [False]\n",
      "Iteration: 6400, Loss: 16.53193473815918 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.707427978515625 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.89535140991211 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.319583892822266 for layer [False]\n",
      "Iteration: 6800, Loss: 5.13711404800415 for layer [ True]\n",
      "Iteration: 6900, Loss: 43.91461944580078 for layer [False]\n",
      "Iteration: 7000, Loss: 10.809536933898926 for layer [False]\n",
      "Iteration: 7100, Loss: 12.112936973571777 for layer [False]\n",
      "Iteration: 7200, Loss: 17.404508590698242 for layer [False]\n",
      "Iteration: 7300, Loss: 4.7785444259643555 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.512983322143555 for layer [False]\n",
      "Iteration: 7500, Loss: 4.501102447509766 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.313473224639893 for layer [ True]\n",
      "Iteration: 7700, Loss: 11.971272468566895 for layer [False]\n",
      "Iteration: 7800, Loss: 9.788516998291016 for layer [False]\n",
      "Iteration: 7900, Loss: 5.5955047607421875 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8973350524902344 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.3772010803222656 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.881828546524048 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.788561820983887 for layer [False]\n",
      "Iteration: 8400, Loss: 2.041740894317627 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.840375900268555 for layer [False]\n",
      "Iteration: 8600, Loss: 7.824395179748535 for layer [False]\n",
      "Iteration: 8700, Loss: 2.741084575653076 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.388213157653809 for layer [False]\n",
      "Iteration: 8900, Loss: 9.32943058013916 for layer [False]\n",
      "Iteration: 9000, Loss: 1.5054028034210205 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.605200171470642 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.907360315322876 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.36841344833374 for layer [False]\n",
      "Iteration: 9400, Loss: 7.834970951080322 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5514651536941528 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.9244778156280518 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0735217332839966 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.909095823764801 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.345912933349609 for layer [False]\n",
      "Iteration: 10000, Loss: 3.604994058609009 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2407474517822266 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7544305920600891 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.597002983093262 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5452831387519836 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5808426737785339 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4458653926849365 for layer [False]\n",
      "Iteration: 10700, Loss: 0.729321300983429 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.218912124633789 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5349335074424744 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.66186237335205 for layer [False]\n",
      "Iteration: 11100, Loss: 2.96114444732666 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3673451840877533 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4559841752052307 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.36041998863220215 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.8613972663879395 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3819528818130493 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.2021830826997757 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2708507776260376 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.27665844559669495 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.12161616235971451 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.19278927147388458 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1748710423707962 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10702338069677353 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15868987143039703 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.747059345245361 for layer [False]\n",
      "Iteration: 12600, Loss: 5.577953815460205 for layer [False]\n",
      "Iteration: 12700, Loss: 4.518847465515137 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10700198262929916 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09698807448148727 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06559889018535614 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.884619235992432 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06707475334405899 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.611017227172852 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04876874014735222 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06344032287597656 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.049868304282426834 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016917822882533073 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.038508057594299316 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.0323939211666584 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.5272700786590576 for layer [False]\n",
      "Iteration: 14100, Loss: 0.030322270467877388 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.018019676208496 for layer [False]\n",
      "Iteration: 14300, Loss: 0.011095575988292694 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.2780561447143555 for layer [False]\n",
      "Iteration: 14500, Loss: 2.1728734970092773 for layer [False]\n",
      "Iteration: 14600, Loss: 3.2620656490325928 for layer [False]\n",
      "Iteration: 14700, Loss: 3.2283132076263428 for layer [False]\n",
      "Iteration: 14800, Loss: 3.734060525894165 for layer [False]\n",
      "Iteration: 14900, Loss: 6.5313720703125 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006945532280951738 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.4372124671936035 for layer [False]\n",
      "Iteration: 15200, Loss: 9.546144485473633 for layer [False]\n",
      "Iteration: 15300, Loss: 4.530092716217041 for layer [False]\n",
      "Iteration: 15400, Loss: 2.2360618114471436 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0034232665784657 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004513061139732599 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.00431144330650568 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.813910961151123 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002352606039494276 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003612314350903034 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.155017614364624 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0012163800420239568 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021871435455977917 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.326551914215088 for layer [False]\n",
      "Iteration: 16500, Loss: 4.708108901977539 for layer [False]\n",
      "Iteration: 16600, Loss: 3.3686447143554688 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014707153895869851 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008550923666916788 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.001042926567606628 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009420253336429596 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.3683080673217773 for layer [False]\n",
      "Iteration: 17200, Loss: 3.0640578269958496 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006861871224828064 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.337677478790283 for layer [False]\n",
      "Iteration: 17500, Loss: 10.67081069946289 for layer [False]\n",
      "Iteration: 17600, Loss: 3.6182198524475098 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008787628612481058 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.5472161769866943 for layer [False]\n",
      "Iteration: 17900, Loss: 4.170845985412598 for layer [False]\n",
      "Iteration: 18000, Loss: 2.4408516883850098 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007274944800883532 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008713888819329441 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.967875003814697 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00038697122363373637 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008237130823545158 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.0766894817352295 for layer [False]\n",
      "Iteration: 18700, Loss: 4.040981292724609 for layer [False]\n",
      "Iteration: 18800, Loss: 4.712060928344727 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8191519975662231 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0004703930753748864 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.234157085418701 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0020281674806028605 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.4959371089935303 for layer [False]\n",
      "Iteration: 19400, Loss: 6.190641403198242 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006614350131712854 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00036267004907131195 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.969964504241943 for layer [False]\n",
      "Iteration: 19800, Loss: 3.436124801635742 for layer [False]\n",
      "Iteration: 19900, Loss: 3.700913190841675 for layer [False]\n",
      "Iteration: 20000, Loss: 2.462977647781372 for layer [False]\n",
      "Iteration: 20100, Loss: 5.128662109375 for layer [False]\n",
      "Iteration: 20200, Loss: 0.001860559219494462 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.0041344165802 for layer [False]\n",
      "Iteration: 20400, Loss: 7.309138891287148e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003393929800949991 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0008322685607708991 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.738948106765747 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0005116334650665522 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0011279229074716568 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.947127342224121 for layer [False]\n",
      "Iteration: 21100, Loss: 8.773916244506836 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0007509264396503568 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.589358806610107 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00014536434900946915 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.4505107402801514 for layer [False]\n",
      "Iteration: 21600, Loss: 4.409516334533691 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00037404641625471413 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.000268331088591367 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.1808574199676514 for layer [False]\n",
      "Iteration: 22000, Loss: 3.9341273307800293 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0003024613833986223 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0002636845747474581 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.170421600341797 for layer [False]\n",
      "Iteration: 22400, Loss: 5.331419944763184 for layer [False]\n",
      "Iteration: 22500, Loss: 1.745891809463501 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0002407431893516332 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00019014993449673057 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0001866858365247026 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.001896342495456338 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0004053930751979351 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0007438973407261074 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0001861404743976891 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00032873364398255944 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002551324723754078 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0002720350748859346 for layer [ True]\n",
      "Iteration: 23600, Loss: 4.962613820680417e-05 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.035282611846924 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0008949078619480133 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00018272957822773606 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0003758598177228123 for layer [ True]\n",
      "Iteration: 24100, Loss: 8.372175216209143e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.999368190765381 for layer [False]\n",
      "Iteration: 24300, Loss: 2.5109708309173584 for layer [False]\n",
      "Iteration: 24400, Loss: 4.163586139678955 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0002006301801884547 for layer [ True]\n",
      "Iteration: 24600, Loss: 6.963059422560036e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.382587909698486 for layer [False]\n",
      "Iteration: 24800, Loss: 4.02946138381958 for layer [False]\n",
      "Iteration: 24900, Loss: 0.0001316658454015851 for layer [ True]\n",
      "Step 14500 | Loss: 0.000448\n",
      "Step 14600 | Loss: 0.000447\n",
      "Step 14700 | Loss: 0.000447\n",
      "Step 14800 | Loss: 0.000446\n",
      "Step 14900 | Loss: 0.000446\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1931.38427734375 for layer [False]\n",
      "Iteration: 100, Loss: 320.62451171875 for layer [ True]\n",
      "Iteration: 200, Loss: 1925.2025146484375 for layer [False]\n",
      "Iteration: 300, Loss: 1536.6031494140625 for layer [False]\n",
      "Iteration: 400, Loss: 239.22796630859375 for layer [ True]\n",
      "Iteration: 500, Loss: 314.5281982421875 for layer [ True]\n",
      "Iteration: 600, Loss: 1437.17578125 for layer [False]\n",
      "Iteration: 700, Loss: 224.50448608398438 for layer [ True]\n",
      "Iteration: 800, Loss: 1152.4635009765625 for layer [False]\n",
      "Iteration: 900, Loss: 1014.9791870117188 for layer [False]\n",
      "Iteration: 1000, Loss: 153.2101287841797 for layer [ True]\n",
      "Iteration: 1100, Loss: 887.8643798828125 for layer [False]\n",
      "Iteration: 1200, Loss: 1052.4564208984375 for layer [False]\n",
      "Iteration: 1300, Loss: 985.669921875 for layer [False]\n",
      "Iteration: 1400, Loss: 244.6734161376953 for layer [ True]\n",
      "Iteration: 1500, Loss: 307.3429870605469 for layer [ True]\n",
      "Iteration: 1600, Loss: 136.3304443359375 for layer [ True]\n",
      "Iteration: 1700, Loss: 124.23590850830078 for layer [ True]\n",
      "Iteration: 1800, Loss: 610.8473510742188 for layer [False]\n",
      "Iteration: 1900, Loss: 119.10923767089844 for layer [ True]\n",
      "Iteration: 2000, Loss: 105.45223999023438 for layer [ True]\n",
      "Iteration: 2100, Loss: 159.4515380859375 for layer [ True]\n",
      "Iteration: 2200, Loss: 109.41001892089844 for layer [ True]\n",
      "Iteration: 2300, Loss: 450.84967041015625 for layer [False]\n",
      "Iteration: 2400, Loss: 130.99998474121094 for layer [ True]\n",
      "Iteration: 2500, Loss: 188.5138702392578 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.4477310180664 for layer [ True]\n",
      "Iteration: 2700, Loss: 91.35823822021484 for layer [ True]\n",
      "Iteration: 2800, Loss: 325.8591003417969 for layer [False]\n",
      "Iteration: 2900, Loss: 249.90489196777344 for layer [False]\n",
      "Iteration: 3000, Loss: 80.80482482910156 for layer [ True]\n",
      "Iteration: 3100, Loss: 218.8799591064453 for layer [False]\n",
      "Iteration: 3200, Loss: 226.4745330810547 for layer [False]\n",
      "Iteration: 3300, Loss: 154.4597625732422 for layer [False]\n",
      "Iteration: 3400, Loss: 73.65302276611328 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.16339111328125 for layer [False]\n",
      "Iteration: 3600, Loss: 61.35533142089844 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.73362731933594 for layer [False]\n",
      "Iteration: 3800, Loss: 113.47925567626953 for layer [False]\n",
      "Iteration: 3900, Loss: 43.63571548461914 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.13710021972656 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.9954833984375 for layer [ True]\n",
      "Iteration: 4200, Loss: 79.04915618896484 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.562835693359375 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.43763732910156 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.70793914794922 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.637474060058594 for layer [ True]\n",
      "Iteration: 4700, Loss: 61.09512710571289 for layer [False]\n",
      "Iteration: 4800, Loss: 39.54343032836914 for layer [False]\n",
      "Iteration: 4900, Loss: 49.84018325805664 for layer [False]\n",
      "Iteration: 5000, Loss: 56.90726852416992 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.646142959594727 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.93352127075195 for layer [False]\n",
      "Iteration: 5300, Loss: 22.86806297302246 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.95011329650879 for layer [False]\n",
      "Iteration: 5500, Loss: 24.84027099609375 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.327192306518555 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.410690307617188 for layer [False]\n",
      "Iteration: 5800, Loss: 21.974367141723633 for layer [False]\n",
      "Iteration: 5900, Loss: 14.052924156188965 for layer [False]\n",
      "Iteration: 6000, Loss: 13.858463287353516 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.531137466430664 for layer [False]\n",
      "Iteration: 6200, Loss: 18.234081268310547 for layer [False]\n",
      "Iteration: 6300, Loss: 15.066607475280762 for layer [False]\n",
      "Iteration: 6400, Loss: 16.39809226989746 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.661224365234375 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.851613998413086 for layer [ True]\n",
      "Iteration: 6700, Loss: 18.199138641357422 for layer [False]\n",
      "Iteration: 6800, Loss: 5.113534927368164 for layer [ True]\n",
      "Iteration: 6900, Loss: 44.330379486083984 for layer [False]\n",
      "Iteration: 7000, Loss: 10.731719970703125 for layer [False]\n",
      "Iteration: 7100, Loss: 12.151896476745605 for layer [False]\n",
      "Iteration: 7200, Loss: 17.381916046142578 for layer [False]\n",
      "Iteration: 7300, Loss: 4.738868713378906 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.7452335357666 for layer [False]\n",
      "Iteration: 7500, Loss: 4.466590881347656 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.285762310028076 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.149603843688965 for layer [False]\n",
      "Iteration: 7800, Loss: 9.880505561828613 for layer [False]\n",
      "Iteration: 7900, Loss: 5.495792865753174 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.8613815307617188 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.355074882507324 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.860243082046509 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.686504364013672 for layer [False]\n",
      "Iteration: 8400, Loss: 2.025458812713623 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.892781257629395 for layer [False]\n",
      "Iteration: 8600, Loss: 7.727367401123047 for layer [False]\n",
      "Iteration: 8700, Loss: 2.7133073806762695 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.447087287902832 for layer [False]\n",
      "Iteration: 8900, Loss: 9.311408042907715 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4983717203140259 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5775082111358643 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8911123275756836 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.437093734741211 for layer [False]\n",
      "Iteration: 9400, Loss: 7.94551420211792 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5180785655975342 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.8900368213653564 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0657241344451904 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8956007361412048 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.33658504486084 for layer [False]\n",
      "Iteration: 10000, Loss: 3.569188356399536 for layer [False]\n",
      "Iteration: 10100, Loss: 1.221300721168518 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7410357594490051 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.598987579345703 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5428968071937561 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5716902613639832 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4769725799560547 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7231897711753845 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.232114315032959 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5263476371765137 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.742692947387695 for layer [False]\n",
      "Iteration: 11100, Loss: 2.9708025455474854 for layer [False]\n",
      "Iteration: 11200, Loss: 0.36066362261772156 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4523948132991791 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3584632873535156 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.916557550430298 for layer [False]\n",
      "Iteration: 11600, Loss: 0.378543883562088 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.2020971029996872 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.26683950424194336 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2764814794063568 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.12133746594190598 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.19012008607387543 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.17209939658641815 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10633610188961029 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15779784321784973 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.79298734664917 for layer [False]\n",
      "Iteration: 12600, Loss: 5.569140911102295 for layer [False]\n",
      "Iteration: 12700, Loss: 4.5817790031433105 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10585608333349228 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09654182195663452 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.0641808956861496 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.9468889236450195 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06653061509132385 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.614598751068115 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04869084432721138 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06308192759752274 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04955150559544563 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016709426417946815 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.038387566804885864 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03201970085501671 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.5523998737335205 for layer [False]\n",
      "Iteration: 14100, Loss: 0.02967563271522522 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.079386234283447 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010875514708459377 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.3141534328460693 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2132043838500977 for layer [False]\n",
      "Iteration: 14600, Loss: 3.2836883068084717 for layer [False]\n",
      "Iteration: 14700, Loss: 3.24253249168396 for layer [False]\n",
      "Iteration: 14800, Loss: 3.7546675205230713 for layer [False]\n",
      "Iteration: 14900, Loss: 6.458387851715088 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006848799996078014 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.452640056610107 for layer [False]\n",
      "Iteration: 15200, Loss: 9.552346229553223 for layer [False]\n",
      "Iteration: 15300, Loss: 4.623260021209717 for layer [False]\n",
      "Iteration: 15400, Loss: 2.2554118633270264 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0034173952881246805 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004427438136190176 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004243454895913601 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.8895416259765625 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002287304727360606 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0035398106556385756 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.148477792739868 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0012084593763574958 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021466754842549562 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.316229820251465 for layer [False]\n",
      "Iteration: 16500, Loss: 4.764180660247803 for layer [False]\n",
      "Iteration: 16600, Loss: 3.3943982124328613 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014456096105277538 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008290839032270014 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010100833605974913 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.000919472542591393 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.4314186573028564 for layer [False]\n",
      "Iteration: 17200, Loss: 3.080181121826172 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006657032645307481 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.399153232574463 for layer [False]\n",
      "Iteration: 17500, Loss: 10.81022834777832 for layer [False]\n",
      "Iteration: 17600, Loss: 3.586432695388794 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008972856448963284 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.5626630783081055 for layer [False]\n",
      "Iteration: 17900, Loss: 4.182460784912109 for layer [False]\n",
      "Iteration: 18000, Loss: 2.496309280395508 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007292927475646138 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008611214580014348 for layer [ True]\n",
      "Iteration: 18300, Loss: 6.040929317474365 for layer [False]\n",
      "Iteration: 18400, Loss: 0.000382910919142887 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008007853757590055 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.0956103801727295 for layer [False]\n",
      "Iteration: 18700, Loss: 4.114552021026611 for layer [False]\n",
      "Iteration: 18800, Loss: 4.606788635253906 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8407541513442993 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00047442896175198257 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.27520489692688 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0015892956871539354 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.525834560394287 for layer [False]\n",
      "Iteration: 19400, Loss: 6.249512672424316 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006477266433648765 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003660792135633528 for layer [ True]\n",
      "Iteration: 19700, Loss: 4.96617317199707 for layer [False]\n",
      "Iteration: 19800, Loss: 3.4007537364959717 for layer [False]\n",
      "Iteration: 19900, Loss: 3.774204969406128 for layer [False]\n",
      "Iteration: 20000, Loss: 2.4453961849212646 for layer [False]\n",
      "Iteration: 20100, Loss: 5.162655353546143 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0018816482042893767 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.0001420974731445 for layer [False]\n",
      "Iteration: 20400, Loss: 7.043327786959708e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003409320779610425 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0007993223844096065 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.806431531906128 for layer [False]\n",
      "Iteration: 20800, Loss: 0.000473307998618111 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0010182606056332588 for layer [ True]\n",
      "Iteration: 21000, Loss: 1.9741473197937012 for layer [False]\n",
      "Iteration: 21100, Loss: 8.781465530395508 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0007121724192984402 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.6117024421691895 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00015050939691718668 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.446589469909668 for layer [False]\n",
      "Iteration: 21600, Loss: 4.437027454376221 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003648884012363851 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00020461197709664702 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.1775355339050293 for layer [False]\n",
      "Iteration: 22000, Loss: 3.9698033332824707 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0003504288906697184 for layer [ True]\n",
      "Iteration: 22200, Loss: 7.456801540683955e-05 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.280277729034424 for layer [False]\n",
      "Iteration: 22400, Loss: 5.342935562133789 for layer [False]\n",
      "Iteration: 22500, Loss: 1.7879868745803833 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0001382301124976948 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00019224650168325752 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00013403169577941298 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0014336089370772243 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00048290996346622705 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0005987240583635867 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00023860789951868355 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0002762169751804322 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002405670384177938 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0004349134105723351 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0002157645794795826 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.0657365322113037 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007476871251128614 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0002471590123604983 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00032606287277303636 for layer [ True]\n",
      "Iteration: 24100, Loss: 4.683309089159593e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 4.032175064086914 for layer [False]\n",
      "Iteration: 24300, Loss: 2.5724594593048096 for layer [False]\n",
      "Iteration: 24400, Loss: 4.2874674797058105 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0004542290698736906 for layer [ True]\n",
      "Iteration: 24600, Loss: 0.00010492884030099958 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.405462741851807 for layer [False]\n",
      "Iteration: 24800, Loss: 4.021420955657959 for layer [False]\n",
      "Iteration: 24900, Loss: 9.047085040947422e-05 for layer [ True]\n",
      "Step 15000 | Loss: 0.000446\n",
      "Step 15100 | Loss: 0.000446\n",
      "Step 15200 | Loss: 0.000446\n",
      "Step 15300 | Loss: 0.000446\n",
      "Step 15400 | Loss: 0.000446\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1929.9105224609375 for layer [False]\n",
      "Iteration: 100, Loss: 320.59014892578125 for layer [ True]\n",
      "Iteration: 200, Loss: 1926.4559326171875 for layer [False]\n",
      "Iteration: 300, Loss: 1535.9681396484375 for layer [False]\n",
      "Iteration: 400, Loss: 238.64044189453125 for layer [ True]\n",
      "Iteration: 500, Loss: 312.1958923339844 for layer [ True]\n",
      "Iteration: 600, Loss: 1438.0164794921875 for layer [False]\n",
      "Iteration: 700, Loss: 223.7241973876953 for layer [ True]\n",
      "Iteration: 800, Loss: 1152.7552490234375 for layer [False]\n",
      "Iteration: 900, Loss: 1014.8002319335938 for layer [False]\n",
      "Iteration: 1000, Loss: 152.48289489746094 for layer [ True]\n",
      "Iteration: 1100, Loss: 887.6931762695312 for layer [False]\n",
      "Iteration: 1200, Loss: 1052.4814453125 for layer [False]\n",
      "Iteration: 1300, Loss: 986.7208862304688 for layer [False]\n",
      "Iteration: 1400, Loss: 242.44749450683594 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.68756103515625 for layer [ True]\n",
      "Iteration: 1600, Loss: 135.44781494140625 for layer [ True]\n",
      "Iteration: 1700, Loss: 124.42752838134766 for layer [ True]\n",
      "Iteration: 1800, Loss: 610.7576904296875 for layer [False]\n",
      "Iteration: 1900, Loss: 118.66085052490234 for layer [ True]\n",
      "Iteration: 2000, Loss: 105.17909240722656 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.5294647216797 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.63784790039062 for layer [ True]\n",
      "Iteration: 2300, Loss: 450.77886962890625 for layer [False]\n",
      "Iteration: 2400, Loss: 130.65109252929688 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.00616455078125 for layer [ True]\n",
      "Iteration: 2600, Loss: 120.52784729003906 for layer [ True]\n",
      "Iteration: 2700, Loss: 91.03784942626953 for layer [ True]\n",
      "Iteration: 2800, Loss: 325.7268981933594 for layer [False]\n",
      "Iteration: 2900, Loss: 250.1381072998047 for layer [False]\n",
      "Iteration: 3000, Loss: 80.06935119628906 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.36087036132812 for layer [False]\n",
      "Iteration: 3200, Loss: 226.63677978515625 for layer [False]\n",
      "Iteration: 3300, Loss: 154.25233459472656 for layer [False]\n",
      "Iteration: 3400, Loss: 73.02641296386719 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.32208251953125 for layer [False]\n",
      "Iteration: 3600, Loss: 61.2569580078125 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.77342224121094 for layer [False]\n",
      "Iteration: 3800, Loss: 113.5198974609375 for layer [False]\n",
      "Iteration: 3900, Loss: 43.30381774902344 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.807498931884766 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.29010772705078 for layer [ True]\n",
      "Iteration: 4200, Loss: 77.97766876220703 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.02800750732422 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.02156066894531 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.02041244506836 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.1155891418457 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.91267013549805 for layer [False]\n",
      "Iteration: 4800, Loss: 39.362613677978516 for layer [False]\n",
      "Iteration: 4900, Loss: 49.831546783447266 for layer [False]\n",
      "Iteration: 5000, Loss: 56.247127532958984 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.3990421295166 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.99789047241211 for layer [False]\n",
      "Iteration: 5300, Loss: 22.52301788330078 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.94551658630371 for layer [False]\n",
      "Iteration: 5500, Loss: 24.473512649536133 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.13241195678711 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.291507720947266 for layer [False]\n",
      "Iteration: 5800, Loss: 21.920223236083984 for layer [False]\n",
      "Iteration: 5900, Loss: 13.855283737182617 for layer [False]\n",
      "Iteration: 6000, Loss: 13.735827445983887 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.527284622192383 for layer [False]\n",
      "Iteration: 6200, Loss: 17.93196678161621 for layer [False]\n",
      "Iteration: 6300, Loss: 15.102544784545898 for layer [False]\n",
      "Iteration: 6400, Loss: 16.158842086791992 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.581186294555664 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.777909278869629 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.759429931640625 for layer [False]\n",
      "Iteration: 6800, Loss: 5.049549102783203 for layer [ True]\n",
      "Iteration: 6900, Loss: 44.33141326904297 for layer [False]\n",
      "Iteration: 7000, Loss: 10.627267837524414 for layer [False]\n",
      "Iteration: 7100, Loss: 12.028627395629883 for layer [False]\n",
      "Iteration: 7200, Loss: 17.211158752441406 for layer [False]\n",
      "Iteration: 7300, Loss: 4.691221714019775 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.801227569580078 for layer [False]\n",
      "Iteration: 7500, Loss: 4.400996208190918 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.186432361602783 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.230147361755371 for layer [False]\n",
      "Iteration: 7800, Loss: 10.000508308410645 for layer [False]\n",
      "Iteration: 7900, Loss: 5.412473678588867 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.7842609882354736 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2794973850250244 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.824636697769165 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.607405662536621 for layer [False]\n",
      "Iteration: 8400, Loss: 2.0075056552886963 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.829089164733887 for layer [False]\n",
      "Iteration: 8600, Loss: 7.833646297454834 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6809160709381104 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.404833793640137 for layer [False]\n",
      "Iteration: 8900, Loss: 9.13334846496582 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4811931848526 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5560054779052734 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8769152164459229 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.478932857513428 for layer [False]\n",
      "Iteration: 9400, Loss: 7.885997772216797 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4986602067947388 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.854185104370117 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0476231575012207 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8851979374885559 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.34711217880249 for layer [False]\n",
      "Iteration: 10000, Loss: 3.5179240703582764 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1987313032150269 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7309269309043884 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.55072021484375 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5342091917991638 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.562816321849823 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4622421264648438 for layer [False]\n",
      "Iteration: 10700, Loss: 0.711452841758728 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.2048420906066895 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5168827176094055 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.78908920288086 for layer [False]\n",
      "Iteration: 11100, Loss: 2.8923606872558594 for layer [False]\n",
      "Iteration: 11200, Loss: 0.35503992438316345 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.44548606872558594 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3538658022880554 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.818856954574585 for layer [False]\n",
      "Iteration: 11600, Loss: 0.378365159034729 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19828028976917267 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2623244524002075 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2711656391620636 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1192852184176445 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18454952538013458 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1686791181564331 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10484444350004196 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15683013200759888 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.816705226898193 for layer [False]\n",
      "Iteration: 12600, Loss: 5.632941722869873 for layer [False]\n",
      "Iteration: 12700, Loss: 4.5237531661987305 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10326147079467773 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09501386433839798 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.0624740868806839 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.871635437011719 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06536105275154114 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.660658359527588 for layer [False]\n",
      "Iteration: 13400, Loss: 0.047601118683815 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.061931971460580826 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.048086460679769516 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.01626017317175865 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03757116198539734 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.0311573538929224 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.5368359088897705 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029069820418953896 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.091395854949951 for layer [False]\n",
      "Iteration: 14300, Loss: 0.01061196532100439 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.294790029525757 for layer [False]\n",
      "Iteration: 14500, Loss: 2.236384868621826 for layer [False]\n",
      "Iteration: 14600, Loss: 3.22074031829834 for layer [False]\n",
      "Iteration: 14700, Loss: 3.2107949256896973 for layer [False]\n",
      "Iteration: 14800, Loss: 3.8280537128448486 for layer [False]\n",
      "Iteration: 14900, Loss: 6.519937038421631 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006572611629962921 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.375184059143066 for layer [False]\n",
      "Iteration: 15200, Loss: 9.577272415161133 for layer [False]\n",
      "Iteration: 15300, Loss: 4.610238552093506 for layer [False]\n",
      "Iteration: 15400, Loss: 2.2723333835601807 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033575217239558697 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004230753984302282 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004084051586687565 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.876084804534912 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002229002770036459 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0033961008302867413 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.123403787612915 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011649722000584006 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0020976720843464136 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.298819065093994 for layer [False]\n",
      "Iteration: 16500, Loss: 4.756407737731934 for layer [False]\n",
      "Iteration: 16600, Loss: 3.3766801357269287 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014372611185535789 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008049008902162313 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0009957527508959174 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008727236418053508 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.436154842376709 for layer [False]\n",
      "Iteration: 17200, Loss: 3.0537264347076416 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006474924739450216 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.349982738494873 for layer [False]\n",
      "Iteration: 17500, Loss: 10.936593055725098 for layer [False]\n",
      "Iteration: 17600, Loss: 3.57045578956604 for layer [False]\n",
      "Iteration: 17700, Loss: 0.00087342708138749 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.508216381072998 for layer [False]\n",
      "Iteration: 17900, Loss: 4.262381076812744 for layer [False]\n",
      "Iteration: 18000, Loss: 2.5503926277160645 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006809440674260259 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008559997077099979 for layer [ True]\n",
      "Iteration: 18300, Loss: 6.006623268127441 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00037070122198201716 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007854817085899413 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.105944871902466 for layer [False]\n",
      "Iteration: 18700, Loss: 4.090795993804932 for layer [False]\n",
      "Iteration: 18800, Loss: 4.671367645263672 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8606518507003784 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00020238703291397542 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.2496252059936523 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0016738647827878594 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.506526231765747 for layer [False]\n",
      "Iteration: 19400, Loss: 6.233876705169678 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007030155975371599 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003248490975238383 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.039355754852295 for layer [False]\n",
      "Iteration: 19800, Loss: 3.385305404663086 for layer [False]\n",
      "Iteration: 19900, Loss: 3.788914918899536 for layer [False]\n",
      "Iteration: 20000, Loss: 2.490220069885254 for layer [False]\n",
      "Iteration: 20100, Loss: 5.068964958190918 for layer [False]\n",
      "Iteration: 20200, Loss: 0.001549973152577877 for layer [ True]\n",
      "Iteration: 20300, Loss: 2.9810967445373535 for layer [False]\n",
      "Iteration: 20400, Loss: 6.117274460848421e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00037956589949317276 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0008834170876070857 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.8150248527526855 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0003694657061714679 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0007043923251330853 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.057476043701172 for layer [False]\n",
      "Iteration: 21100, Loss: 8.870478630065918 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00040875127888284624 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.593866348266602 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0007267568726092577 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.4739139080047607 for layer [False]\n",
      "Iteration: 21600, Loss: 4.419278621673584 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0002481846313457936 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0003169526462443173 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.099118232727051 for layer [False]\n",
      "Iteration: 22000, Loss: 4.00747537612915 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00010241464769933373 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00014714770077262074 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.294898509979248 for layer [False]\n",
      "Iteration: 22400, Loss: 5.360757827758789 for layer [False]\n",
      "Iteration: 22500, Loss: 1.7896952629089355 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00014749201363883913 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00010288474732078612 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0005596472765319049 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.00027824557037092745 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0001972350582946092 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.002023509470745921 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00025963460211642087 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0005184205947443843 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00015153885760810226 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00020098745881114155 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0003101691254414618 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.08984112739563 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006409595953300595 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0001305845653405413 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0001810465764719993 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.0002058632962871343 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.9685866832733154 for layer [False]\n",
      "Iteration: 24300, Loss: 2.5606343746185303 for layer [False]\n",
      "Iteration: 24400, Loss: 4.4118757247924805 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00011488657037261873 for layer [ True]\n",
      "Iteration: 24600, Loss: 8.734327275305986e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.381407737731934 for layer [False]\n",
      "Iteration: 24800, Loss: 4.1045403480529785 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00022518369951285422 for layer [ True]\n",
      "Step 15500 | Loss: 0.000446\n",
      "Step 15600 | Loss: 0.000445\n",
      "Step 15700 | Loss: 0.000445\n",
      "Step 15800 | Loss: 0.000444\n",
      "Step 15900 | Loss: 0.000445\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1929.1280517578125 for layer [False]\n",
      "Iteration: 100, Loss: 321.24432373046875 for layer [ True]\n",
      "Iteration: 200, Loss: 1925.2896728515625 for layer [False]\n",
      "Iteration: 300, Loss: 1535.0921630859375 for layer [False]\n",
      "Iteration: 400, Loss: 238.63807678222656 for layer [ True]\n",
      "Iteration: 500, Loss: 313.419921875 for layer [ True]\n",
      "Iteration: 600, Loss: 1436.82275390625 for layer [False]\n",
      "Iteration: 700, Loss: 223.94540405273438 for layer [ True]\n",
      "Iteration: 800, Loss: 1151.4324951171875 for layer [False]\n",
      "Iteration: 900, Loss: 1013.5050048828125 for layer [False]\n",
      "Iteration: 1000, Loss: 153.07559204101562 for layer [ True]\n",
      "Iteration: 1100, Loss: 886.5571899414062 for layer [False]\n",
      "Iteration: 1200, Loss: 1051.787109375 for layer [False]\n",
      "Iteration: 1300, Loss: 986.427001953125 for layer [False]\n",
      "Iteration: 1400, Loss: 243.7466278076172 for layer [ True]\n",
      "Iteration: 1500, Loss: 305.5614318847656 for layer [ True]\n",
      "Iteration: 1600, Loss: 136.2465057373047 for layer [ True]\n",
      "Iteration: 1700, Loss: 125.38670349121094 for layer [ True]\n",
      "Iteration: 1800, Loss: 610.4712524414062 for layer [False]\n",
      "Iteration: 1900, Loss: 118.81246948242188 for layer [ True]\n",
      "Iteration: 2000, Loss: 105.80500030517578 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.32164001464844 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.90872192382812 for layer [ True]\n",
      "Iteration: 2300, Loss: 449.8938293457031 for layer [False]\n",
      "Iteration: 2400, Loss: 131.3929901123047 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.49148559570312 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.17527770996094 for layer [ True]\n",
      "Iteration: 2700, Loss: 91.47508239746094 for layer [ True]\n",
      "Iteration: 2800, Loss: 325.3751220703125 for layer [False]\n",
      "Iteration: 2900, Loss: 249.67002868652344 for layer [False]\n",
      "Iteration: 3000, Loss: 80.39327239990234 for layer [ True]\n",
      "Iteration: 3100, Loss: 218.91148376464844 for layer [False]\n",
      "Iteration: 3200, Loss: 226.14804077148438 for layer [False]\n",
      "Iteration: 3300, Loss: 154.42811584472656 for layer [False]\n",
      "Iteration: 3400, Loss: 72.93672943115234 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.2434539794922 for layer [False]\n",
      "Iteration: 3600, Loss: 61.425540924072266 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.5040283203125 for layer [False]\n",
      "Iteration: 3800, Loss: 113.41973114013672 for layer [False]\n",
      "Iteration: 3900, Loss: 43.683963775634766 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.11085891723633 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.58498001098633 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.42108154296875 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.37071990966797 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.102989196777344 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.41265106201172 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.29541015625 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.751556396484375 for layer [False]\n",
      "Iteration: 4800, Loss: 39.40294647216797 for layer [False]\n",
      "Iteration: 4900, Loss: 49.68880081176758 for layer [False]\n",
      "Iteration: 5000, Loss: 56.90909194946289 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.617204666137695 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.8729133605957 for layer [False]\n",
      "Iteration: 5300, Loss: 22.709020614624023 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.989301681518555 for layer [False]\n",
      "Iteration: 5500, Loss: 24.648412704467773 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.479248046875 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.21666145324707 for layer [False]\n",
      "Iteration: 5800, Loss: 21.890960693359375 for layer [False]\n",
      "Iteration: 5900, Loss: 13.964329719543457 for layer [False]\n",
      "Iteration: 6000, Loss: 13.844195365905762 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.644750595092773 for layer [False]\n",
      "Iteration: 6200, Loss: 17.92299461364746 for layer [False]\n",
      "Iteration: 6300, Loss: 15.240694046020508 for layer [False]\n",
      "Iteration: 6400, Loss: 16.340177536010742 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.615104675292969 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.824728965759277 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.96206283569336 for layer [False]\n",
      "Iteration: 6800, Loss: 5.122289180755615 for layer [ True]\n",
      "Iteration: 6900, Loss: 44.5107307434082 for layer [False]\n",
      "Iteration: 7000, Loss: 10.722281455993652 for layer [False]\n",
      "Iteration: 7100, Loss: 12.057809829711914 for layer [False]\n",
      "Iteration: 7200, Loss: 17.44606590270996 for layer [False]\n",
      "Iteration: 7300, Loss: 4.7202959060668945 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.81260871887207 for layer [False]\n",
      "Iteration: 7500, Loss: 4.420445919036865 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.215860366821289 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.30652904510498 for layer [False]\n",
      "Iteration: 7800, Loss: 10.071142196655273 for layer [False]\n",
      "Iteration: 7900, Loss: 5.475022315979004 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.795332670211792 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.302452325820923 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.8374834060668945 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.81823444366455 for layer [False]\n",
      "Iteration: 8400, Loss: 2.025362253189087 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.909417152404785 for layer [False]\n",
      "Iteration: 8600, Loss: 7.922945499420166 for layer [False]\n",
      "Iteration: 8700, Loss: 2.717381238937378 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.421502113342285 for layer [False]\n",
      "Iteration: 8900, Loss: 9.204849243164062 for layer [False]\n",
      "Iteration: 9000, Loss: 1.490061640739441 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5711127519607544 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8875335454940796 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.531411170959473 for layer [False]\n",
      "Iteration: 9400, Loss: 7.952179431915283 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5225707292556763 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.941228151321411 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0652893781661987 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8976776599884033 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.446608066558838 for layer [False]\n",
      "Iteration: 10000, Loss: 3.5726726055145264 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2115706205368042 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7341647148132324 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.582967281341553 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5394740104675293 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5696345567703247 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.5241591930389404 for layer [False]\n",
      "Iteration: 10700, Loss: 0.717668890953064 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.260370254516602 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5231742262840271 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.814353942871094 for layer [False]\n",
      "Iteration: 11100, Loss: 2.9133126735687256 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3607337176799774 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.45330241322517395 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3565911650657654 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.820730209350586 for layer [False]\n",
      "Iteration: 11600, Loss: 0.380879282951355 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.20033729076385498 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2660004794597626 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2726956605911255 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11980117857456207 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18913054466247559 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1719971001148224 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10557672381401062 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15844132006168365 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.854226112365723 for layer [False]\n",
      "Iteration: 12600, Loss: 5.730259895324707 for layer [False]\n",
      "Iteration: 12700, Loss: 4.581255912780762 for layer [False]\n",
      "Iteration: 12800, Loss: 0.1059163436293602 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09605753421783447 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06376101076602936 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.925632953643799 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06619846820831299 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.687047958374023 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04780010133981705 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06274697184562683 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.0489148385822773 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016617199406027794 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.037842757999897 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.031651999801397324 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.579545021057129 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029660798609256744 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.136784553527832 for layer [False]\n",
      "Iteration: 14300, Loss: 0.01086254883557558 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.32680606842041 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2411739826202393 for layer [False]\n",
      "Iteration: 14600, Loss: 3.283964157104492 for layer [False]\n",
      "Iteration: 14700, Loss: 3.26046085357666 for layer [False]\n",
      "Iteration: 14800, Loss: 3.882659912109375 for layer [False]\n",
      "Iteration: 14900, Loss: 6.634134292602539 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006736136507242918 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.448681354522705 for layer [False]\n",
      "Iteration: 15200, Loss: 9.780975341796875 for layer [False]\n",
      "Iteration: 15300, Loss: 4.7243804931640625 for layer [False]\n",
      "Iteration: 15400, Loss: 2.299186944961548 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033859917894005775 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004338132683187723 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004196099005639553 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.9042904376983643 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0022686657030135393 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0035532366018742323 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.1775639057159424 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011934589128941298 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021751970052719116 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.389312744140625 for layer [False]\n",
      "Iteration: 16500, Loss: 4.79514217376709 for layer [False]\n",
      "Iteration: 16600, Loss: 3.385948657989502 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014134153025224805 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008053117780946195 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010364637710154057 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.000935200194362551 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.4607295989990234 for layer [False]\n",
      "Iteration: 17200, Loss: 3.1455461978912354 for layer [False]\n",
      "Iteration: 17300, Loss: 0.000675258575938642 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.506125450134277 for layer [False]\n",
      "Iteration: 17500, Loss: 11.073995590209961 for layer [False]\n",
      "Iteration: 17600, Loss: 3.6225671768188477 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008993708761408925 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.5587573051452637 for layer [False]\n",
      "Iteration: 17900, Loss: 4.308644771575928 for layer [False]\n",
      "Iteration: 18000, Loss: 2.5406224727630615 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007180125685408711 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008724711369723082 for layer [ True]\n",
      "Iteration: 18300, Loss: 6.0504021644592285 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0003844616876449436 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0008134528761729598 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.1433348655700684 for layer [False]\n",
      "Iteration: 18700, Loss: 4.091305255889893 for layer [False]\n",
      "Iteration: 18800, Loss: 4.758093357086182 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8768163919448853 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00025275882217101753 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.290830612182617 for layer [False]\n",
      "Iteration: 19200, Loss: 0.001852910383604467 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.5493462085723877 for layer [False]\n",
      "Iteration: 19400, Loss: 6.298305511474609 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007198141538538039 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003340328694321215 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.083659648895264 for layer [False]\n",
      "Iteration: 19800, Loss: 3.4335060119628906 for layer [False]\n",
      "Iteration: 19900, Loss: 3.851945638656616 for layer [False]\n",
      "Iteration: 20000, Loss: 2.5192782878875732 for layer [False]\n",
      "Iteration: 20100, Loss: 5.176926136016846 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0014631371013820171 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.025618314743042 for layer [False]\n",
      "Iteration: 20400, Loss: 6.789996405132115e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00038464198587462306 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009874608367681503 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.814197301864624 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0003856079711113125 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0006802963907830417 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.038888692855835 for layer [False]\n",
      "Iteration: 21100, Loss: 8.939047813415527 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00037771501229144633 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.648969650268555 for layer [False]\n",
      "Iteration: 21400, Loss: 0.000593793170992285 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.4890894889831543 for layer [False]\n",
      "Iteration: 21600, Loss: 4.512165546417236 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00019651585898827761 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0004759330186061561 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.1149439811706543 for layer [False]\n",
      "Iteration: 22000, Loss: 4.093320369720459 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0001730040821712464 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00045262734056450427 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.332764148712158 for layer [False]\n",
      "Iteration: 22400, Loss: 5.47504997253418 for layer [False]\n",
      "Iteration: 22500, Loss: 1.8189343214035034 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0004372818802949041 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.0001582065742695704 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0008471569744870067 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0004618381499312818 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0001840257173171267 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0014029694721102715 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00026308716041967273 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00012386596063151956 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002387713175266981 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.0004296317929401994 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00020530617621261626 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.1665802001953125 for layer [False]\n",
      "Iteration: 23800, Loss: 0.00042097753612324595 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00034131354186683893 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0005832117749378085 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.00019252233323641121 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.9760680198669434 for layer [False]\n",
      "Iteration: 24300, Loss: 2.609936237335205 for layer [False]\n",
      "Iteration: 24400, Loss: 4.430498123168945 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00013597100041806698 for layer [ True]\n",
      "Iteration: 24600, Loss: 8.160479046637192e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.388383865356445 for layer [False]\n",
      "Iteration: 24800, Loss: 4.146916389465332 for layer [False]\n",
      "Iteration: 24900, Loss: 0.0003164657100569457 for layer [ True]\n",
      "Step 16000 | Loss: 0.000446\n",
      "Step 16100 | Loss: 0.000445\n",
      "Step 16200 | Loss: 0.000445\n",
      "Step 16300 | Loss: 0.000444\n",
      "Step 16400 | Loss: 0.000444\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1928.3988037109375 for layer [False]\n",
      "Iteration: 100, Loss: 320.8708190917969 for layer [ True]\n",
      "Iteration: 200, Loss: 1924.8416748046875 for layer [False]\n",
      "Iteration: 300, Loss: 1535.466796875 for layer [False]\n",
      "Iteration: 400, Loss: 238.7938690185547 for layer [ True]\n",
      "Iteration: 500, Loss: 313.3144836425781 for layer [ True]\n",
      "Iteration: 600, Loss: 1437.3548583984375 for layer [False]\n",
      "Iteration: 700, Loss: 224.2664031982422 for layer [ True]\n",
      "Iteration: 800, Loss: 1152.1160888671875 for layer [False]\n",
      "Iteration: 900, Loss: 1013.1508178710938 for layer [False]\n",
      "Iteration: 1000, Loss: 152.79530334472656 for layer [ True]\n",
      "Iteration: 1100, Loss: 886.806396484375 for layer [False]\n",
      "Iteration: 1200, Loss: 1051.651611328125 for layer [False]\n",
      "Iteration: 1300, Loss: 986.04052734375 for layer [False]\n",
      "Iteration: 1400, Loss: 244.15066528320312 for layer [ True]\n",
      "Iteration: 1500, Loss: 306.58489990234375 for layer [ True]\n",
      "Iteration: 1600, Loss: 135.76776123046875 for layer [ True]\n",
      "Iteration: 1700, Loss: 125.37100982666016 for layer [ True]\n",
      "Iteration: 1800, Loss: 610.1986694335938 for layer [False]\n",
      "Iteration: 1900, Loss: 119.16586303710938 for layer [ True]\n",
      "Iteration: 2000, Loss: 105.83427429199219 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.42727661132812 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.98434448242188 for layer [ True]\n",
      "Iteration: 2300, Loss: 450.26519775390625 for layer [False]\n",
      "Iteration: 2400, Loss: 131.94883728027344 for layer [ True]\n",
      "Iteration: 2500, Loss: 188.1150360107422 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.35987854003906 for layer [ True]\n",
      "Iteration: 2700, Loss: 91.62278747558594 for layer [ True]\n",
      "Iteration: 2800, Loss: 325.3523254394531 for layer [False]\n",
      "Iteration: 2900, Loss: 249.6040802001953 for layer [False]\n",
      "Iteration: 3000, Loss: 80.2884292602539 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.34410095214844 for layer [False]\n",
      "Iteration: 3200, Loss: 226.11737060546875 for layer [False]\n",
      "Iteration: 3300, Loss: 154.04708862304688 for layer [False]\n",
      "Iteration: 3400, Loss: 72.89273071289062 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.0263214111328 for layer [False]\n",
      "Iteration: 3600, Loss: 61.44434356689453 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.4704132080078 for layer [False]\n",
      "Iteration: 3800, Loss: 113.3992919921875 for layer [False]\n",
      "Iteration: 3900, Loss: 43.60172653198242 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.20951461791992 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.72181701660156 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.48217010498047 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.242889404296875 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.090232849121094 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.280086517333984 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.28380584716797 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.66861343383789 for layer [False]\n",
      "Iteration: 4800, Loss: 39.20983123779297 for layer [False]\n",
      "Iteration: 4900, Loss: 49.72938537597656 for layer [False]\n",
      "Iteration: 5000, Loss: 56.83911895751953 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.62877082824707 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.9854736328125 for layer [False]\n",
      "Iteration: 5300, Loss: 22.71131706237793 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.92068099975586 for layer [False]\n",
      "Iteration: 5500, Loss: 24.564640045166016 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.413227081298828 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.225849151611328 for layer [False]\n",
      "Iteration: 5800, Loss: 21.81195068359375 for layer [False]\n",
      "Iteration: 5900, Loss: 13.851303100585938 for layer [False]\n",
      "Iteration: 6000, Loss: 13.84935474395752 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.545297622680664 for layer [False]\n",
      "Iteration: 6200, Loss: 17.82428741455078 for layer [False]\n",
      "Iteration: 6300, Loss: 15.201748847961426 for layer [False]\n",
      "Iteration: 6400, Loss: 16.256534576416016 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.622631072998047 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.803398132324219 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.42641830444336 for layer [False]\n",
      "Iteration: 6800, Loss: 5.110910892486572 for layer [ True]\n",
      "Iteration: 6900, Loss: 44.66102981567383 for layer [False]\n",
      "Iteration: 7000, Loss: 10.621889114379883 for layer [False]\n",
      "Iteration: 7100, Loss: 12.007115364074707 for layer [False]\n",
      "Iteration: 7200, Loss: 17.16375732421875 for layer [False]\n",
      "Iteration: 7300, Loss: 4.708182334899902 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.86414337158203 for layer [False]\n",
      "Iteration: 7500, Loss: 4.4240851402282715 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.2130303382873535 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.398365020751953 for layer [False]\n",
      "Iteration: 7800, Loss: 10.077797889709473 for layer [False]\n",
      "Iteration: 7900, Loss: 5.445641994476318 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.7804458141326904 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.292027473449707 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.837906837463379 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.694764137268066 for layer [False]\n",
      "Iteration: 8400, Loss: 2.019981861114502 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.954062461853027 for layer [False]\n",
      "Iteration: 8600, Loss: 7.97214412689209 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6978204250335693 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.436912536621094 for layer [False]\n",
      "Iteration: 8900, Loss: 9.094106674194336 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4969065189361572 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5565171241760254 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.886186957359314 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.547706127166748 for layer [False]\n",
      "Iteration: 9400, Loss: 7.989715576171875 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5087918043136597 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.920933246612549 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0502928495407104 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8945326209068298 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.385136604309082 for layer [False]\n",
      "Iteration: 10000, Loss: 3.5570263862609863 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2021477222442627 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7327502369880676 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.5384521484375 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5341673493385315 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5632845759391785 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4950225353240967 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7115459442138672 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.25790548324585 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5198500156402588 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.782480239868164 for layer [False]\n",
      "Iteration: 11100, Loss: 2.8774569034576416 for layer [False]\n",
      "Iteration: 11200, Loss: 0.35670584440231323 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.45027318596839905 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3556917607784271 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.81703519821167 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3803982436656952 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19911977648735046 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.26391327381134033 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2727769911289215 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1196761280298233 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18547721207141876 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.17002460360527039 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10512646287679672 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15901756286621094 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.864236354827881 for layer [False]\n",
      "Iteration: 12600, Loss: 5.692811965942383 for layer [False]\n",
      "Iteration: 12700, Loss: 4.5907440185546875 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10454431176185608 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09559421241283417 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.0626252293586731 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.8989410400390625 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06597422808408737 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.720108985900879 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04756004363298416 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06232023984193802 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04841006547212601 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016315685585141182 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03781883046030998 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03142235055565834 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.557464122772217 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029309934005141258 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.148282527923584 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010689595714211464 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.310007333755493 for layer [False]\n",
      "Iteration: 14500, Loss: 2.230783224105835 for layer [False]\n",
      "Iteration: 14600, Loss: 3.227736234664917 for layer [False]\n",
      "Iteration: 14700, Loss: 3.2217369079589844 for layer [False]\n",
      "Iteration: 14800, Loss: 3.8279495239257812 for layer [False]\n",
      "Iteration: 14900, Loss: 6.485005855560303 for layer [False]\n",
      "Iteration: 15000, Loss: 0.00661052018404007 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.338090896606445 for layer [False]\n",
      "Iteration: 15200, Loss: 9.773478507995605 for layer [False]\n",
      "Iteration: 15300, Loss: 4.693687438964844 for layer [False]\n",
      "Iteration: 15400, Loss: 2.2803053855895996 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033562725875526667 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004282149486243725 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004186616279184818 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.9221670627593994 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002229992998763919 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0034291394986212254 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.151580810546875 for layer [False]\n",
      "Iteration: 16200, Loss: 0.00118351134005934 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.002128191292285919 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.345618724822998 for layer [False]\n",
      "Iteration: 16500, Loss: 4.82966423034668 for layer [False]\n",
      "Iteration: 16600, Loss: 3.4307024478912354 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014209045330062509 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008232913096435368 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010169356828555465 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008987761102616787 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.4491288661956787 for layer [False]\n",
      "Iteration: 17200, Loss: 3.1111629009246826 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006558677414432168 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.387727737426758 for layer [False]\n",
      "Iteration: 17500, Loss: 11.213810920715332 for layer [False]\n",
      "Iteration: 17600, Loss: 3.5847384929656982 for layer [False]\n",
      "Iteration: 17700, Loss: 0.000878223218023777 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.5683977603912354 for layer [False]\n",
      "Iteration: 17900, Loss: 4.278618812561035 for layer [False]\n",
      "Iteration: 18000, Loss: 2.585768699645996 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006923566688783467 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008464670972898602 for layer [ True]\n",
      "Iteration: 18300, Loss: 6.106450080871582 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0003837066178675741 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007856882293708622 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.148242712020874 for layer [False]\n",
      "Iteration: 18700, Loss: 4.116810321807861 for layer [False]\n",
      "Iteration: 18800, Loss: 4.749520778656006 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8648263216018677 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00029277251451276243 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.2594053745269775 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0016936735482886434 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.525336742401123 for layer [False]\n",
      "Iteration: 19400, Loss: 6.285027503967285 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007202367996796966 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003172735741827637 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.08968448638916 for layer [False]\n",
      "Iteration: 19800, Loss: 3.373521327972412 for layer [False]\n",
      "Iteration: 19900, Loss: 3.827798366546631 for layer [False]\n",
      "Iteration: 20000, Loss: 2.4762120246887207 for layer [False]\n",
      "Iteration: 20100, Loss: 5.055824279785156 for layer [False]\n",
      "Iteration: 20200, Loss: 0.00146686309017241 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.0299618244171143 for layer [False]\n",
      "Iteration: 20400, Loss: 6.63613245706074e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003794041112996638 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009182822541333735 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.809161901473999 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00038633766234852374 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0006808782345615327 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.097212553024292 for layer [False]\n",
      "Iteration: 21100, Loss: 8.958891868591309 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00027169004897587 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.59865140914917 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0004242743016220629 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.464717388153076 for layer [False]\n",
      "Iteration: 21600, Loss: 4.453002452850342 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00037141668144613504 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0003154190198983997 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.077439308166504 for layer [False]\n",
      "Iteration: 22000, Loss: 4.01049280166626 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00013553812459576875 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0003545317449606955 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.342504978179932 for layer [False]\n",
      "Iteration: 22400, Loss: 5.399730205535889 for layer [False]\n",
      "Iteration: 22500, Loss: 1.8166154623031616 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00019029300892725587 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00014772042050026357 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00031417908030562103 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0008392139570787549 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0009615197777748108 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.00014204038598109037 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0002649035886861384 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0004992096219211817 for layer [ True]\n",
      "Iteration: 23400, Loss: 7.787968206685036e-05 for layer [ True]\n",
      "Iteration: 23500, Loss: 6.247151759453118e-05 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0004673923831433058 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.0889434814453125 for layer [False]\n",
      "Iteration: 23800, Loss: 0.001099045155569911 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00016590423183515668 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.000622571271378547 for layer [ True]\n",
      "Iteration: 24100, Loss: 8.682371844770387e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 4.002518177032471 for layer [False]\n",
      "Iteration: 24300, Loss: 2.6224052906036377 for layer [False]\n",
      "Iteration: 24400, Loss: 4.515195846557617 for layer [False]\n",
      "Iteration: 24500, Loss: 3.47036802850198e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 6.630183634115383e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.352929592132568 for layer [False]\n",
      "Iteration: 24800, Loss: 4.1454997062683105 for layer [False]\n",
      "Iteration: 24900, Loss: 6.243241659831256e-05 for layer [ True]\n",
      "Step 16500 | Loss: 0.000444\n",
      "Step 16600 | Loss: 0.000444\n",
      "Step 16700 | Loss: 0.000444\n",
      "Step 16800 | Loss: 0.000444\n",
      "Step 16900 | Loss: 0.000444\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1929.2904052734375 for layer [False]\n",
      "Iteration: 100, Loss: 319.980712890625 for layer [ True]\n",
      "Iteration: 200, Loss: 1925.9910888671875 for layer [False]\n",
      "Iteration: 300, Loss: 1536.0516357421875 for layer [False]\n",
      "Iteration: 400, Loss: 238.87789916992188 for layer [ True]\n",
      "Iteration: 500, Loss: 312.238037109375 for layer [ True]\n",
      "Iteration: 600, Loss: 1437.007568359375 for layer [False]\n",
      "Iteration: 700, Loss: 223.54530334472656 for layer [ True]\n",
      "Iteration: 800, Loss: 1150.8465576171875 for layer [False]\n",
      "Iteration: 900, Loss: 1013.6586303710938 for layer [False]\n",
      "Iteration: 1000, Loss: 152.89108276367188 for layer [ True]\n",
      "Iteration: 1100, Loss: 886.8253173828125 for layer [False]\n",
      "Iteration: 1200, Loss: 1052.2884521484375 for layer [False]\n",
      "Iteration: 1300, Loss: 986.3248901367188 for layer [False]\n",
      "Iteration: 1400, Loss: 243.5564422607422 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.891357421875 for layer [ True]\n",
      "Iteration: 1600, Loss: 135.47412109375 for layer [ True]\n",
      "Iteration: 1700, Loss: 124.99940490722656 for layer [ True]\n",
      "Iteration: 1800, Loss: 610.2469482421875 for layer [False]\n",
      "Iteration: 1900, Loss: 118.61162567138672 for layer [ True]\n",
      "Iteration: 2000, Loss: 105.50569152832031 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.0604705810547 for layer [ True]\n",
      "Iteration: 2200, Loss: 108.75176239013672 for layer [ True]\n",
      "Iteration: 2300, Loss: 450.20025634765625 for layer [False]\n",
      "Iteration: 2400, Loss: 131.71466064453125 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.7380828857422 for layer [ True]\n",
      "Iteration: 2600, Loss: 120.82405853271484 for layer [ True]\n",
      "Iteration: 2700, Loss: 91.48573303222656 for layer [ True]\n",
      "Iteration: 2800, Loss: 325.6833801269531 for layer [False]\n",
      "Iteration: 2900, Loss: 249.41964721679688 for layer [False]\n",
      "Iteration: 3000, Loss: 80.12210845947266 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.2989044189453 for layer [False]\n",
      "Iteration: 3200, Loss: 225.80807495117188 for layer [False]\n",
      "Iteration: 3300, Loss: 154.37173461914062 for layer [False]\n",
      "Iteration: 3400, Loss: 72.66838073730469 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.1717071533203 for layer [False]\n",
      "Iteration: 3600, Loss: 61.26527786254883 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.3903045654297 for layer [False]\n",
      "Iteration: 3800, Loss: 113.32147216796875 for layer [False]\n",
      "Iteration: 3900, Loss: 43.3160285949707 for layer [ True]\n",
      "Iteration: 4000, Loss: 50.87416458129883 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.42376708984375 for layer [ True]\n",
      "Iteration: 4200, Loss: 77.94071960449219 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.02919006347656 for layer [ True]\n",
      "Iteration: 4400, Loss: 41.886756896972656 for layer [ True]\n",
      "Iteration: 4500, Loss: 53.82917404174805 for layer [ True]\n",
      "Iteration: 4600, Loss: 38.86554718017578 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.59621047973633 for layer [False]\n",
      "Iteration: 4800, Loss: 39.23346710205078 for layer [False]\n",
      "Iteration: 4900, Loss: 49.607025146484375 for layer [False]\n",
      "Iteration: 5000, Loss: 56.23487091064453 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.49443244934082 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.99903106689453 for layer [False]\n",
      "Iteration: 5300, Loss: 22.47855567932129 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.981557846069336 for layer [False]\n",
      "Iteration: 5500, Loss: 24.242284774780273 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.093900680541992 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.05724334716797 for layer [False]\n",
      "Iteration: 5800, Loss: 21.87194061279297 for layer [False]\n",
      "Iteration: 5900, Loss: 13.937588691711426 for layer [False]\n",
      "Iteration: 6000, Loss: 13.724017143249512 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.496673583984375 for layer [False]\n",
      "Iteration: 6200, Loss: 18.051416397094727 for layer [False]\n",
      "Iteration: 6300, Loss: 15.229559898376465 for layer [False]\n",
      "Iteration: 6400, Loss: 16.085840225219727 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.55469036102295 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.697025299072266 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.61492347717285 for layer [False]\n",
      "Iteration: 6800, Loss: 5.036604404449463 for layer [ True]\n",
      "Iteration: 6900, Loss: 44.77043533325195 for layer [False]\n",
      "Iteration: 7000, Loss: 10.648299217224121 for layer [False]\n",
      "Iteration: 7100, Loss: 12.081369400024414 for layer [False]\n",
      "Iteration: 7200, Loss: 17.225175857543945 for layer [False]\n",
      "Iteration: 7300, Loss: 4.657061576843262 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.966554641723633 for layer [False]\n",
      "Iteration: 7500, Loss: 4.381530284881592 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.1659698486328125 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.252930641174316 for layer [False]\n",
      "Iteration: 7800, Loss: 10.169927597045898 for layer [False]\n",
      "Iteration: 7900, Loss: 5.384293079376221 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.727083921432495 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2674520015716553 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.801743984222412 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.697553634643555 for layer [False]\n",
      "Iteration: 8400, Loss: 2.002732276916504 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.86793327331543 for layer [False]\n",
      "Iteration: 8600, Loss: 7.97048807144165 for layer [False]\n",
      "Iteration: 8700, Loss: 2.671090841293335 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.407011985778809 for layer [False]\n",
      "Iteration: 8900, Loss: 9.145492553710938 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4871057271957397 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5340371131896973 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.874355435371399 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.5443902015686035 for layer [False]\n",
      "Iteration: 9400, Loss: 7.995485782623291 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4786208868026733 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.9041929244995117 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0454341173171997 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8861996531486511 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.3421149253845215 for layer [False]\n",
      "Iteration: 10000, Loss: 3.5864226818084717 for layer [False]\n",
      "Iteration: 10100, Loss: 1.183242678642273 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7206434607505798 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.5769524574279785 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5328318476676941 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5551865100860596 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4924004077911377 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7030830979347229 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.217201232910156 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5133733749389648 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.834006309509277 for layer [False]\n",
      "Iteration: 11100, Loss: 2.8587121963500977 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3512040972709656 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4462483525276184 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3535459041595459 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.8249785900115967 for layer [False]\n",
      "Iteration: 11600, Loss: 0.374979704618454 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.19817835092544556 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2621237635612488 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2699701488018036 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.11973404884338379 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18387407064437866 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.169142484664917 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10490843653678894 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.15697135031223297 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.864513874053955 for layer [False]\n",
      "Iteration: 12600, Loss: 5.640572547912598 for layer [False]\n",
      "Iteration: 12700, Loss: 4.573520183563232 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10404087603092194 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09510813653469086 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06240370124578476 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.909394264221191 for layer [False]\n",
      "Iteration: 13200, Loss: 0.0654110386967659 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.737179756164551 for layer [False]\n",
      "Iteration: 13400, Loss: 0.047927454113960266 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06222337484359741 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04832955077290535 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016328521072864532 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.037914879620075226 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03146069124341011 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.5574018955230713 for layer [False]\n",
      "Iteration: 14100, Loss: 0.0292205773293972 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.119067668914795 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010783778503537178 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.3261184692382812 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2292749881744385 for layer [False]\n",
      "Iteration: 14600, Loss: 3.2331292629241943 for layer [False]\n",
      "Iteration: 14700, Loss: 3.238675832748413 for layer [False]\n",
      "Iteration: 14800, Loss: 3.8271825313568115 for layer [False]\n",
      "Iteration: 14900, Loss: 6.522317886352539 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006602068897336721 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.374215602874756 for layer [False]\n",
      "Iteration: 15200, Loss: 9.802803993225098 for layer [False]\n",
      "Iteration: 15300, Loss: 4.656695365905762 for layer [False]\n",
      "Iteration: 15400, Loss: 2.315896511077881 for layer [False]\n",
      "Iteration: 15500, Loss: 0.003327741054818034 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004255491774529219 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004135977942496538 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.9431564807891846 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0022201298270374537 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003424018621444702 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.159205198287964 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011764797382056713 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021331612952053547 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.381479740142822 for layer [False]\n",
      "Iteration: 16500, Loss: 4.826387405395508 for layer [False]\n",
      "Iteration: 16600, Loss: 3.422219753265381 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014086032751947641 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007784620393067598 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.001012412365525961 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009057183633558452 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.4673027992248535 for layer [False]\n",
      "Iteration: 17200, Loss: 3.1387226581573486 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006627795519307256 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.450613021850586 for layer [False]\n",
      "Iteration: 17500, Loss: 10.987698554992676 for layer [False]\n",
      "Iteration: 17600, Loss: 3.6131930351257324 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008728841785341501 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.567279577255249 for layer [False]\n",
      "Iteration: 17900, Loss: 4.3238019943237305 for layer [False]\n",
      "Iteration: 18000, Loss: 2.5855231285095215 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007058582850731909 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008625416085124016 for layer [ True]\n",
      "Iteration: 18300, Loss: 6.022792816162109 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0003780447877943516 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.000792155449744314 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.136995553970337 for layer [False]\n",
      "Iteration: 18700, Loss: 4.086207866668701 for layer [False]\n",
      "Iteration: 18800, Loss: 4.842350482940674 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8823837041854858 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0003002490266226232 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.2418174743652344 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0017124027945101261 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.528247117996216 for layer [False]\n",
      "Iteration: 19400, Loss: 6.307379722595215 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007104281685315073 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.000320090854074806 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.075880527496338 for layer [False]\n",
      "Iteration: 19800, Loss: 3.418429374694824 for layer [False]\n",
      "Iteration: 19900, Loss: 3.8229012489318848 for layer [False]\n",
      "Iteration: 20000, Loss: 2.5047459602355957 for layer [False]\n",
      "Iteration: 20100, Loss: 5.089400291442871 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0014543818542733788 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.0206613540649414 for layer [False]\n",
      "Iteration: 20400, Loss: 6.424357707146555e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003828776825685054 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0009583955979906023 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.814620018005371 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00037814481765963137 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0006300226668827236 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.092667818069458 for layer [False]\n",
      "Iteration: 21100, Loss: 8.996228218078613 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00023854864411987364 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.666637420654297 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00038244586903601885 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.4695467948913574 for layer [False]\n",
      "Iteration: 21600, Loss: 4.493019104003906 for layer [False]\n",
      "Iteration: 21700, Loss: 7.741325680399314e-05 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0005757274921052158 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.1238784790039062 for layer [False]\n",
      "Iteration: 22000, Loss: 4.071549415588379 for layer [False]\n",
      "Iteration: 22100, Loss: 0.000352600502083078 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00012339600652921945 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.3697381019592285 for layer [False]\n",
      "Iteration: 22400, Loss: 5.421487808227539 for layer [False]\n",
      "Iteration: 22500, Loss: 1.8086256980895996 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0001469175040256232 for layer [ True]\n",
      "Iteration: 22700, Loss: 5.0439408369129524e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00029290455859154463 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.00036167504731565714 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00043682282557711005 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0022326912730932236 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00017363260849379003 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0005591827793978155 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0001393660350004211 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00012208025145810097 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0005477180820889771 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.1561453342437744 for layer [False]\n",
      "Iteration: 23800, Loss: 0.00029353032005019486 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00023278444132301956 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0001544070546515286 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.00012316089123487473 for layer [ True]\n",
      "Iteration: 24200, Loss: 4.021486759185791 for layer [False]\n",
      "Iteration: 24300, Loss: 2.613391637802124 for layer [False]\n",
      "Iteration: 24400, Loss: 4.512886047363281 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00014837784692645073 for layer [ True]\n",
      "Iteration: 24600, Loss: 5.8709698350867257e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.397512912750244 for layer [False]\n",
      "Iteration: 24800, Loss: 4.161787509918213 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00017207024211529642 for layer [ True]\n",
      "Step 17000 | Loss: 0.000444\n",
      "Step 17100 | Loss: 0.000445\n",
      "Step 17200 | Loss: 0.000444\n",
      "Step 17300 | Loss: 0.000444\n",
      "Step 17400 | Loss: 0.000444\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1928.0726318359375 for layer [False]\n",
      "Iteration: 100, Loss: 321.1777038574219 for layer [ True]\n",
      "Iteration: 200, Loss: 1924.5399169921875 for layer [False]\n",
      "Iteration: 300, Loss: 1536.8839111328125 for layer [False]\n",
      "Iteration: 400, Loss: 238.93917846679688 for layer [ True]\n",
      "Iteration: 500, Loss: 313.3963317871094 for layer [ True]\n",
      "Iteration: 600, Loss: 1437.2763671875 for layer [False]\n",
      "Iteration: 700, Loss: 224.47808837890625 for layer [ True]\n",
      "Iteration: 800, Loss: 1150.99658203125 for layer [False]\n",
      "Iteration: 900, Loss: 1013.1265258789062 for layer [False]\n",
      "Iteration: 1000, Loss: 152.37734985351562 for layer [ True]\n",
      "Iteration: 1100, Loss: 887.0807495117188 for layer [False]\n",
      "Iteration: 1200, Loss: 1053.0152587890625 for layer [False]\n",
      "Iteration: 1300, Loss: 985.5910034179688 for layer [False]\n",
      "Iteration: 1400, Loss: 245.31260681152344 for layer [ True]\n",
      "Iteration: 1500, Loss: 306.16717529296875 for layer [ True]\n",
      "Iteration: 1600, Loss: 135.92767333984375 for layer [ True]\n",
      "Iteration: 1700, Loss: 126.1943359375 for layer [ True]\n",
      "Iteration: 1800, Loss: 610.0987548828125 for layer [False]\n",
      "Iteration: 1900, Loss: 119.9552993774414 for layer [ True]\n",
      "Iteration: 2000, Loss: 106.60540008544922 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.7389678955078 for layer [ True]\n",
      "Iteration: 2200, Loss: 109.67990112304688 for layer [ True]\n",
      "Iteration: 2300, Loss: 449.59954833984375 for layer [False]\n",
      "Iteration: 2400, Loss: 132.44869995117188 for layer [ True]\n",
      "Iteration: 2500, Loss: 189.18405151367188 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.80522918701172 for layer [ True]\n",
      "Iteration: 2700, Loss: 92.11457824707031 for layer [ True]\n",
      "Iteration: 2800, Loss: 325.6869812011719 for layer [False]\n",
      "Iteration: 2900, Loss: 249.1110076904297 for layer [False]\n",
      "Iteration: 3000, Loss: 80.71105194091797 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.28794860839844 for layer [False]\n",
      "Iteration: 3200, Loss: 225.7876739501953 for layer [False]\n",
      "Iteration: 3300, Loss: 153.9882354736328 for layer [False]\n",
      "Iteration: 3400, Loss: 73.30531311035156 for layer [ True]\n",
      "Iteration: 3500, Loss: 185.96432495117188 for layer [False]\n",
      "Iteration: 3600, Loss: 61.94144058227539 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.24221801757812 for layer [False]\n",
      "Iteration: 3800, Loss: 113.3036880493164 for layer [False]\n",
      "Iteration: 3900, Loss: 43.810428619384766 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.66682052612305 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.914188385009766 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.79988861083984 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.43988037109375 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.25699996948242 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.53990173339844 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.45969009399414 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.58095169067383 for layer [False]\n",
      "Iteration: 4800, Loss: 39.12512969970703 for layer [False]\n",
      "Iteration: 4900, Loss: 49.5045166015625 for layer [False]\n",
      "Iteration: 5000, Loss: 57.15261459350586 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.831893920898438 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.967586517333984 for layer [False]\n",
      "Iteration: 5300, Loss: 22.827438354492188 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.900007247924805 for layer [False]\n",
      "Iteration: 5500, Loss: 24.643070220947266 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.676029205322266 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.1304931640625 for layer [False]\n",
      "Iteration: 5800, Loss: 21.87594985961914 for layer [False]\n",
      "Iteration: 5900, Loss: 13.90817642211914 for layer [False]\n",
      "Iteration: 6000, Loss: 13.950640678405762 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.3582763671875 for layer [False]\n",
      "Iteration: 6200, Loss: 17.954511642456055 for layer [False]\n",
      "Iteration: 6300, Loss: 15.135502815246582 for layer [False]\n",
      "Iteration: 6400, Loss: 16.34477424621582 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.662588119506836 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.847689628601074 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.558963775634766 for layer [False]\n",
      "Iteration: 6800, Loss: 5.172824382781982 for layer [ True]\n",
      "Iteration: 6900, Loss: 45.14760208129883 for layer [False]\n",
      "Iteration: 7000, Loss: 10.75430679321289 for layer [False]\n",
      "Iteration: 7100, Loss: 12.003995895385742 for layer [False]\n",
      "Iteration: 7200, Loss: 17.244626998901367 for layer [False]\n",
      "Iteration: 7300, Loss: 4.70660400390625 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.695934295654297 for layer [False]\n",
      "Iteration: 7500, Loss: 4.447347164154053 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.235628128051758 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.330140113830566 for layer [False]\n",
      "Iteration: 7800, Loss: 10.182978630065918 for layer [False]\n",
      "Iteration: 7900, Loss: 5.458461761474609 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.780665397644043 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.289703607559204 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.843937873840332 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.811812400817871 for layer [False]\n",
      "Iteration: 8400, Loss: 2.027618885040283 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.977151870727539 for layer [False]\n",
      "Iteration: 8600, Loss: 8.019179344177246 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6999568939208984 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.457738399505615 for layer [False]\n",
      "Iteration: 8900, Loss: 9.065659523010254 for layer [False]\n",
      "Iteration: 9000, Loss: 1.501741647720337 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5608030557632446 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.891581654548645 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.5562920570373535 for layer [False]\n",
      "Iteration: 9400, Loss: 8.051125526428223 for layer [False]\n",
      "Iteration: 9500, Loss: 1.5093541145324707 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.9024910926818848 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0502525568008423 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.898750364780426 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.356977462768555 for layer [False]\n",
      "Iteration: 10000, Loss: 3.5598673820495605 for layer [False]\n",
      "Iteration: 10100, Loss: 1.2014762163162231 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.732024073600769 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.539481163024902 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5392546653747559 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5646194815635681 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.5171775817871094 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7114784717559814 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.217823028564453 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5237782001495361 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.83323860168457 for layer [False]\n",
      "Iteration: 11100, Loss: 2.8340885639190674 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3595944941043854 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.4539605975151062 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3596987724304199 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.8136706352233887 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3820686340332031 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.20131778717041016 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2669617533683777 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2752004861831665 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.12189504504203796 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18743102252483368 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1722453236579895 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10630714148283005 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.16119477152824402 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.8863444328308105 for layer [False]\n",
      "Iteration: 12600, Loss: 5.634034156799316 for layer [False]\n",
      "Iteration: 12700, Loss: 4.501430034637451 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10658667236566544 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09650371223688126 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06380606442689896 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.894458770751953 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06678739935159683 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.763377666473389 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04858293756842613 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06349016726016998 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.049158938229084015 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016786541789770126 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.038446202874183655 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03192515671253204 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.5615365505218506 for layer [False]\n",
      "Iteration: 14100, Loss: 0.02991344779729843 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.172119617462158 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010959950275719166 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.3107781410217285 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2267682552337646 for layer [False]\n",
      "Iteration: 14600, Loss: 3.241197347640991 for layer [False]\n",
      "Iteration: 14700, Loss: 3.2482457160949707 for layer [False]\n",
      "Iteration: 14800, Loss: 3.8338372707366943 for layer [False]\n",
      "Iteration: 14900, Loss: 6.609996318817139 for layer [False]\n",
      "Iteration: 15000, Loss: 0.0067827533930540085 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.369536399841309 for layer [False]\n",
      "Iteration: 15200, Loss: 9.867213249206543 for layer [False]\n",
      "Iteration: 15300, Loss: 4.702967643737793 for layer [False]\n",
      "Iteration: 15400, Loss: 2.3235361576080322 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033741488587111235 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004387900698930025 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004263132810592651 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.9570648670196533 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002278329338878393 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003495498327538371 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.18314528465271 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011763081420212984 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.002176768146455288 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.36137580871582 for layer [False]\n",
      "Iteration: 16500, Loss: 4.80047607421875 for layer [False]\n",
      "Iteration: 16600, Loss: 3.4328832626342773 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014203027822077274 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008175044786185026 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010559093207120895 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009112711995840073 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.4828593730926514 for layer [False]\n",
      "Iteration: 17200, Loss: 3.137181282043457 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006743664271198213 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.495368003845215 for layer [False]\n",
      "Iteration: 17500, Loss: 11.221182823181152 for layer [False]\n",
      "Iteration: 17600, Loss: 3.556854724884033 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008350520511157811 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.5520434379577637 for layer [False]\n",
      "Iteration: 17900, Loss: 4.336382865905762 for layer [False]\n",
      "Iteration: 18000, Loss: 2.6061019897460938 for layer [False]\n",
      "Iteration: 18100, Loss: 0.000702127639669925 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008866410353220999 for layer [ True]\n",
      "Iteration: 18300, Loss: 6.072734355926514 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0003821639693342149 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007960536167956889 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.160832405090332 for layer [False]\n",
      "Iteration: 18700, Loss: 4.155314922332764 for layer [False]\n",
      "Iteration: 18800, Loss: 4.827743053436279 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8764493465423584 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00043900927994400263 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.2541627883911133 for layer [False]\n",
      "Iteration: 19200, Loss: 0.001657032291404903 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.534492015838623 for layer [False]\n",
      "Iteration: 19400, Loss: 6.272531986236572 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007173929479904473 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003282603865955025 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.1093854904174805 for layer [False]\n",
      "Iteration: 19800, Loss: 3.3838841915130615 for layer [False]\n",
      "Iteration: 19900, Loss: 3.826953649520874 for layer [False]\n",
      "Iteration: 20000, Loss: 2.513854503631592 for layer [False]\n",
      "Iteration: 20100, Loss: 5.044833660125732 for layer [False]\n",
      "Iteration: 20200, Loss: 0.001712108962237835 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.037083864212036 for layer [False]\n",
      "Iteration: 20400, Loss: 7.12860855855979e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003438034327700734 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0007935686153359711 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.847290515899658 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00042500952258706093 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0010986358392983675 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.103933095932007 for layer [False]\n",
      "Iteration: 21100, Loss: 9.100543022155762 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0006520779570564628 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.652516841888428 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00014503742568194866 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.5194759368896484 for layer [False]\n",
      "Iteration: 21600, Loss: 4.507506847381592 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003663889365270734 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0003038268769159913 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.120544672012329 for layer [False]\n",
      "Iteration: 22000, Loss: 4.040968418121338 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00039527463377453387 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00023536905064247549 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.373359680175781 for layer [False]\n",
      "Iteration: 22400, Loss: 5.436074256896973 for layer [False]\n",
      "Iteration: 22500, Loss: 1.7984172105789185 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0002779078495223075 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00017135053349193186 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00012536060239654034 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0020306333899497986 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0003799011174123734 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0004914444871246815 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00020006767590530217 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00030652174609713256 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00027736584888771176 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00032348043168894947 for layer [ True]\n",
      "Iteration: 23600, Loss: 9.475372644374147e-05 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.174128770828247 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007786346832290292 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00027870506164617836 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.00032686753547750413 for layer [ True]\n",
      "Iteration: 24100, Loss: 5.608556602965109e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.987116813659668 for layer [False]\n",
      "Iteration: 24300, Loss: 2.6487016677856445 for layer [False]\n",
      "Iteration: 24400, Loss: 4.552509784698486 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0005807743291370571 for layer [ True]\n",
      "Iteration: 24600, Loss: 3.7558042095042765e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.405028343200684 for layer [False]\n",
      "Iteration: 24800, Loss: 4.161102771759033 for layer [False]\n",
      "Iteration: 24900, Loss: 5.208396396483295e-05 for layer [ True]\n",
      "Step 17500 | Loss: 0.000443\n",
      "Step 17600 | Loss: 0.000444\n",
      "Step 17700 | Loss: 0.000444\n",
      "Step 17800 | Loss: 0.000444\n",
      "Step 17900 | Loss: 0.000444\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1928.6016845703125 for layer [False]\n",
      "Iteration: 100, Loss: 321.68023681640625 for layer [ True]\n",
      "Iteration: 200, Loss: 1925.0794677734375 for layer [False]\n",
      "Iteration: 300, Loss: 1538.4820556640625 for layer [False]\n",
      "Iteration: 400, Loss: 238.72352600097656 for layer [ True]\n",
      "Iteration: 500, Loss: 311.7607421875 for layer [ True]\n",
      "Iteration: 600, Loss: 1438.4620361328125 for layer [False]\n",
      "Iteration: 700, Loss: 223.83628845214844 for layer [ True]\n",
      "Iteration: 800, Loss: 1151.4052734375 for layer [False]\n",
      "Iteration: 900, Loss: 1013.5484008789062 for layer [False]\n",
      "Iteration: 1000, Loss: 152.30972290039062 for layer [ True]\n",
      "Iteration: 1100, Loss: 887.7516479492188 for layer [False]\n",
      "Iteration: 1200, Loss: 1053.9197998046875 for layer [False]\n",
      "Iteration: 1300, Loss: 986.1630859375 for layer [False]\n",
      "Iteration: 1400, Loss: 245.31288146972656 for layer [ True]\n",
      "Iteration: 1500, Loss: 305.02838134765625 for layer [ True]\n",
      "Iteration: 1600, Loss: 135.3213653564453 for layer [ True]\n",
      "Iteration: 1700, Loss: 126.03705596923828 for layer [ True]\n",
      "Iteration: 1800, Loss: 610.7328491210938 for layer [False]\n",
      "Iteration: 1900, Loss: 120.39005279541016 for layer [ True]\n",
      "Iteration: 2000, Loss: 106.47178649902344 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.80471801757812 for layer [ True]\n",
      "Iteration: 2200, Loss: 109.89411926269531 for layer [ True]\n",
      "Iteration: 2300, Loss: 449.81494140625 for layer [False]\n",
      "Iteration: 2400, Loss: 133.00254821777344 for layer [ True]\n",
      "Iteration: 2500, Loss: 188.08981323242188 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.31072235107422 for layer [ True]\n",
      "Iteration: 2700, Loss: 92.00366973876953 for layer [ True]\n",
      "Iteration: 2800, Loss: 325.98681640625 for layer [False]\n",
      "Iteration: 2900, Loss: 249.25540161132812 for layer [False]\n",
      "Iteration: 3000, Loss: 80.59626007080078 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.85723876953125 for layer [False]\n",
      "Iteration: 3200, Loss: 225.94631958007812 for layer [False]\n",
      "Iteration: 3300, Loss: 154.25765991210938 for layer [False]\n",
      "Iteration: 3400, Loss: 73.36467742919922 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.27493286132812 for layer [False]\n",
      "Iteration: 3600, Loss: 62.08943176269531 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.37440490722656 for layer [False]\n",
      "Iteration: 3800, Loss: 113.35697937011719 for layer [False]\n",
      "Iteration: 3900, Loss: 43.71441650390625 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.57330322265625 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.61478805541992 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.43743133544922 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.236083984375 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.14699172973633 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.31083297729492 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.42676544189453 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.5688591003418 for layer [False]\n",
      "Iteration: 4800, Loss: 39.04181671142578 for layer [False]\n",
      "Iteration: 4900, Loss: 49.61418914794922 for layer [False]\n",
      "Iteration: 5000, Loss: 56.88394546508789 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.745649337768555 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.061439514160156 for layer [False]\n",
      "Iteration: 5300, Loss: 22.69189453125 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.966400146484375 for layer [False]\n",
      "Iteration: 5500, Loss: 24.48584747314453 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.65384864807129 for layer [ True]\n",
      "Iteration: 5700, Loss: 23.99478530883789 for layer [False]\n",
      "Iteration: 5800, Loss: 21.86836051940918 for layer [False]\n",
      "Iteration: 5900, Loss: 13.853678703308105 for layer [False]\n",
      "Iteration: 6000, Loss: 13.887837409973145 for layer [ True]\n",
      "Iteration: 6100, Loss: 22.319665908813477 for layer [False]\n",
      "Iteration: 6200, Loss: 17.904476165771484 for layer [False]\n",
      "Iteration: 6300, Loss: 15.105159759521484 for layer [False]\n",
      "Iteration: 6400, Loss: 16.262670516967773 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.623188018798828 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.811484336853027 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.236364364624023 for layer [False]\n",
      "Iteration: 6800, Loss: 5.15651273727417 for layer [ True]\n",
      "Iteration: 6900, Loss: 45.09501266479492 for layer [False]\n",
      "Iteration: 7000, Loss: 10.653813362121582 for layer [False]\n",
      "Iteration: 7100, Loss: 11.967416763305664 for layer [False]\n",
      "Iteration: 7200, Loss: 17.171344757080078 for layer [False]\n",
      "Iteration: 7300, Loss: 4.703304767608643 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.62592887878418 for layer [False]\n",
      "Iteration: 7500, Loss: 4.407461643218994 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.201779842376709 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.23015308380127 for layer [False]\n",
      "Iteration: 7800, Loss: 10.17881965637207 for layer [False]\n",
      "Iteration: 7900, Loss: 5.389345169067383 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.754539966583252 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2659246921539307 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.8237786293029785 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.729005813598633 for layer [False]\n",
      "Iteration: 8400, Loss: 2.0143110752105713 for layer [ True]\n",
      "Iteration: 8500, Loss: 11.940131187438965 for layer [False]\n",
      "Iteration: 8600, Loss: 8.059826850891113 for layer [False]\n",
      "Iteration: 8700, Loss: 2.681957244873047 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.40956449508667 for layer [False]\n",
      "Iteration: 8900, Loss: 8.967641830444336 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4894824028015137 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5382764339447021 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8819983005523682 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.576052188873291 for layer [False]\n",
      "Iteration: 9400, Loss: 8.053876876831055 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4851168394088745 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.880638360977173 for layer [False]\n",
      "Iteration: 9700, Loss: 1.049589991569519 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8932231068611145 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.337164878845215 for layer [False]\n",
      "Iteration: 10000, Loss: 3.5530178546905518 for layer [False]\n",
      "Iteration: 10100, Loss: 1.18608820438385 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.721616268157959 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.512142181396484 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5358530282974243 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5609557032585144 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4726247787475586 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7073076367378235 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.208393573760986 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5168588757514954 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.764738082885742 for layer [False]\n",
      "Iteration: 11100, Loss: 2.7983498573303223 for layer [False]\n",
      "Iteration: 11200, Loss: 0.3565096855163574 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.45273569226264954 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3587442636489868 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.7836873531341553 for layer [False]\n",
      "Iteration: 11600, Loss: 0.38182884454727173 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.20224812626838684 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2648215591907501 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2732205390930176 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1225891187787056 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18594709038734436 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1714557260274887 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10641302168369293 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.16028992831707 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.819570064544678 for layer [False]\n",
      "Iteration: 12600, Loss: 5.558298110961914 for layer [False]\n",
      "Iteration: 12700, Loss: 4.461780071258545 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10597652941942215 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09651706367731094 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06341004371643066 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.888981342315674 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06645981967449188 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.760351181030273 for layer [False]\n",
      "Iteration: 13400, Loss: 0.048927802592515945 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06376289576292038 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04930291697382927 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016872523352503777 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03847186639904976 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.031947337090969086 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.5443239212036133 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029939822852611542 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.168635845184326 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010994312353432178 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.301760673522949 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2175378799438477 for layer [False]\n",
      "Iteration: 14600, Loss: 3.2115752696990967 for layer [False]\n",
      "Iteration: 14700, Loss: 3.2419331073760986 for layer [False]\n",
      "Iteration: 14800, Loss: 3.8242290019989014 for layer [False]\n",
      "Iteration: 14900, Loss: 6.530375957489014 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006754523143172264 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.344255447387695 for layer [False]\n",
      "Iteration: 15200, Loss: 9.842989921569824 for layer [False]\n",
      "Iteration: 15300, Loss: 4.71178674697876 for layer [False]\n",
      "Iteration: 15400, Loss: 2.322261095046997 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033670878037810326 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004322613589465618 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.0041931974701583385 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.963684558868408 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002264164388179779 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003494886914268136 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.163930654525757 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011737447930499911 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021754081826657057 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.4099531173706055 for layer [False]\n",
      "Iteration: 16500, Loss: 4.78004264831543 for layer [False]\n",
      "Iteration: 16600, Loss: 3.4376542568206787 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0013952309964224696 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007919786148704588 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010463647777214646 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009057493298314512 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.4965760707855225 for layer [False]\n",
      "Iteration: 17200, Loss: 3.161804676055908 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006693910108879209 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.454383850097656 for layer [False]\n",
      "Iteration: 17500, Loss: 11.207630157470703 for layer [False]\n",
      "Iteration: 17600, Loss: 3.5236241817474365 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008019546512514353 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.53908371925354 for layer [False]\n",
      "Iteration: 17900, Loss: 4.311481952667236 for layer [False]\n",
      "Iteration: 18000, Loss: 2.5964694023132324 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0006759201642125845 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0009137466549873352 for layer [ True]\n",
      "Iteration: 18300, Loss: 6.048781394958496 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0003695247578434646 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007927750120870769 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.1649973392486572 for layer [False]\n",
      "Iteration: 18700, Loss: 4.13818359375 for layer [False]\n",
      "Iteration: 18800, Loss: 4.8245439529418945 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8716421127319336 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0006894281250424683 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.194730520248413 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0019394980045035481 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.5182106494903564 for layer [False]\n",
      "Iteration: 19400, Loss: 6.224954605102539 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0006210417486727238 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00034180734655819833 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.072714328765869 for layer [False]\n",
      "Iteration: 19800, Loss: 3.3747613430023193 for layer [False]\n",
      "Iteration: 19900, Loss: 3.7946465015411377 for layer [False]\n",
      "Iteration: 20000, Loss: 2.495504856109619 for layer [False]\n",
      "Iteration: 20100, Loss: 4.991182327270508 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0016100254142656922 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.070164918899536 for layer [False]\n",
      "Iteration: 20400, Loss: 7.118298526620492e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00035169502370990813 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0008318519685417414 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.8220057487487793 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00031533389119431376 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0010316737461835146 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.0837655067443848 for layer [False]\n",
      "Iteration: 21100, Loss: 9.01623249053955 for layer [False]\n",
      "Iteration: 21200, Loss: 0.000515343330334872 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.631715774536133 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00022574940521735698 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.4722354412078857 for layer [False]\n",
      "Iteration: 21600, Loss: 4.5113372802734375 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0004127061984036118 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00023508227604907006 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.088701009750366 for layer [False]\n",
      "Iteration: 22000, Loss: 4.033661842346191 for layer [False]\n",
      "Iteration: 22100, Loss: 0.0006146558444015682 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00017184657917823642 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.325166702270508 for layer [False]\n",
      "Iteration: 22400, Loss: 5.458864212036133 for layer [False]\n",
      "Iteration: 22500, Loss: 1.796353816986084 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00012172028073109686 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.0002218748995801434 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00014937418745830655 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0015112849650904536 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00048665469512343407 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.000466615631012246 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00020638594287447631 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0002483018906787038 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.0002462879638187587 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00043899405864067376 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.0002763530646916479 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.193079710006714 for layer [False]\n",
      "Iteration: 23800, Loss: 0.000644737621769309 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00017103571735788137 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0002970037457998842 for layer [ True]\n",
      "Iteration: 24100, Loss: 8.03973525762558e-05 for layer [ True]\n",
      "Iteration: 24200, Loss: 3.970937490463257 for layer [False]\n",
      "Iteration: 24300, Loss: 2.636232614517212 for layer [False]\n",
      "Iteration: 24400, Loss: 4.558269023895264 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00024985524942167103 for layer [ True]\n",
      "Iteration: 24600, Loss: 7.252091745613143e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.337934970855713 for layer [False]\n",
      "Iteration: 24800, Loss: 4.14961051940918 for layer [False]\n",
      "Iteration: 24900, Loss: 8.14012237242423e-05 for layer [ True]\n",
      "Step 18000 | Loss: 0.000444\n",
      "Step 18100 | Loss: 0.000443\n",
      "Step 18200 | Loss: 0.000444\n",
      "Step 18300 | Loss: 0.000443\n",
      "Step 18400 | Loss: 0.000444\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1929.0985107421875 for layer [False]\n",
      "Iteration: 100, Loss: 321.6396789550781 for layer [ True]\n",
      "Iteration: 200, Loss: 1924.887451171875 for layer [False]\n",
      "Iteration: 300, Loss: 1540.828369140625 for layer [False]\n",
      "Iteration: 400, Loss: 238.54844665527344 for layer [ True]\n",
      "Iteration: 500, Loss: 312.0006103515625 for layer [ True]\n",
      "Iteration: 600, Loss: 1439.29150390625 for layer [False]\n",
      "Iteration: 700, Loss: 223.53871154785156 for layer [ True]\n",
      "Iteration: 800, Loss: 1154.0050048828125 for layer [False]\n",
      "Iteration: 900, Loss: 1015.7003173828125 for layer [False]\n",
      "Iteration: 1000, Loss: 152.6160125732422 for layer [ True]\n",
      "Iteration: 1100, Loss: 888.5579223632812 for layer [False]\n",
      "Iteration: 1200, Loss: 1053.9232177734375 for layer [False]\n",
      "Iteration: 1300, Loss: 986.6744995117188 for layer [False]\n",
      "Iteration: 1400, Loss: 245.9910125732422 for layer [ True]\n",
      "Iteration: 1500, Loss: 305.22198486328125 for layer [ True]\n",
      "Iteration: 1600, Loss: 135.05909729003906 for layer [ True]\n",
      "Iteration: 1700, Loss: 126.61841583251953 for layer [ True]\n",
      "Iteration: 1800, Loss: 611.1602172851562 for layer [False]\n",
      "Iteration: 1900, Loss: 120.87093353271484 for layer [ True]\n",
      "Iteration: 2000, Loss: 106.75186157226562 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.885498046875 for layer [ True]\n",
      "Iteration: 2200, Loss: 109.99532318115234 for layer [ True]\n",
      "Iteration: 2300, Loss: 450.2652282714844 for layer [False]\n",
      "Iteration: 2400, Loss: 133.55792236328125 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.1101837158203 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.23908996582031 for layer [ True]\n",
      "Iteration: 2700, Loss: 92.07112121582031 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.2930908203125 for layer [False]\n",
      "Iteration: 2900, Loss: 249.02005004882812 for layer [False]\n",
      "Iteration: 3000, Loss: 80.38087463378906 for layer [ True]\n",
      "Iteration: 3100, Loss: 220.10574340820312 for layer [False]\n",
      "Iteration: 3200, Loss: 226.23794555664062 for layer [False]\n",
      "Iteration: 3300, Loss: 154.14794921875 for layer [False]\n",
      "Iteration: 3400, Loss: 73.09635925292969 for layer [ True]\n",
      "Iteration: 3500, Loss: 186.08645629882812 for layer [False]\n",
      "Iteration: 3600, Loss: 62.30207443237305 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.55291748046875 for layer [False]\n",
      "Iteration: 3800, Loss: 113.49269104003906 for layer [False]\n",
      "Iteration: 3900, Loss: 43.54670333862305 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.75405502319336 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.28458786010742 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.2960433959961 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.06715774536133 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.009944915771484 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.02621078491211 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.345462799072266 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.70933151245117 for layer [False]\n",
      "Iteration: 4800, Loss: 38.993133544921875 for layer [False]\n",
      "Iteration: 4900, Loss: 49.599613189697266 for layer [False]\n",
      "Iteration: 5000, Loss: 56.847938537597656 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.70549201965332 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.9335823059082 for layer [False]\n",
      "Iteration: 5300, Loss: 22.68631935119629 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.95093536376953 for layer [False]\n",
      "Iteration: 5500, Loss: 24.3984317779541 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.782073974609375 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.150400161743164 for layer [False]\n",
      "Iteration: 5800, Loss: 21.9056339263916 for layer [False]\n",
      "Iteration: 5900, Loss: 13.798185348510742 for layer [False]\n",
      "Iteration: 6000, Loss: 13.888144493103027 for layer [ True]\n",
      "Iteration: 6100, Loss: 21.88033676147461 for layer [False]\n",
      "Iteration: 6200, Loss: 17.96918487548828 for layer [False]\n",
      "Iteration: 6300, Loss: 15.050567626953125 for layer [False]\n",
      "Iteration: 6400, Loss: 16.263822555541992 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.613607406616211 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.815360069274902 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.09463882446289 for layer [False]\n",
      "Iteration: 6800, Loss: 5.178990364074707 for layer [ True]\n",
      "Iteration: 6900, Loss: 45.4942512512207 for layer [False]\n",
      "Iteration: 7000, Loss: 10.749960899353027 for layer [False]\n",
      "Iteration: 7100, Loss: 12.015058517456055 for layer [False]\n",
      "Iteration: 7200, Loss: 17.24203872680664 for layer [False]\n",
      "Iteration: 7300, Loss: 4.706528663635254 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.56578254699707 for layer [False]\n",
      "Iteration: 7500, Loss: 4.411612510681152 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.212432861328125 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.194401741027832 for layer [False]\n",
      "Iteration: 7800, Loss: 10.245595932006836 for layer [False]\n",
      "Iteration: 7900, Loss: 5.3710150718688965 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.730788230895996 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.253941535949707 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.8222758769989014 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.610719680786133 for layer [False]\n",
      "Iteration: 8400, Loss: 2.0146987438201904 for layer [ True]\n",
      "Iteration: 8500, Loss: 12.049362182617188 for layer [False]\n",
      "Iteration: 8600, Loss: 8.196610450744629 for layer [False]\n",
      "Iteration: 8700, Loss: 2.668724536895752 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.392750263214111 for layer [False]\n",
      "Iteration: 8900, Loss: 8.949305534362793 for layer [False]\n",
      "Iteration: 9000, Loss: 1.484776258468628 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5316108465194702 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.8803571462631226 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.5510687828063965 for layer [False]\n",
      "Iteration: 9400, Loss: 8.033065795898438 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4719107151031494 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.891486167907715 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0338270664215088 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8907835483551025 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.403838157653809 for layer [False]\n",
      "Iteration: 10000, Loss: 3.5731070041656494 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1777193546295166 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.7160653471946716 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.501509189605713 for layer [False]\n",
      "Iteration: 10400, Loss: 0.53509122133255 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.559341549873352 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.467322826385498 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6998983025550842 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.186118125915527 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5127266049385071 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.781481742858887 for layer [False]\n",
      "Iteration: 11100, Loss: 2.7652599811553955 for layer [False]\n",
      "Iteration: 11200, Loss: 0.35555773973464966 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.45039108395576477 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3608190715312958 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.823441743850708 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3824617564678192 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.20299625396728516 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.26450279355049133 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2758190333843231 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.12394087761640549 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.183717280626297 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.17077022790908813 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.1060701459646225 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.16178785264492035 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.871728420257568 for layer [False]\n",
      "Iteration: 12600, Loss: 5.570934295654297 for layer [False]\n",
      "Iteration: 12700, Loss: 4.444903373718262 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10595954954624176 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.0960782915353775 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06342849135398865 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.914344787597656 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06674105674028397 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.825404167175293 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04906681925058365 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06368879228830338 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.049118779599666595 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016883816570043564 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.03849185258150101 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.0316903218626976 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.540928602218628 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029662294313311577 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.078654766082764 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010897227562963963 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.3216090202331543 for layer [False]\n",
      "Iteration: 14500, Loss: 2.202986478805542 for layer [False]\n",
      "Iteration: 14600, Loss: 3.1454315185546875 for layer [False]\n",
      "Iteration: 14700, Loss: 3.2426905632019043 for layer [False]\n",
      "Iteration: 14800, Loss: 3.7907824516296387 for layer [False]\n",
      "Iteration: 14900, Loss: 6.570927619934082 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006707172840833664 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.33723258972168 for layer [False]\n",
      "Iteration: 15200, Loss: 9.757390975952148 for layer [False]\n",
      "Iteration: 15300, Loss: 4.684463977813721 for layer [False]\n",
      "Iteration: 15400, Loss: 2.311335325241089 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033511247020214796 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004291670862585306 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004256885033100843 for layer [ True]\n",
      "Iteration: 15800, Loss: 2.972571611404419 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002247023629024625 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.003441272070631385 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.2029550075531006 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011559234699234366 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0021641459316015244 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.415396690368652 for layer [False]\n",
      "Iteration: 16500, Loss: 4.772750377655029 for layer [False]\n",
      "Iteration: 16600, Loss: 3.4684972763061523 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0013856325531378388 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0008139415876939893 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010660295374691486 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0008893755148164928 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.5101518630981445 for layer [False]\n",
      "Iteration: 17200, Loss: 3.137679100036621 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006699366495013237 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.348485469818115 for layer [False]\n",
      "Iteration: 17500, Loss: 11.227435111999512 for layer [False]\n",
      "Iteration: 17600, Loss: 3.5229408740997314 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008233313565142453 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.520745277404785 for layer [False]\n",
      "Iteration: 17900, Loss: 4.313814163208008 for layer [False]\n",
      "Iteration: 18000, Loss: 2.610844612121582 for layer [False]\n",
      "Iteration: 18100, Loss: 0.000685161619912833 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0009131927508860826 for layer [ True]\n",
      "Iteration: 18300, Loss: 6.006680965423584 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00037326119490899146 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007805567001923919 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.192885637283325 for layer [False]\n",
      "Iteration: 18700, Loss: 4.179867267608643 for layer [False]\n",
      "Iteration: 18800, Loss: 4.854880332946777 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8630120754241943 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0002127426996594295 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.1690821647644043 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0017829615389928222 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.4745864868164062 for layer [False]\n",
      "Iteration: 19400, Loss: 6.257937908172607 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007184807909652591 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0002912461932282895 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.106565475463867 for layer [False]\n",
      "Iteration: 19800, Loss: 3.3360798358917236 for layer [False]\n",
      "Iteration: 19900, Loss: 3.7539665699005127 for layer [False]\n",
      "Iteration: 20000, Loss: 2.449826717376709 for layer [False]\n",
      "Iteration: 20100, Loss: 4.910267353057861 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0011652741814032197 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.0925002098083496 for layer [False]\n",
      "Iteration: 20400, Loss: 5.6463508371962234e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.00039666745578870177 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0010348689975216985 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.8187854290008545 for layer [False]\n",
      "Iteration: 20800, Loss: 0.0003080642782151699 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0006675452459603548 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.09963059425354 for layer [False]\n",
      "Iteration: 21100, Loss: 9.024553298950195 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0004425359074957669 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.599245071411133 for layer [False]\n",
      "Iteration: 21400, Loss: 0.00015768228331580758 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.4995195865631104 for layer [False]\n",
      "Iteration: 21600, Loss: 4.535179138183594 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0003946053038816899 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.00023335361038334668 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.103224515914917 for layer [False]\n",
      "Iteration: 22000, Loss: 3.9456963539123535 for layer [False]\n",
      "Iteration: 22100, Loss: 0.000279441213933751 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.00016844911442603916 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.282359600067139 for layer [False]\n",
      "Iteration: 22400, Loss: 5.405559062957764 for layer [False]\n",
      "Iteration: 22500, Loss: 1.8167343139648438 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00016938064072746783 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.0002243575727334246 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00015975511632859707 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0022185812704265118 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0003761668049264699 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0004486078687477857 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0001821690530050546 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00035647733602672815 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00023125275038182735 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00023544422583654523 for layer [ True]\n",
      "Iteration: 23600, Loss: 5.9212961787125096e-05 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.1380984783172607 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0009459550492465496 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0001628253230592236 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0005832503084093332 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.00012685818364843726 for layer [ True]\n",
      "Iteration: 24200, Loss: 4.019705295562744 for layer [False]\n",
      "Iteration: 24300, Loss: 2.659559965133667 for layer [False]\n",
      "Iteration: 24400, Loss: 4.43980598449707 for layer [False]\n",
      "Iteration: 24500, Loss: 0.0003657476045191288 for layer [ True]\n",
      "Iteration: 24600, Loss: 4.174997957306914e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.378365516662598 for layer [False]\n",
      "Iteration: 24800, Loss: 4.126986503601074 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00016385568596888334 for layer [ True]\n",
      "Step 18500 | Loss: 0.000443\n",
      "Step 18600 | Loss: 0.000443\n",
      "Step 18700 | Loss: 0.000443\n",
      "Step 18800 | Loss: 0.000443\n",
      "Step 18900 | Loss: 0.000443\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1928.0029296875 for layer [False]\n",
      "Iteration: 100, Loss: 321.248046875 for layer [ True]\n",
      "Iteration: 200, Loss: 1922.9609375 for layer [False]\n",
      "Iteration: 300, Loss: 1540.0732421875 for layer [False]\n",
      "Iteration: 400, Loss: 239.31216430664062 for layer [ True]\n",
      "Iteration: 500, Loss: 310.68170166015625 for layer [ True]\n",
      "Iteration: 600, Loss: 1437.485595703125 for layer [False]\n",
      "Iteration: 700, Loss: 223.70968627929688 for layer [ True]\n",
      "Iteration: 800, Loss: 1151.2806396484375 for layer [False]\n",
      "Iteration: 900, Loss: 1013.4624633789062 for layer [False]\n",
      "Iteration: 1000, Loss: 152.26431274414062 for layer [ True]\n",
      "Iteration: 1100, Loss: 888.1051635742188 for layer [False]\n",
      "Iteration: 1200, Loss: 1053.2432861328125 for layer [False]\n",
      "Iteration: 1300, Loss: 986.5704345703125 for layer [False]\n",
      "Iteration: 1400, Loss: 246.0077667236328 for layer [ True]\n",
      "Iteration: 1500, Loss: 304.7801208496094 for layer [ True]\n",
      "Iteration: 1600, Loss: 134.8967742919922 for layer [ True]\n",
      "Iteration: 1700, Loss: 126.160400390625 for layer [ True]\n",
      "Iteration: 1800, Loss: 611.1909790039062 for layer [False]\n",
      "Iteration: 1900, Loss: 120.9577865600586 for layer [ True]\n",
      "Iteration: 2000, Loss: 106.73855590820312 for layer [ True]\n",
      "Iteration: 2100, Loss: 158.88912963867188 for layer [ True]\n",
      "Iteration: 2200, Loss: 110.21366882324219 for layer [ True]\n",
      "Iteration: 2300, Loss: 449.99462890625 for layer [False]\n",
      "Iteration: 2400, Loss: 133.9694061279297 for layer [ True]\n",
      "Iteration: 2500, Loss: 187.9761505126953 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.09140014648438 for layer [ True]\n",
      "Iteration: 2700, Loss: 92.32231140136719 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.56744384765625 for layer [False]\n",
      "Iteration: 2900, Loss: 249.15333557128906 for layer [False]\n",
      "Iteration: 3000, Loss: 80.61546325683594 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.32481384277344 for layer [False]\n",
      "Iteration: 3200, Loss: 226.0722198486328 for layer [False]\n",
      "Iteration: 3300, Loss: 154.02273559570312 for layer [False]\n",
      "Iteration: 3400, Loss: 73.4522705078125 for layer [ True]\n",
      "Iteration: 3500, Loss: 185.91395568847656 for layer [False]\n",
      "Iteration: 3600, Loss: 62.436092376708984 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.20687866210938 for layer [False]\n",
      "Iteration: 3800, Loss: 113.08702850341797 for layer [False]\n",
      "Iteration: 3900, Loss: 43.580081939697266 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.697288513183594 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.40298080444336 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.15027618408203 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.133689880371094 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.14801788330078 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.00519561767578 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.37010955810547 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.5976448059082 for layer [False]\n",
      "Iteration: 4800, Loss: 39.0024299621582 for layer [False]\n",
      "Iteration: 4900, Loss: 49.527889251708984 for layer [False]\n",
      "Iteration: 5000, Loss: 56.76938247680664 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.72698211669922 for layer [ True]\n",
      "Iteration: 5200, Loss: 36.0491943359375 for layer [False]\n",
      "Iteration: 5300, Loss: 22.629974365234375 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.902353286743164 for layer [False]\n",
      "Iteration: 5500, Loss: 24.311506271362305 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.663129806518555 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.07025909423828 for layer [False]\n",
      "Iteration: 5800, Loss: 22.02286148071289 for layer [False]\n",
      "Iteration: 5900, Loss: 14.024703979492188 for layer [False]\n",
      "Iteration: 6000, Loss: 13.877883911132812 for layer [ True]\n",
      "Iteration: 6100, Loss: 21.80512237548828 for layer [False]\n",
      "Iteration: 6200, Loss: 18.190593719482422 for layer [False]\n",
      "Iteration: 6300, Loss: 14.996394157409668 for layer [False]\n",
      "Iteration: 6400, Loss: 16.19921112060547 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.603988647460938 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.809735298156738 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.21125030517578 for layer [False]\n",
      "Iteration: 6800, Loss: 5.16664981842041 for layer [ True]\n",
      "Iteration: 6900, Loss: 45.42020797729492 for layer [False]\n",
      "Iteration: 7000, Loss: 10.758776664733887 for layer [False]\n",
      "Iteration: 7100, Loss: 12.00758171081543 for layer [False]\n",
      "Iteration: 7200, Loss: 17.357080459594727 for layer [False]\n",
      "Iteration: 7300, Loss: 4.689150333404541 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.576932907104492 for layer [False]\n",
      "Iteration: 7500, Loss: 4.3983893394470215 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.199774742126465 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.27816390991211 for layer [False]\n",
      "Iteration: 7800, Loss: 10.184657096862793 for layer [False]\n",
      "Iteration: 7900, Loss: 5.337800979614258 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.710759401321411 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2444937229156494 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.8083114624023438 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.654053688049316 for layer [False]\n",
      "Iteration: 8400, Loss: 2.0113115310668945 for layer [ True]\n",
      "Iteration: 8500, Loss: 12.076252937316895 for layer [False]\n",
      "Iteration: 8600, Loss: 8.193367004394531 for layer [False]\n",
      "Iteration: 8700, Loss: 2.6640923023223877 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.43518590927124 for layer [False]\n",
      "Iteration: 8900, Loss: 8.900801658630371 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4866299629211426 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.519439458847046 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.883170485496521 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.623082160949707 for layer [False]\n",
      "Iteration: 9400, Loss: 8.099621772766113 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4540992975234985 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.8730151653289795 for layer [False]\n",
      "Iteration: 9700, Loss: 1.038510799407959 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.889110803604126 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.360018253326416 for layer [False]\n",
      "Iteration: 10000, Loss: 3.5620219707489014 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1691436767578125 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.709972620010376 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.480484485626221 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5378804206848145 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5549504160881042 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.5011892318725586 for layer [False]\n",
      "Iteration: 10700, Loss: 0.6972931623458862 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.202720642089844 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5090881586074829 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.807063102722168 for layer [False]\n",
      "Iteration: 11100, Loss: 2.7627274990081787 for layer [False]\n",
      "Iteration: 11200, Loss: 0.35262447595596313 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.44931474328041077 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3619227409362793 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.8042311668395996 for layer [False]\n",
      "Iteration: 11600, Loss: 0.38128408789634705 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.20374074578285217 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.26437467336654663 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.27456915378570557 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.12396928668022156 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.1831357479095459 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.17055565118789673 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10609184950590134 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.161554753780365 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.920119285583496 for layer [False]\n",
      "Iteration: 12600, Loss: 5.599127769470215 for layer [False]\n",
      "Iteration: 12700, Loss: 4.46055793762207 for layer [False]\n",
      "Iteration: 12800, Loss: 0.10583844780921936 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09613081812858582 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06335590779781342 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.961258888244629 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06667386740446091 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.811296463012695 for layer [False]\n",
      "Iteration: 13400, Loss: 0.0493018813431263 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06395025551319122 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.04931756854057312 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.016961868852376938 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.038603998720645905 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.031934499740600586 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.557666063308716 for layer [False]\n",
      "Iteration: 14100, Loss: 0.029951708391308784 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.1756463050842285 for layer [False]\n",
      "Iteration: 14300, Loss: 0.010962300933897495 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.326465129852295 for layer [False]\n",
      "Iteration: 14500, Loss: 2.2262496948242188 for layer [False]\n",
      "Iteration: 14600, Loss: 3.188363552093506 for layer [False]\n",
      "Iteration: 14700, Loss: 3.2367522716522217 for layer [False]\n",
      "Iteration: 14800, Loss: 3.8101091384887695 for layer [False]\n",
      "Iteration: 14900, Loss: 6.547197341918945 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006727044004946947 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.319413185119629 for layer [False]\n",
      "Iteration: 15200, Loss: 9.791364669799805 for layer [False]\n",
      "Iteration: 15300, Loss: 4.726513862609863 for layer [False]\n",
      "Iteration: 15400, Loss: 2.3362436294555664 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033816408831626177 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.004304892849177122 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.00418065907433629 for layer [ True]\n",
      "Iteration: 15800, Loss: 3.0189766883850098 for layer [False]\n",
      "Iteration: 15900, Loss: 0.002246097894385457 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0035029298160225153 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.2073521614074707 for layer [False]\n",
      "Iteration: 16200, Loss: 0.001153964432887733 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.002203478245064616 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.386258125305176 for layer [False]\n",
      "Iteration: 16500, Loss: 4.775907039642334 for layer [False]\n",
      "Iteration: 16600, Loss: 3.440298318862915 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0013803602196276188 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007680837879888713 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.001084840507246554 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009083840996026993 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.5023584365844727 for layer [False]\n",
      "Iteration: 17200, Loss: 3.18499493598938 for layer [False]\n",
      "Iteration: 17300, Loss: 0.0006846469477750361 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.399111747741699 for layer [False]\n",
      "Iteration: 17500, Loss: 11.315199851989746 for layer [False]\n",
      "Iteration: 17600, Loss: 3.563246965408325 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008746160310693085 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.5315914154052734 for layer [False]\n",
      "Iteration: 17900, Loss: 4.3251776695251465 for layer [False]\n",
      "Iteration: 18000, Loss: 2.5935559272766113 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007268570479936898 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0008950685732997954 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.998851299285889 for layer [False]\n",
      "Iteration: 18400, Loss: 0.00038040056824684143 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.000797873770352453 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.1576950550079346 for layer [False]\n",
      "Iteration: 18700, Loss: 4.179553031921387 for layer [False]\n",
      "Iteration: 18800, Loss: 4.865139484405518 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8580214977264404 for layer [False]\n",
      "Iteration: 19000, Loss: 0.00020071974722668529 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.1613075733184814 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0018565822392702103 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.4962875843048096 for layer [False]\n",
      "Iteration: 19400, Loss: 6.278867244720459 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007205524016171694 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.0003150622360408306 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.133796215057373 for layer [False]\n",
      "Iteration: 19800, Loss: 3.380770444869995 for layer [False]\n",
      "Iteration: 19900, Loss: 3.7913334369659424 for layer [False]\n",
      "Iteration: 20000, Loss: 2.4866786003112793 for layer [False]\n",
      "Iteration: 20100, Loss: 4.946714878082275 for layer [False]\n",
      "Iteration: 20200, Loss: 0.0012814437504857779 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.070188283920288 for layer [False]\n",
      "Iteration: 20400, Loss: 5.924131619394757e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003988393291365355 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0011621115263551474 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.852205991744995 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00031124186352826655 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0007626030710525811 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.143416166305542 for layer [False]\n",
      "Iteration: 21100, Loss: 9.109559059143066 for layer [False]\n",
      "Iteration: 21200, Loss: 0.00019976196927018464 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.590464115142822 for layer [False]\n",
      "Iteration: 21400, Loss: 0.0004434429865796119 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.5187487602233887 for layer [False]\n",
      "Iteration: 21600, Loss: 4.486693859100342 for layer [False]\n",
      "Iteration: 21700, Loss: 0.00015444458404090255 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.000724691606592387 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.0891151428222656 for layer [False]\n",
      "Iteration: 22000, Loss: 4.026993751525879 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00012081782188033685 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0006672237068414688 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.321903705596924 for layer [False]\n",
      "Iteration: 22400, Loss: 5.410630226135254 for layer [False]\n",
      "Iteration: 22500, Loss: 1.8026831150054932 for layer [False]\n",
      "Iteration: 22600, Loss: 0.0002121169090969488 for layer [ True]\n",
      "Iteration: 22700, Loss: 7.91747443145141e-05 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.0007047374383546412 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.0005062496056780219 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.0008264930220320821 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0019560656510293484 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.0001465629175072536 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.0006531781400553882 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00013846656656824052 for layer [ True]\n",
      "Iteration: 23500, Loss: 0.00020326247613411397 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00023481243988499045 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.172957181930542 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0007424653740599751 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.0001301826850976795 for layer [ True]\n",
      "Iteration: 24000, Loss: 9.607781248632818e-05 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.0002193079999415204 for layer [ True]\n",
      "Iteration: 24200, Loss: 4.012279033660889 for layer [False]\n",
      "Iteration: 24300, Loss: 2.677717924118042 for layer [False]\n",
      "Iteration: 24400, Loss: 4.562535762786865 for layer [False]\n",
      "Iteration: 24500, Loss: 4.760294905281626e-05 for layer [ True]\n",
      "Iteration: 24600, Loss: 7.849054236430675e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.385758399963379 for layer [False]\n",
      "Iteration: 24800, Loss: 4.186111927032471 for layer [False]\n",
      "Iteration: 24900, Loss: 0.0001701455475995317 for layer [ True]\n",
      "Step 19000 | Loss: 0.000443\n",
      "Step 19100 | Loss: 0.000443\n",
      "Step 19200 | Loss: 0.000443\n",
      "Step 19300 | Loss: 0.000442\n",
      "Step 19400 | Loss: 0.000443\n",
      "Recomputing GGN\n",
      "Iteration: 0, Loss: 1928.004638671875 for layer [False]\n",
      "Iteration: 100, Loss: 321.0660705566406 for layer [ True]\n",
      "Iteration: 200, Loss: 1922.9661865234375 for layer [False]\n",
      "Iteration: 300, Loss: 1540.305419921875 for layer [False]\n",
      "Iteration: 400, Loss: 239.42955017089844 for layer [ True]\n",
      "Iteration: 500, Loss: 311.4949645996094 for layer [ True]\n",
      "Iteration: 600, Loss: 1437.0262451171875 for layer [False]\n",
      "Iteration: 700, Loss: 223.73971557617188 for layer [ True]\n",
      "Iteration: 800, Loss: 1151.875244140625 for layer [False]\n",
      "Iteration: 900, Loss: 1013.7020263671875 for layer [False]\n",
      "Iteration: 1000, Loss: 152.2702178955078 for layer [ True]\n",
      "Iteration: 1100, Loss: 888.3780517578125 for layer [False]\n",
      "Iteration: 1200, Loss: 1052.7518310546875 for layer [False]\n",
      "Iteration: 1300, Loss: 986.4291381835938 for layer [False]\n",
      "Iteration: 1400, Loss: 246.804443359375 for layer [ True]\n",
      "Iteration: 1500, Loss: 306.5518493652344 for layer [ True]\n",
      "Iteration: 1600, Loss: 135.3747100830078 for layer [ True]\n",
      "Iteration: 1700, Loss: 126.56419372558594 for layer [ True]\n",
      "Iteration: 1800, Loss: 610.907470703125 for layer [False]\n",
      "Iteration: 1900, Loss: 121.5526351928711 for layer [ True]\n",
      "Iteration: 2000, Loss: 107.18477630615234 for layer [ True]\n",
      "Iteration: 2100, Loss: 159.37060546875 for layer [ True]\n",
      "Iteration: 2200, Loss: 110.58243560791016 for layer [ True]\n",
      "Iteration: 2300, Loss: 449.76678466796875 for layer [False]\n",
      "Iteration: 2400, Loss: 134.3828125 for layer [ True]\n",
      "Iteration: 2500, Loss: 188.74386596679688 for layer [ True]\n",
      "Iteration: 2600, Loss: 121.8593978881836 for layer [ True]\n",
      "Iteration: 2700, Loss: 92.6209945678711 for layer [ True]\n",
      "Iteration: 2800, Loss: 326.7614440917969 for layer [False]\n",
      "Iteration: 2900, Loss: 248.61192321777344 for layer [False]\n",
      "Iteration: 3000, Loss: 80.8216323852539 for layer [ True]\n",
      "Iteration: 3100, Loss: 219.2874755859375 for layer [False]\n",
      "Iteration: 3200, Loss: 225.90907287597656 for layer [False]\n",
      "Iteration: 3300, Loss: 153.84120178222656 for layer [False]\n",
      "Iteration: 3400, Loss: 73.60079193115234 for layer [ True]\n",
      "Iteration: 3500, Loss: 185.75498962402344 for layer [False]\n",
      "Iteration: 3600, Loss: 62.65055847167969 for layer [ True]\n",
      "Iteration: 3700, Loss: 131.24569702148438 for layer [False]\n",
      "Iteration: 3800, Loss: 112.92117309570312 for layer [False]\n",
      "Iteration: 3900, Loss: 43.77886199951172 for layer [ True]\n",
      "Iteration: 4000, Loss: 51.994075775146484 for layer [ True]\n",
      "Iteration: 4100, Loss: 53.57326889038086 for layer [ True]\n",
      "Iteration: 4200, Loss: 78.50818634033203 for layer [ True]\n",
      "Iteration: 4300, Loss: 46.33465576171875 for layer [ True]\n",
      "Iteration: 4400, Loss: 42.24848556518555 for layer [ True]\n",
      "Iteration: 4500, Loss: 54.250282287597656 for layer [ True]\n",
      "Iteration: 4600, Loss: 39.49085998535156 for layer [ True]\n",
      "Iteration: 4700, Loss: 60.49741744995117 for layer [False]\n",
      "Iteration: 4800, Loss: 38.98318099975586 for layer [False]\n",
      "Iteration: 4900, Loss: 49.43220520019531 for layer [False]\n",
      "Iteration: 5000, Loss: 56.9168701171875 for layer [ True]\n",
      "Iteration: 5100, Loss: 23.813106536865234 for layer [ True]\n",
      "Iteration: 5200, Loss: 35.94367599487305 for layer [False]\n",
      "Iteration: 5300, Loss: 22.73255729675293 for layer [ True]\n",
      "Iteration: 5400, Loss: 25.837581634521484 for layer [False]\n",
      "Iteration: 5500, Loss: 24.409448623657227 for layer [ True]\n",
      "Iteration: 5600, Loss: 30.809894561767578 for layer [ True]\n",
      "Iteration: 5700, Loss: 24.176660537719727 for layer [False]\n",
      "Iteration: 5800, Loss: 21.960800170898438 for layer [False]\n",
      "Iteration: 5900, Loss: 14.0452241897583 for layer [False]\n",
      "Iteration: 6000, Loss: 13.901944160461426 for layer [ True]\n",
      "Iteration: 6100, Loss: 21.440221786499023 for layer [False]\n",
      "Iteration: 6200, Loss: 18.22798728942871 for layer [False]\n",
      "Iteration: 6300, Loss: 14.861773490905762 for layer [False]\n",
      "Iteration: 6400, Loss: 16.25640296936035 for layer [ True]\n",
      "Iteration: 6500, Loss: 8.623332977294922 for layer [ True]\n",
      "Iteration: 6600, Loss: 9.832371711730957 for layer [ True]\n",
      "Iteration: 6700, Loss: 17.087430953979492 for layer [False]\n",
      "Iteration: 6800, Loss: 5.18675422668457 for layer [ True]\n",
      "Iteration: 6900, Loss: 45.73487091064453 for layer [False]\n",
      "Iteration: 7000, Loss: 10.701988220214844 for layer [False]\n",
      "Iteration: 7100, Loss: 12.010626792907715 for layer [False]\n",
      "Iteration: 7200, Loss: 17.287309646606445 for layer [False]\n",
      "Iteration: 7300, Loss: 4.697751522064209 for layer [ True]\n",
      "Iteration: 7400, Loss: 16.368364334106445 for layer [False]\n",
      "Iteration: 7500, Loss: 4.419591426849365 for layer [ True]\n",
      "Iteration: 7600, Loss: 5.224199295043945 for layer [ True]\n",
      "Iteration: 7700, Loss: 12.236711502075195 for layer [False]\n",
      "Iteration: 7800, Loss: 10.171229362487793 for layer [False]\n",
      "Iteration: 7900, Loss: 5.346802234649658 for layer [ True]\n",
      "Iteration: 8000, Loss: 3.7282826900482178 for layer [ True]\n",
      "Iteration: 8100, Loss: 3.2617056369781494 for layer [ True]\n",
      "Iteration: 8200, Loss: 2.821765899658203 for layer [ True]\n",
      "Iteration: 8300, Loss: 13.451010704040527 for layer [False]\n",
      "Iteration: 8400, Loss: 2.0140902996063232 for layer [ True]\n",
      "Iteration: 8500, Loss: 12.066986083984375 for layer [False]\n",
      "Iteration: 8600, Loss: 8.154220581054688 for layer [False]\n",
      "Iteration: 8700, Loss: 2.669447660446167 for layer [ True]\n",
      "Iteration: 8800, Loss: 5.39563512802124 for layer [False]\n",
      "Iteration: 8900, Loss: 8.82698917388916 for layer [False]\n",
      "Iteration: 9000, Loss: 1.4953416585922241 for layer [ True]\n",
      "Iteration: 9100, Loss: 1.5279287099838257 for layer [ True]\n",
      "Iteration: 9200, Loss: 1.886549472808838 for layer [ True]\n",
      "Iteration: 9300, Loss: 6.5995049476623535 for layer [False]\n",
      "Iteration: 9400, Loss: 8.036949157714844 for layer [False]\n",
      "Iteration: 9500, Loss: 1.4619966745376587 for layer [ True]\n",
      "Iteration: 9600, Loss: 3.881911039352417 for layer [False]\n",
      "Iteration: 9700, Loss: 1.0374889373779297 for layer [ True]\n",
      "Iteration: 9800, Loss: 0.8926221132278442 for layer [ True]\n",
      "Iteration: 9900, Loss: 7.310213565826416 for layer [False]\n",
      "Iteration: 10000, Loss: 3.556103467941284 for layer [False]\n",
      "Iteration: 10100, Loss: 1.1734733581542969 for layer [ True]\n",
      "Iteration: 10200, Loss: 0.713914155960083 for layer [ True]\n",
      "Iteration: 10300, Loss: 4.470723628997803 for layer [False]\n",
      "Iteration: 10400, Loss: 0.5413302183151245 for layer [ True]\n",
      "Iteration: 10500, Loss: 0.5556532740592957 for layer [ True]\n",
      "Iteration: 10600, Loss: 3.4934725761413574 for layer [False]\n",
      "Iteration: 10700, Loss: 0.7006601691246033 for layer [ True]\n",
      "Iteration: 10800, Loss: 4.186099052429199 for layer [False]\n",
      "Iteration: 10900, Loss: 0.5136123299598694 for layer [ True]\n",
      "Iteration: 11000, Loss: 8.69479751586914 for layer [False]\n",
      "Iteration: 11100, Loss: 2.7156524658203125 for layer [False]\n",
      "Iteration: 11200, Loss: 0.35542067885398865 for layer [ True]\n",
      "Iteration: 11300, Loss: 0.45160382986068726 for layer [ True]\n",
      "Iteration: 11400, Loss: 0.3638489544391632 for layer [ True]\n",
      "Iteration: 11500, Loss: 2.8365330696105957 for layer [False]\n",
      "Iteration: 11600, Loss: 0.3825927972793579 for layer [ True]\n",
      "Iteration: 11700, Loss: 0.2046152949333191 for layer [ True]\n",
      "Iteration: 11800, Loss: 0.2659669518470764 for layer [ True]\n",
      "Iteration: 11900, Loss: 0.2779053747653961 for layer [ True]\n",
      "Iteration: 12000, Loss: 0.1248757615685463 for layer [ True]\n",
      "Iteration: 12100, Loss: 0.18491214513778687 for layer [ True]\n",
      "Iteration: 12200, Loss: 0.1717747449874878 for layer [ True]\n",
      "Iteration: 12300, Loss: 0.10672999918460846 for layer [ True]\n",
      "Iteration: 12400, Loss: 0.1630541831254959 for layer [ True]\n",
      "Iteration: 12500, Loss: 5.924722671508789 for layer [False]\n",
      "Iteration: 12600, Loss: 5.5837883949279785 for layer [False]\n",
      "Iteration: 12700, Loss: 4.456013202667236 for layer [False]\n",
      "Iteration: 12800, Loss: 0.1068403497338295 for layer [ True]\n",
      "Iteration: 12900, Loss: 0.09691820293664932 for layer [ True]\n",
      "Iteration: 13000, Loss: 0.06382875889539719 for layer [ True]\n",
      "Iteration: 13100, Loss: 4.950263023376465 for layer [False]\n",
      "Iteration: 13200, Loss: 0.06714761257171631 for layer [ True]\n",
      "Iteration: 13300, Loss: 4.809573650360107 for layer [False]\n",
      "Iteration: 13400, Loss: 0.04964149743318558 for layer [ True]\n",
      "Iteration: 13500, Loss: 0.06441161036491394 for layer [ True]\n",
      "Iteration: 13600, Loss: 0.0497068352997303 for layer [ True]\n",
      "Iteration: 13700, Loss: 0.017074666917324066 for layer [ True]\n",
      "Iteration: 13800, Loss: 0.039109375327825546 for layer [ True]\n",
      "Iteration: 13900, Loss: 0.03219004347920418 for layer [ True]\n",
      "Iteration: 14000, Loss: 2.561704397201538 for layer [False]\n",
      "Iteration: 14100, Loss: 0.03014908730983734 for layer [ True]\n",
      "Iteration: 14200, Loss: 5.140574932098389 for layer [False]\n",
      "Iteration: 14300, Loss: 0.011043719947338104 for layer [ True]\n",
      "Iteration: 14400, Loss: 3.336669445037842 for layer [False]\n",
      "Iteration: 14500, Loss: 2.221773386001587 for layer [False]\n",
      "Iteration: 14600, Loss: 3.15791916847229 for layer [False]\n",
      "Iteration: 14700, Loss: 3.2498629093170166 for layer [False]\n",
      "Iteration: 14800, Loss: 3.779370069503784 for layer [False]\n",
      "Iteration: 14900, Loss: 6.573619842529297 for layer [False]\n",
      "Iteration: 15000, Loss: 0.006818567402660847 for layer [ True]\n",
      "Iteration: 15100, Loss: 5.314094066619873 for layer [False]\n",
      "Iteration: 15200, Loss: 9.683338165283203 for layer [False]\n",
      "Iteration: 15300, Loss: 4.721033096313477 for layer [False]\n",
      "Iteration: 15400, Loss: 2.3161680698394775 for layer [False]\n",
      "Iteration: 15500, Loss: 0.0033824837300926447 for layer [ True]\n",
      "Iteration: 15600, Loss: 0.00438534002751112 for layer [ True]\n",
      "Iteration: 15700, Loss: 0.004244574811309576 for layer [ True]\n",
      "Iteration: 15800, Loss: 3.0410728454589844 for layer [False]\n",
      "Iteration: 15900, Loss: 0.0022709660697728395 for layer [ True]\n",
      "Iteration: 16000, Loss: 0.0034993388690054417 for layer [ True]\n",
      "Iteration: 16100, Loss: 3.2185561656951904 for layer [False]\n",
      "Iteration: 16200, Loss: 0.0011595539981499314 for layer [ True]\n",
      "Iteration: 16300, Loss: 0.0022105600219219923 for layer [ True]\n",
      "Iteration: 16400, Loss: 4.333566188812256 for layer [False]\n",
      "Iteration: 16500, Loss: 4.781005859375 for layer [False]\n",
      "Iteration: 16600, Loss: 3.428152322769165 for layer [False]\n",
      "Iteration: 16700, Loss: 0.0014047515578567982 for layer [ True]\n",
      "Iteration: 16800, Loss: 0.0007838360616005957 for layer [ True]\n",
      "Iteration: 16900, Loss: 0.0010945521062240005 for layer [ True]\n",
      "Iteration: 17000, Loss: 0.0009079240262508392 for layer [ True]\n",
      "Iteration: 17100, Loss: 2.544309377670288 for layer [False]\n",
      "Iteration: 17200, Loss: 3.186859607696533 for layer [False]\n",
      "Iteration: 17300, Loss: 0.000690072774887085 for layer [ True]\n",
      "Iteration: 17400, Loss: 6.409561634063721 for layer [False]\n",
      "Iteration: 17500, Loss: 11.302017211914062 for layer [False]\n",
      "Iteration: 17600, Loss: 3.5117154121398926 for layer [False]\n",
      "Iteration: 17700, Loss: 0.0008567944169044495 for layer [ True]\n",
      "Iteration: 17800, Loss: 3.498030424118042 for layer [False]\n",
      "Iteration: 17900, Loss: 4.276794910430908 for layer [False]\n",
      "Iteration: 18000, Loss: 2.589367389678955 for layer [False]\n",
      "Iteration: 18100, Loss: 0.0007168877054937184 for layer [ True]\n",
      "Iteration: 18200, Loss: 0.0009019775316119194 for layer [ True]\n",
      "Iteration: 18300, Loss: 5.957058429718018 for layer [False]\n",
      "Iteration: 18400, Loss: 0.0003879491123370826 for layer [ True]\n",
      "Iteration: 18500, Loss: 0.0007956964545883238 for layer [ True]\n",
      "Iteration: 18600, Loss: 2.1593751907348633 for layer [False]\n",
      "Iteration: 18700, Loss: 4.164436340332031 for layer [False]\n",
      "Iteration: 18800, Loss: 4.86100435256958 for layer [False]\n",
      "Iteration: 18900, Loss: 1.8465181589126587 for layer [False]\n",
      "Iteration: 19000, Loss: 0.0002074146905215457 for layer [ True]\n",
      "Iteration: 19100, Loss: 3.1444804668426514 for layer [False]\n",
      "Iteration: 19200, Loss: 0.0018558816518634558 for layer [ True]\n",
      "Iteration: 19300, Loss: 2.4862310886383057 for layer [False]\n",
      "Iteration: 19400, Loss: 6.2689714431762695 for layer [False]\n",
      "Iteration: 19500, Loss: 0.0007200230611488223 for layer [ True]\n",
      "Iteration: 19600, Loss: 0.00030687509570270777 for layer [ True]\n",
      "Iteration: 19700, Loss: 5.148601531982422 for layer [False]\n",
      "Iteration: 19800, Loss: 3.314312696456909 for layer [False]\n",
      "Iteration: 19900, Loss: 3.776780843734741 for layer [False]\n",
      "Iteration: 20000, Loss: 2.470773458480835 for layer [False]\n",
      "Iteration: 20100, Loss: 4.895514011383057 for layer [False]\n",
      "Iteration: 20200, Loss: 0.001233850372955203 for layer [ True]\n",
      "Iteration: 20300, Loss: 3.0504562854766846 for layer [False]\n",
      "Iteration: 20400, Loss: 5.749489355366677e-05 for layer [ True]\n",
      "Iteration: 20500, Loss: 0.0003992312995251268 for layer [ True]\n",
      "Iteration: 20600, Loss: 0.0011086208978667855 for layer [ True]\n",
      "Iteration: 20700, Loss: 2.864551067352295 for layer [False]\n",
      "Iteration: 20800, Loss: 0.00031277810921892524 for layer [ True]\n",
      "Iteration: 20900, Loss: 0.0007334785186685622 for layer [ True]\n",
      "Iteration: 21000, Loss: 2.153188943862915 for layer [False]\n",
      "Iteration: 21100, Loss: 9.07269287109375 for layer [False]\n",
      "Iteration: 21200, Loss: 0.0003805042360909283 for layer [ True]\n",
      "Iteration: 21300, Loss: 4.55739688873291 for layer [False]\n",
      "Iteration: 21400, Loss: 0.000630767724942416 for layer [ True]\n",
      "Iteration: 21500, Loss: 3.4977595806121826 for layer [False]\n",
      "Iteration: 21600, Loss: 4.473350524902344 for layer [False]\n",
      "Iteration: 21700, Loss: 0.0002733076980803162 for layer [ True]\n",
      "Iteration: 21800, Loss: 0.0005715427687391639 for layer [ True]\n",
      "Iteration: 21900, Loss: 3.094667673110962 for layer [False]\n",
      "Iteration: 22000, Loss: 4.012214183807373 for layer [False]\n",
      "Iteration: 22100, Loss: 0.00011565303429961205 for layer [ True]\n",
      "Iteration: 22200, Loss: 0.0007558637880720198 for layer [ True]\n",
      "Iteration: 22300, Loss: 4.342065811157227 for layer [False]\n",
      "Iteration: 22400, Loss: 5.4231672286987305 for layer [False]\n",
      "Iteration: 22500, Loss: 1.817814588546753 for layer [False]\n",
      "Iteration: 22600, Loss: 0.00012519554002210498 for layer [ True]\n",
      "Iteration: 22700, Loss: 0.00011125471064588055 for layer [ True]\n",
      "Iteration: 22800, Loss: 0.00043974665459245443 for layer [ True]\n",
      "Iteration: 22900, Loss: 0.00038499030051752925 for layer [ True]\n",
      "Iteration: 23000, Loss: 0.00078109948663041 for layer [ True]\n",
      "Iteration: 23100, Loss: 0.0021008506882935762 for layer [ True]\n",
      "Iteration: 23200, Loss: 0.00020013433822896332 for layer [ True]\n",
      "Iteration: 23300, Loss: 0.00011121178977191448 for layer [ True]\n",
      "Iteration: 23400, Loss: 0.00023770438565406948 for layer [ True]\n",
      "Iteration: 23500, Loss: 6.168917752802372e-05 for layer [ True]\n",
      "Iteration: 23600, Loss: 0.00031638616928830743 for layer [ True]\n",
      "Iteration: 23700, Loss: 3.1380105018615723 for layer [False]\n",
      "Iteration: 23800, Loss: 0.0006161135388538241 for layer [ True]\n",
      "Iteration: 23900, Loss: 0.00018072866078000516 for layer [ True]\n",
      "Iteration: 24000, Loss: 0.0002632148680277169 for layer [ True]\n",
      "Iteration: 24100, Loss: 0.0001209190086228773 for layer [ True]\n",
      "Iteration: 24200, Loss: 4.009400367736816 for layer [False]\n",
      "Iteration: 24300, Loss: 2.698870897293091 for layer [False]\n",
      "Iteration: 24400, Loss: 4.544946193695068 for layer [False]\n",
      "Iteration: 24500, Loss: 0.00017415483307559043 for layer [ True]\n",
      "Iteration: 24600, Loss: 5.0721711886581033e-05 for layer [ True]\n",
      "Iteration: 24700, Loss: 5.338846206665039 for layer [False]\n",
      "Iteration: 24800, Loss: 4.167933464050293 for layer [False]\n",
      "Iteration: 24900, Loss: 0.00012274298933334649 for layer [ True]\n",
      "Step 19500 | Loss: 0.000442\n",
      "Step 19600 | Loss: 0.000442\n",
      "Step 19700 | Loss: 0.000443\n",
      "Step 19800 | Loss: 0.000443\n",
      "Step 19900 | Loss: 0.000442\n"
     ]
    }
   ],
   "source": [
    "sofo_eigs_losses_keep_learning = nn_SOFO_eigs2(K, no_of_iters, student_params, x, y, layers, \n",
    "                                               approx_freq=500, sketching_iters=25000, approx_K=10, \n",
    "                                               learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eba075df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "plt.plot(sofo_losses, label='SOFO')\n",
    "plt.plot(sofo_eigs_losses, label='EIG-SOFO (static GGN approximation)')\n",
    "plt.plot(adam_losses, label='Adam')\n",
    "plt.plot(sofo_eigs_losses_keep_learning, label='EIG-SOFO (dynamic GGN approximation)')\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Log Loss', fontsize=14)\n",
    "plt.title('Training the Student Network', fontsize=16)\n",
    "plt.yscale('log')\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "with open(f'sofo_losses10.pkl', 'wb') as f:\n",
    "    pickle.dump(sofo_losses, f)\n",
    "\n",
    "with open(f'small_sofo_eigs_losses10.pkl', 'wb') as f:\n",
    "    pickle.dump(sofo_eigs_losses, f)\n",
    "\n",
    "with open(f'small_sofo_eigs_losses_keep_learning10.pkl', 'wb') as f:\n",
    "    pickle.dump(sofo_eigs_losses_keep_learning, f)\n",
    "\n",
    "with open(f'small_adam_losses10.pkl', 'wb') as f:\n",
    "    pickle.dump(adam_losses, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e4cafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sofo_losses7.pkl', 'rb') as f:\n",
    "    sofo_losses_loaded = pickle.load(f)\n",
    "\n",
    "with open('sofo_eigs_losses7.pkl', 'rb') as f:\n",
    "    sofo_losses_loaded2 = pickle.load(f)\n",
    "\n",
    "with open('sofo_eigs_losses_keep_learning7.pkl', 'rb') as f:\n",
    "    sofo_losses_loaded3 = pickle.load(f)\n",
    "\n",
    "with open('adam_losses7.pkl', 'rb') as f:\n",
    "    sofo_losses_loaded4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbd50be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sofo_losses_loaded, label='SOFO')\n",
    "plt.plot(sofo_losses_loaded2, label='EIG-SOFO (static GGN approximation)')\n",
    "plt.plot(sofo_losses_loaded4, label='Adam')\n",
    "plt.plot(sofo_losses_loaded3, label='EIG-SOFO (dynamic GGN approximation)')\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Log Loss', fontsize=14)\n",
    "plt.title('Training the Student Network', fontsize=16)\n",
    "plt.yscale('log')\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08971636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "indices = [0, 1, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "no_of_iters = 20000\n",
    "\n",
    "sofo_total = np.zeros((len(indices), no_of_iters))\n",
    "sofo_eigs_total = np.zeros((len(indices), no_of_iters))\n",
    "sofo_eigs_keep_learning_total = np.zeros((len(indices), no_of_iters))\n",
    "adam_total = np.zeros((len(indices), no_of_iters))\n",
    "\n",
    "c = 0\n",
    "for i in indices:\n",
    "    with open(f'sofo_losses{i}.pkl', 'rb') as f:\n",
    "        sofo_losses_loaded = pickle.load(f)\n",
    "        sofo_total[c, :] = sofo_losses_loaded\n",
    "    with open(f'sofo_eigs_losses{i}.pkl', 'rb') as f:\n",
    "        sofo_losses_loaded2 = pickle.load(f)\n",
    "        sofo_eigs_total[c, :] = sofo_losses_loaded2\n",
    "    with open(f'sofo_eigs_losses_keep_learning{i}.pkl', 'rb') as f:\n",
    "        sofo_losses_loaded3 = pickle.load(f)\n",
    "        sofo_eigs_keep_learning_total[c, :] = sofo_losses_loaded3\n",
    "    with open(f'adam_losses{i}.pkl', 'rb') as f:\n",
    "        sofo_losses_loaded4 = pickle.load(f)\n",
    "        adam_total[c, :] = sofo_losses_loaded4\n",
    "    c += 1\n",
    "\n",
    "std_dev_sofo = np.std(sofo_total, axis=0)\n",
    "std_dev_sofo_eigs = np.std(sofo_eigs_total, axis=0)\n",
    "std_dev_sofo_eigs_keep_learning = np.std(sofo_eigs_keep_learning_total, axis=0)\n",
    "std_dev_adam = np.std(adam_total, axis=0)\n",
    "\n",
    "mean_sofo = np.mean(sofo_total, axis=0)\n",
    "mean_sofo_eigs = np.mean(sofo_eigs_total, axis=0)\n",
    "mean_sofo_eigs_keep_learning = np.mean(sofo_eigs_keep_learning_total, axis=0)\n",
    "\n",
    "mean_adam = np.mean(adam_total, axis=0)\n",
    "\n",
    "median_sofo = np.median(sofo_total, axis=0)\n",
    "median_sofo_eigs = np.median(sofo_eigs_total, axis=0)\n",
    "median_sofo_eigs_keep_learning = np.median(sofo_eigs_keep_learning_total, axis=0)\n",
    "median_adam = np.median(adam_total, axis=0)\n",
    "\n",
    "percentile25_sofo = np.percentile(sofo_total, 25, axis=0)\n",
    "percentile75_sofo = np.percentile(sofo_total, 75, axis=0)\n",
    "percentile25_sofo_eigs = np.percentile(sofo_eigs_total, 25, axis=0)\n",
    "percentile75_sofo_eigs = np.percentile(sofo_eigs_total, 75, axis=0)\n",
    "percentile25_sofo_eigs_keep_learning = np.percentile(sofo_eigs_keep_learning_total, 25, axis=0)\n",
    "percentile75_sofo_eigs_keep_learning = np.percentile(sofo_eigs_keep_learning_total, 75, axis=0)\n",
    "percentile25_adam = np.percentile(adam_total, 25, axis=0)\n",
    "percentile75_adam = np.percentile(adam_total, 75, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0441167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(median_sofo, label='SOFO')\n",
    "plt.fill_between(np.arange(no_of_iters), percentile25_sofo, percentile75_sofo, alpha=0.2)\n",
    "\n",
    "plt.plot(median_sofo_eigs, label='EIG-SOFO (static GGN approximation)')\n",
    "plt.fill_between(np.arange(no_of_iters), percentile25_sofo_eigs, percentile75_sofo_eigs, alpha=0.2)\n",
    "\n",
    "plt.plot(median_adam, label='Adam')\n",
    "plt.fill_between(np.arange(no_of_iters), percentile25_adam, percentile75_adam, alpha=0.2)\n",
    "\n",
    "plt.plot(median_sofo_eigs_keep_learning, label='EIG-SOFO (dynamic GGN approximation)')\n",
    "plt.fill_between(np.arange(no_of_iters), percentile25_sofo_eigs_keep_learning, percentile75_sofo_eigs_keep_learning, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Log Loss', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.legend(fontsize=12)\n",
    "plt.savefig('final_results(Nh=25).png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0936f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "no_of_iters = 20000\n",
    "\n",
    "sofo_total = np.zeros((len(indices), no_of_iters))\n",
    "sofo_eigs_total = np.zeros((len(indices), no_of_iters))\n",
    "sofo_eigs_keep_learning_total = np.zeros((len(indices), no_of_iters))\n",
    "adam_total = np.zeros((len(indices), no_of_iters))\n",
    "\n",
    "c = 0\n",
    "for i in indices:\n",
    "    with open(f'small_sofo_losses{i}.pkl', 'rb') as f:\n",
    "        sofo_losses_loaded = pickle.load(f)\n",
    "        sofo_total[c, :] = sofo_losses_loaded\n",
    "    with open(f'small_sofo_eigs_losses{i}.pkl', 'rb') as f:\n",
    "        sofo_losses_loaded2 = pickle.load(f)\n",
    "        sofo_eigs_total[c, :] = sofo_losses_loaded2\n",
    "    with open(f'small_sofo_eigs_losses_keep_learning{i}.pkl', 'rb') as f:\n",
    "        sofo_losses_loaded3 = pickle.load(f)\n",
    "        sofo_eigs_keep_learning_total[c, :] = sofo_losses_loaded3\n",
    "    with open(f'small_adam_losses{i}.pkl', 'rb') as f:\n",
    "        sofo_losses_loaded4 = pickle.load(f)\n",
    "        adam_total[c, :] = sofo_losses_loaded4\n",
    "    c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb3df556",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_sofo = np.median(sofo_total, axis=0)\n",
    "median_sofo_eigs = np.median(sofo_eigs_total, axis=0)\n",
    "median_sofo_eigs_keep_learning = np.median(sofo_eigs_keep_learning_total, axis=0)\n",
    "median_adam = np.median(adam_total, axis=0)\n",
    "percentile25_sofo = np.percentile(sofo_total, 25, axis=0)\n",
    "percentile75_sofo = np.percentile(sofo_total, 75, axis=0)\n",
    "percentile25_sofo_eigs = np.percentile(sofo_eigs_total, 25, axis=0)\n",
    "percentile75_sofo_eigs = np.percentile(sofo_eigs_total, 75, axis=0)\n",
    "percentile25_sofo_eigs_keep_learning = np.percentile(sofo_eigs_keep_learning_total, 25, axis=0)\n",
    "percentile75_sofo_eigs_keep_learning = np.percentile(sofo_eigs_keep_learning_total, 75, axis=0)\n",
    "percentile25_adam = np.percentile(adam_total, 25, axis=0)\n",
    "percentile75_adam = np.percentile(adam_total, 75, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f71bfd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(median_sofo, label='SOFO')\n",
    "plt.fill_between(np.arange(no_of_iters), percentile25_sofo, percentile75_sofo, alpha=0.2)\n",
    "\n",
    "plt.plot(median_sofo_eigs, label='EIG-SOFO (static GGN approximation)')\n",
    "plt.fill_between(np.arange(no_of_iters), percentile25_sofo_eigs, percentile75_sofo_eigs, alpha=0.2)\n",
    "\n",
    "plt.plot(median_adam, label='Adam')\n",
    "plt.fill_between(np.arange(no_of_iters), percentile25_adam, percentile75_adam, alpha=0.2)\n",
    "\n",
    "plt.plot(median_sofo_eigs_keep_learning, label='EIG-SOFO (dynamic GGN approximation)')\n",
    "plt.fill_between(np.arange(no_of_iters), percentile25_sofo_eigs_keep_learning, percentile75_sofo_eigs_keep_learning, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Log Loss', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.legend(fontsize=13)\n",
    "plt.savefig('final_results(Nh=10)_extra.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
